from typing import Sequence
from aleph_alpha_client import Client, CompletionRequest, Prompt
from pydantic import BaseModel

from .completion import Completion, CompletionInput, CompletionOutput
from .prompt_template import PromptTemplate, PromptWithMetadata
from .text_highlight import TextHighlight, TextHighlightInput
from .task import DebugLogger, Task


class SummarizeInput(BaseModel):
    """The input for a summarize task.

    Attributes:
        text: The text to be summarized. Must fit within the token limit (after accounting for prompt & completion).

    """

    text: str


class SummarizeOutput(BaseModel):
    """The output of a summarize task.

    Attributes:
        summary: The summary generated by the task.
        highlights: Highlights indicating which parts of the chunk contributed to the summary. Each highlight is a quote from the text.

    """

    summary: str
    highlights: Sequence[str]


class ShortBodySummarize(Task[SummarizeInput, SummarizeOutput]):
    """Task implementation of summarizing a text in just a few sentences.

    Depends on SummarizeInput and SummarizeOutput. Uses Aleph Alpha models to generate a short natural language summary.

    Will also return highlights that explain the parts of the input that contributed strongly to the completion.

    Note:
        'model' provided should be a control-type model.

    Args:
        client: Aleph Alpha client instance for running model related API calls.
        model: A valid Aleph Alpha model name.

    Attributes:
        PROMPT_TEMPLATE: The prompt template used for answering the question. 'text' will be inserted here. Includes liquid logic interpreted by 'PromptTemplate':
        MODEL: We recommend using 'luminous-supreme-control' for this task, so it has been pre-set.

    Example:
        >>> client = Client(token="YOUR_AA_TOKEN")
        >>> task = ShortBodySummarize(client)
        >>> input = SummarizeInput(
        >>>     text="This is a story about pizza. Tina hates pizza. However, Mike likes it. Pete strongly believes that pizza is the best thing to exist."
        >>> )
        >>> logger = InMemoryLogger(name="Summary")
        >>> output = task.run(input, logger)
        >>> print(output.summary)
        Tina does not like pizza, but Mike and Pete do.

    """

    PROMPT_TEMPLATE: str = """### Instruction:
Summarize in just one or two sentences.

### Input:
{% promptrange text %}{{text}}{% endpromptrange %}

### Response:"""
    _client: Client

    def __init__(self, client: Client, model: str = "luminous-supreme-control") -> None:
        super().__init__()
        self._client = client
        self._model = model
        self._completion = Completion(client)
        self._text_highlight = TextHighlight(client)

    def run(self, input: SummarizeInput, logger: DebugLogger) -> SummarizeOutput:
        prompt_with_metadata = self._format_prompt(text=input.text, logger=logger)
        completion = self._complete(
            prompt_with_metadata.prompt, logger.child_logger("Generate Summary")
        )
        highlights = self._get_highlights(
            prompt_with_metadata, completion.completion, logger
        )
        return SummarizeOutput(summary=completion.completion, highlights=highlights)

    def _format_prompt(self, text: str, logger: DebugLogger) -> PromptWithMetadata:
        logger.log(
            "Prompt template/text", {"template": self.PROMPT_TEMPLATE, "text": text}
        )
        prompt_with_metadata = PromptTemplate(
            self.PROMPT_TEMPLATE
        ).to_prompt_with_metadata(text=text)
        return prompt_with_metadata

    def _complete(self, prompt: Prompt, logger: DebugLogger) -> CompletionOutput:
        request = CompletionRequest(
            prompt=prompt,
            maximum_tokens=128,
            log_probs=3,
        )
        response = self._completion.run(
            CompletionInput(request=request, model=self._model),
            logger,
        )
        return response

    def _get_highlights(
        self,
        prompt_with_metadata: PromptWithMetadata,
        completion: str,
        logger: DebugLogger,
    ) -> Sequence[str]:
        highlight_input = TextHighlightInput(
            prompt_with_metadata=prompt_with_metadata,
            target=completion,
            model=self._model,
        )
        highlight_output = self._text_highlight.run(highlight_input, logger)
        return [h.text for h in highlight_output.highlights if h.score > 0]
