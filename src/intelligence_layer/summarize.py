from typing import Sequence
from aleph_alpha_client import Client
from pydantic import BaseModel

from .completion import (
    Instruction,
    InstructionInput,
    InstructionOutput,
)
from .prompt_template import PromptWithMetadata
from .text_highlight import TextHighlight, TextHighlightInput
from intelligence_layer.task import Chunk, DebugLogger, Task


class SummarizeInput(BaseModel):
    """The input for a summarize task.

    Attributes:
        chunk: The text to be summarized. Must fit within the token limit (after accounting for prompt & completion).

    """

    chunk: Chunk


class SummarizeOutput(BaseModel):
    """The output of a summarize task.

    Attributes:
        summary: The summary generated by the task.
        highlights: Highlights indicating which parts of the chunk contributed to the summary. Each highlight is a quote from the text.
    """

    summary: str
    highlights: Sequence[str]


class ShortBodySummarize(Task[SummarizeInput, SummarizeOutput]):
    """Task implementation of summarizing a text in just a few sentences.

    Depends on SummarizeInput and SummarizeOutput. Uses Aleph Alpha models to generate a short natural language summary.

    Will also return highlights that explain the parts of the input that contributed strongly to the completion.

    Note:
        'model' provided should be a control-type model.

    Args:
        client: Aleph Alpha client instance for running model related API calls.
        model: A valid Aleph Alpha model name. We recommend using 'luminous-supreme-control' for this task, so it is the default.

    Attributes:
        MAXIMUM_RESPONSE_TOKENS: The maximum number of tokens the summary will contain.
        INSTRUCTION: The verbal instruction sent to the model to make it generate the summary.

    Example:
        >>> client = Client(token="YOUR_AA_TOKEN")
        >>> task = ShortBodySummarize(client)
        >>> input = SummarizeInput(
        >>>     text="This is a story about pizza. Tina hates pizza. However, Mike likes it. Pete strongly believes that pizza is the best thing to exist."
        >>> )
        >>> logger = InMemoryLogger(name="Summary")
        >>> output = task.run(input, logger)
        >>> print(output.summary)
        Tina does not like pizza, but Mike and Pete do.

    """

    MAXIMUM_RESPONSE_TOKENS = 128
    INSTRUCTION = "Summarize in just one or two sentences."
    _client: Client

    def __init__(self, client: Client, model: str = "luminous-supreme-control") -> None:
        super().__init__()
        self._client = client
        self._model = model
        self._instruction = Instruction(client)
        self._text_highlight = TextHighlight(client)

    def run(self, input: SummarizeInput, logger: DebugLogger) -> SummarizeOutput:
        instruction_output = self._instruct(input.chunk, logger)
        highlights = self._get_highlights(
            instruction_output.prompt_with_metadata, instruction_output.response, logger
        )
        return SummarizeOutput(
            summary=instruction_output.response, highlights=highlights
        )

    def _instruct(self, input: str, logger: DebugLogger) -> InstructionOutput:
        return self._instruction.run(
            InstructionInput(
                instruction=self.INSTRUCTION,
                input=input,
                maximum_response_tokens=self.MAXIMUM_RESPONSE_TOKENS,
                model=self._model,
            ),
            logger,
        )

    def _get_highlights(
        self,
        prompt_with_metadata: PromptWithMetadata,
        completion: str,
        logger: DebugLogger,
    ) -> Sequence[str]:
        highlight_input = TextHighlightInput(
            prompt_with_metadata=prompt_with_metadata,
            target=completion,
            model=self._model,
            focus_ranges=frozenset({"input"}),
        )
        highlight_output = self._text_highlight.run(highlight_input, logger)
        return [h.text for h in highlight_output.highlights if h.score > 0]
