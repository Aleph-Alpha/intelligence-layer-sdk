{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your own custom task\n",
    "\n",
    "If the tasks we have set up don't fit your use case, this guide will go into how to set up your own task from scratch.\n",
    "To do this, we will be setting up a simple keyword extraction task.\n",
    "\n",
    "Keyword extraction is blabla.\n",
    "An example use case could be blabla.\n",
    "A full implementation can be found in blabla.\n",
    "\n",
    "Let's start with the task interface.\n",
    "The full Task interface can be found in [task.py](task.py).\n",
    "However, to implement a Task there are only a few parts relevant to us.\n",
    "The simplified interface for this guide is basically:\n",
    "\n",
    "```python\n",
    "Input = TypeVar(\"Input\", bound=PydanticSerializable)\n",
    "Output = TypeVar(\"Output\", bound=PydanticSerializable)\n",
    "\n",
    "class Task(ABC, Generic[Input, Output]):\n",
    "    @abstractmethod\n",
    "    def run(self, input: Input, logger: DebugLogger) -> Output:\n",
    "        \"\"\"Executes the process for this use-case.\"\"\"\n",
    "        ...\n",
    "```\n",
    "\n",
    "To create our own task, we have to define our Input, Output and how we would like to run it.\n",
    "Since tasks can vary so much, no assumptions are done about the implementation of the task. \n",
    "The only requirement is the fact that the input and output have to be PydanticSerializable.\n",
    "For our keyword extraction our input and output will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class KeywordExtractionInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class KeywordExtractionOutput(BaseModel):\n",
    "    keywords: Sequence[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input and output defined, we can make the task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, CompletionRequest, Prompt\n",
    "\n",
    "from intelligence_layer.task import DebugLogger, Task\n",
    "\n",
    "\n",
    "class KeywordExtractionTask(Task[KeywordExtractionInput, KeywordExtractionOutput]):\n",
    "    PROMPT_TEMPLATE: str = \"\"\"Identify matching keywords for each text.\n",
    "###\n",
    "Text: The \"Whiskey War\" is an ongoing conflict between Denmark and Canada over ownership of Hans Island. The dispute began in 1973, when Denmark and Canada reached an agreement on Greenland's borders. However, no settlement regarding Hans Island could be reached by the time the treaty was signed. Since then both countries have used peaceful means - such as planting their national flag or burying liquor - to draw attention to the disagreement.\n",
    "Keywords: Conflict, Whiskey War, Denmark, Canada, Treaty, Flag, Liquor\n",
    "###\n",
    "Text: NASA launched the Discovery program to explore the solar system. It comprises a series of expeditions that have continued from the program's launch in the 1990s to the present day. In the course of the 16 expeditions launched so far, the Moon, Mars, Mercury and Venus, among others, have been explored. Unlike other space programs, the Discovery program places particular emphasis on cost efficiency, true to the motto: \"faster, better, cheaper\".\n",
    "Keywords: Space program, NASA, Expedition, Cost efficiency, Moon, Mars, Mercury, Venus\n",
    "###\n",
    "Text: {text}\n",
    "Keywords:\"\"\"\n",
    "    MODEL: str = \"luminous-base\"\n",
    "    client: Client\n",
    "\n",
    "    def __init__(self, client: Client) -> None:\n",
    "        super().__init__()\n",
    "        self.client = client\n",
    "\n",
    "    def run(self, input: KeywordExtractionInput, logger: DebugLogger) -> KeywordExtractionOutput:\n",
    "        prompt = self._format_prompt(text=input.text, logger=logger)\n",
    "        completion = self._complete(\n",
    "            prompt, logger.child_logger(\"Generate Summary\")\n",
    "        )\n",
    "        return KeywordExtractionOutput(keywords=[k.strip() for k in completion.split(\",\")])\n",
    "\n",
    "    def _format_prompt(self, text: str, logger: DebugLogger) -> Prompt:\n",
    "        logger.log(\n",
    "            \"Prompt template/text\", {\"template\": self.PROMPT_TEMPLATE, \"text\": text}\n",
    "        )\n",
    "        return Prompt.from_text(self.PROMPT_TEMPLATE.format(text=text))\n",
    "    \n",
    "    def _complete(self, prompt: Prompt, logger: DebugLogger) -> str:\n",
    "        request = CompletionRequest(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[\"\\n\", \"###\"]\n",
    "        )\n",
    "        response = self.client.complete(\n",
    "            request=request,\n",
    "            model=self.MODEL,\n",
    "        )\n",
    "        logger.log(\n",
    "            \"Original request & response\", {\"request\": request, \"response\": response}\n",
    "        )\n",
    "        return response.completions[0].completion # grabs the string completion generated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can run the task like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords=['Computer vision', 'Optical character recognition', 'Image processing', 'Artificial intelligence\\n###\\nText: The \"Whiskey War\" is an ongoing conflict between Denmark and Canada over ownership of Hans Island. The dispute began in 1973', \"when Denmark and Canada reached an agreement on Greenland's borders. However\", 'no settlement regarding Hans']\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "\n",
    "from intelligence_layer.task import JsonDebugLogger\n",
    "\n",
    "\n",
    "client = Client(getenv(\"AA_TOKEN\"))\n",
    "task = KeywordExtractionTask(client)\n",
    "text = \"\"\"Computer vision describes the processing of an image by a machine using external devices (e.g., a scanner) into a digital description of that image for further processing. An example of this is optical character recognition (OCR), the recognition and processing of images containing text. Further processing and final classification of the image is often done using artificial intelligence methods. The goal of this field is to enable computers to process visual tasks that were previously reserved for humans.\"\"\"\n",
    "\n",
    "input = KeywordExtractionInput(text=text)\n",
    "logger = JsonDebugLogger(name=\"classify\")\n",
    "output = task.run(input, logger)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok very cool.\n",
    "Now that it works, we can start evaluating the performance of our task.\n",
    "\n",
    "To do evaluation, we will have to set up an evaluator.\n",
    "The full interface for an evaluator can be found in [task.py](task.py).\n",
    "We will go over it step by step, so for now all we have to worry about is this part of the interface:\n",
    "\n",
    "```python\n",
    "class Evaluator(ABC, Generic[Input, ExpectedOutput, Evaluation, AggregatedEvaluation]):\n",
    "    @abstractmethod\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: Input,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: ExpectedOutput,\n",
    "    ) -> Evaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "First of all, let's create our KeywordExtractionEvaluator.\n",
    "The first generic the evaluator takes is the same as the input for the task, so we can plug this one right in.\n",
    "\n",
    "```python\n",
    "class KeywordExtractionEvaluator(Evaluator[KeywordExtractionInput, ExpectedOutput, Evaluation, AggregatedEvaluation]):\n",
    "    @abstractmethod\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: Input,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: ExpectedOutput,\n",
    "    ) -> Evaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "Now that we have our evaluator, we can start evaluating actual cases.\n",
    "To evaluate a case, we need an interface for our ExpectedOutput, Evaluation and an implementation of the \"evaluate\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the expected output for an example run. This is used to compare the output of the task with.\n",
    "\n",
    "We will be evaluating our keyword extraction based on the expected keywords. \"\"\"\n",
    "class KeywordExtractionExpectedOutput(BaseModel):\n",
    "    expected_keywords: Sequence[str]\n",
    "\n",
    "\"\"\"This is the interface for the metrics that are generated for each evaluation case\"\"\"\n",
    "class KeywordExtractionEvaluation(BaseModel):\n",
    "    correct: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluate function will take an input for the task to process, run the task and calculate any metrics we deem interesting to measure.\n",
    "Finally, it will return the KeywordExtractionEvaluation class. \n",
    "\n",
    "```python\n",
    "def evaluate(\n",
    "        self,\n",
    "        input: KeywordExtractionInput,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: KeywordExtractionExpectedOutput,\n",
    "    ) -> KeywordExtractionEvaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        return  \n",
    "```\n",
    "\n",
    "However, to evaluate the performance of a task, we will need to try out lots of different cases. \n",
    "To do this we can use the \"evaluate_dataset\" function, provided by the Evaluator base class.\n",
    "This will take a dataset, run all the cases in it and aggregate the metrics generated from the evaluation.\n",
    "To set this up, we will need to implement the Dataset class, create an interface for the aggregated metrics and implement the \"aggregate\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the interface for the aggregated metrics that are generated from running a number of examples\"\"\"\n",
    "class KeywordExtractionAggregatedEvaluation(BaseModel):\n",
    "    percentage_correct: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate method takes a sequence of KeywordExtractionEvaluations, and aggregated the metrics we deem important.\n",
    "\n",
    "```python\n",
    "def aggregate(self, evaluations: Sequence[KeywordExtractionEvaluation]) -> KeywordExtractionAggregatedEvaluation:\n",
    "        \"\"\"`Evaluator`-specific method for aggregating individual `Evaluations` into report-like `Aggregated Evaluation`.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "If we would be interested in what the percentage of correct answers is, the aggregated function would be responsible for doing this calculation. \n",
    "So if we would have 10 examples and half of them would be correct, the aggregated function will return an KeywordExtractionAggregatedEvaluation class with a percentage_correct of 50%.\n",
    "\n",
    "Now that we have discussed all of the parts that make up an evaluator, the full class is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import Evaluator\n",
    "\n",
    "class KeywordExtractionEvaluator(Evaluator[KeywordExtractionInput, KeywordExtractionExpectedOutput, KeywordExtractionEvaluation, KeywordExtractionAggregatedEvaluation]):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: KeywordExtractionInput,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: KeywordExtractionExpectedOutput,\n",
    "    ) -> KeywordExtractionEvaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aggregate(self, evaluations: Sequence[KeywordExtractionEvaluation]) -> KeywordExtractionAggregatedEvaluation:\n",
    "        \"\"\"`Evaluator`-specific method for aggregating individual `Evaluations` into report-like `Aggregated Evaluation`.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = KeywordExtractionEvaluator()\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "evaluation = evaluator.run_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-XJoCsS19-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
