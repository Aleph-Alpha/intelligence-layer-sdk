{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your own custom task\n",
    "\n",
    "If the tasks we have set up don't fit your use case, this guide will go into how to set up your own task from scratch.\n",
    "To do this, we will be setting up a simple keyword extraction task.\n",
    "\n",
    "Keyword extraction is basically what the name suggests, extracting keywords from a piece of text.\n",
    "An example use case could be blabla.\n",
    "A full implementation can be found in blabla.\n",
    "\n",
    "Let's start with the task interface.\n",
    "The full Task interface can be found in [task.py](task.py).\n",
    "However, to implement a Task there are only a few parts relevant to us.\n",
    "For now, all we will worry about in terms of the interface is the following part:\n",
    "\n",
    "```python\n",
    "Input = TypeVar(\"Input\", bound=PydanticSerializable)\n",
    "Output = TypeVar(\"Output\", bound=PydanticSerializable)\n",
    "\n",
    "class Task(ABC, Generic[Input, Output]):\n",
    "    @abstractmethod\n",
    "    def run(self, input: Input, logger: DebugLogger) -> Output:\n",
    "        \"\"\"Executes the process for this use-case.\"\"\"\n",
    "        ...\n",
    "```\n",
    "\n",
    "To create our own task, we have to define our Input, Output and how we would like to run it.\n",
    "Since tasks can vary so much, no assumptions are done about the implementation of the task. \n",
    "The only requirement is the fact that the input and output have to be PydanticSerializable.\n",
    "This is done so we can easily save our evaluation datasets.\n",
    "For our keyword extraction our input and output will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class KeywordExtractionInput(BaseModel):\n",
    "    \"\"\"This is the text we will extract keywords from\"\"\"\n",
    "    text: str\n",
    "\n",
    "class KeywordExtractionOutput(BaseModel):\n",
    "    keywords: set[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input and output defined, we can make the task.\n",
    "The steps that the task consists of are:\n",
    "- Create a prompt\n",
    "- Send the prompt to the model\n",
    "- Extract the keywords from the model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, CompletionRequest, Prompt\n",
    "\n",
    "from intelligence_layer.task import DebugLogger, Task\n",
    "\n",
    "\n",
    "class KeywordExtractionTask(Task[KeywordExtractionInput, KeywordExtractionOutput]):\n",
    "    PROMPT_TEMPLATE: str = \"\"\"Identify matching keywords for each text.\n",
    "###\n",
    "Text: The \"Whiskey War\" is an ongoing conflict between Denmark and Canada over ownership of Hans Island. The dispute began in 1973, when Denmark and Canada reached an agreement on Greenland's borders. However, no settlement regarding Hans Island could be reached by the time the treaty was signed. Since then both countries have used peaceful means - such as planting their national flag or burying liquor - to draw attention to the disagreement.\n",
    "Keywords: Conflict, Whiskey War, Denmark, Canada, Treaty, Flag, Liquor\n",
    "###\n",
    "Text: NASA launched the Discovery program to explore the solar system. It comprises a series of expeditions that have continued from the program's launch in the 1990s to the present day. In the course of the 16 expeditions launched so far, the Moon, Mars, Mercury and Venus, among others, have been explored. Unlike other space programs, the Discovery program places particular emphasis on cost efficiency, true to the motto: \"faster, better, cheaper\".\n",
    "Keywords: Space program, NASA, Expedition, Cost efficiency, Moon, Mars, Mercury, Venus\n",
    "###\n",
    "Text: {text}\n",
    "Keywords:\"\"\"\n",
    "    MODEL: str = \"luminous-base\"\n",
    "    client: Client\n",
    "\n",
    "    def __init__(self, client: Client) -> None:\n",
    "        super().__init__()\n",
    "        self.client = client\n",
    "\n",
    "    def run(self, input: KeywordExtractionInput, logger: DebugLogger) -> KeywordExtractionOutput:\n",
    "        prompt = self._format_prompt(text=input.text, logger=logger)\n",
    "        completion = self._complete(\n",
    "            prompt, logger.child_logger(\"Generate Summary\")\n",
    "        )\n",
    "        return KeywordExtractionOutput(keywords=set(k.strip().lower() for k in completion.split(\",\") if k.strip()))\n",
    "\n",
    "    def _format_prompt(self, text: str, logger: DebugLogger) -> Prompt:\n",
    "        logger.log(\n",
    "            \"Prompt template/text\", {\"template\": self.PROMPT_TEMPLATE, \"text\": text}\n",
    "        )\n",
    "        return Prompt.from_text(self.PROMPT_TEMPLATE.format(text=text))\n",
    "    \n",
    "    def _complete(self, prompt: Prompt, logger: DebugLogger) -> str:\n",
    "        request = CompletionRequest(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[\"\\n\", \"###\"],\n",
    "            frequency_penalty=0.25\n",
    "        )\n",
    "        response = self.client.complete(\n",
    "            request=request,\n",
    "            model=self.MODEL,\n",
    "        )\n",
    "        logger.log(\n",
    "            \"Original request & response\", {\"request\": request.to_json(), \"response\": response.to_json()}\n",
    "        )\n",
    "        return response.completions[0].completion # grabs the string completion generated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can run the task like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "\n",
    "from intelligence_layer.task import JsonDebugLogger\n",
    "\n",
    "\n",
    "client = Client(getenv(\"AA_TOKEN\"))\n",
    "task = KeywordExtractionTask(client)\n",
    "text = \"\"\"Computer vision describes the processing of an image by a machine using external devices (e.g., a scanner) into a digital description of that image for further processing. An example of this is optical character recognition (OCR), the recognition and processing of images containing text. Further processing and final classification of the image is often done using artificial intelligence methods. The goal of this field is to enable computers to process visual tasks that were previously reserved for humans.\"\"\"\n",
    "\n",
    "input = KeywordExtractionInput(text=text)\n",
    "logger = JsonDebugLogger(name=\"classify\")\n",
    "output = task.run(input, logger)\n",
    "\n",
    "print(output)\n",
    "logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great!\n",
    "Now that our task is setup, we can start evaluating the performance of our task.\n",
    "\n",
    "To do evaluation, we will have to set up an evaluator.\n",
    "The full interface for an evaluator can be found in [task.py](task.py).\n",
    "We will go over it step by step, so for now all we have to worry about is this part of the interface:\n",
    "\n",
    "```python\n",
    "class Evaluator(ABC, Generic[Input, ExpectedOutput, Evaluation, AggregatedEvaluation]):\n",
    "    @abstractmethod\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: Input,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: ExpectedOutput,\n",
    "    ) -> Evaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "First of all, let's create our KeywordExtractionEvaluator.\n",
    "The first generic the evaluator takes is the same as the input for the task, so we can plug this one right in.\n",
    "\n",
    "```python\n",
    "class KeywordExtractionEvaluator(Evaluator[KeywordExtractionInput, ExpectedOutput, Evaluation, AggregatedEvaluation]):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: Input,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: ExpectedOutput,\n",
    "    ) -> Evaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "Now that we have our evaluator, we can start evaluating actual examples.\n",
    "To evaluate a case, we need an interface for our `ExpectedOutput`, `Evaluation` and an implementation of the `evaluate` function.\n",
    "In our case, we are interested in the proportion of correctly generate keywords compared to all expected keywords. \n",
    "This is also known as the `true positive rate`.\n",
    "To calculate this, the evaluate function will need a set of the expected keywords.\n",
    "This can be seen in the `KeywordExtractionExpectedOutput` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordExtractionExpectedOutput(BaseModel):\n",
    "    \"\"\"This is the expected output for an example run. This is used to compare the output of the task with.\n",
    "\n",
    "    We will be evaluating our keyword extraction based on the expected keywords. \"\"\"\n",
    "    keywords: set[str]\n",
    "\n",
    "class KeywordExtractionEvaluation(BaseModel):\n",
    "    \"\"\"This is the interface for the metrics that are generated for each evaluation case\"\"\"\n",
    "    true_positive_rate: float \n",
    "    true_positives: set[str]\n",
    "    false_positives: set[str]\n",
    "    false_negatives: set[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluate function will take an input for the task to process, runs the task and calculates the true positive rate. \n",
    "Finally, it will return an instance of the KeywordExtractionEvaluation class with the rate and the (in)correct keywords. \n",
    "\n",
    "```python\n",
    "def evaluate(\n",
    "        self,\n",
    "        input: KeywordExtractionInput,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: KeywordExtractionExpectedOutput,\n",
    "    ) -> KeywordExtractionEvaluation:\n",
    "        output = self.task.run(input, logger)\n",
    "        true_positives = expected_output.keywords & output.keywords\n",
    "        false_positives = output.keywords - true_positives\n",
    "        false_negatives = true_positives - output.keywords\n",
    "        return KeywordExtractionEvaluation(true_positive_rate=len(true_positives) / len(expected_output.keywords), \n",
    "                                           true_positives=true_positives, \n",
    "                                           false_positive=false_positives, \n",
    "                                           false_negatives=false_negatives)\n",
    "```\n",
    "\n",
    "However, to evaluate the performance of a task, we will need to try out lots of different examples. \n",
    "To do this we can use the \"evaluate_dataset\" function, provided by the Evaluator base class.\n",
    "This will take a dataset, run all the examples in the dataset and aggregate the metrics generated from the evaluation.\n",
    "To set this up, we will need to create a dataset, an interface for the aggregated metrics and implement the \"aggregate\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the interface for the aggregated metrics that are generated from running a number of examples\"\"\"\n",
    "class KeywordExtractionAggregatedEvaluation(BaseModel):\n",
    "    average_true_positive_rate: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate method takes as input a sequence of KeywordExtractionEvaluations that are generated by the `evaluate_dataset` method.\n",
    "It is responsible for aggregating the metrics generated from running the dataset.\n",
    "\n",
    "```python\n",
    "def aggregate(self, evaluations: Sequence[KeywordExtractionEvaluation]) -> KeywordExtractionAggregatedEvaluation:\n",
    "        \"\"\"`Evaluator`-specific method for aggregating individual `Evaluations` into report-like `Aggregated Evaluation`.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "Now that we have discussed all of the parts that make up an evaluator, the full class is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from intelligence_layer.task import Evaluator\n",
    "\n",
    "class KeywordExtractionEvaluator(Evaluator[KeywordExtractionInput, KeywordExtractionExpectedOutput, KeywordExtractionEvaluation, KeywordExtractionAggregatedEvaluation]):\n",
    "    def __init__(self, task: KeywordExtractionTask) -> None:\n",
    "        \"\"\"We recommend adding the task to the init method of the evaluator\n",
    "        \n",
    "        This allows for easy comparing of different implementations of the same task.\"\"\"\n",
    "        self.task = task\n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: KeywordExtractionInput,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: KeywordExtractionExpectedOutput,\n",
    "    ) -> KeywordExtractionEvaluation:\n",
    "        output = self.task.run(input, logger)\n",
    "        true_positives = output.keywords & expected_output.keywords\n",
    "        false_positives = output.keywords - expected_output.keywords \n",
    "        false_negatives = expected_output.keywords - output.keywords\n",
    "        return KeywordExtractionEvaluation(true_positive_rate=len(true_positives) / len(expected_output.keywords), \n",
    "                                           true_positives=true_positives, \n",
    "                                           false_positives=false_positives, \n",
    "                                           false_negatives=false_negatives)\n",
    "         \n",
    "\n",
    "    def aggregate(self, evaluations: Sequence[KeywordExtractionEvaluation]) -> KeywordExtractionAggregatedEvaluation:\n",
    "        true_positive_rate = mean(e.true_positive_rate for e in evaluations)\n",
    "        return KeywordExtractionAggregatedEvaluation(average_true_positive_rate=true_positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = KeywordExtractionEvaluator(task=task)\n",
    "\n",
    "logger = JsonDebugLogger(name=\"Evaluation logger\")\n",
    "input = KeywordExtractionInput(text=\"A text about dolphins and sharks\")\n",
    "expected_output = KeywordExtractionExpectedOutput(keywords=[\"dolphins\", \"sharks\"])\n",
    "evaluation = evaluator.evaluate(input, \n",
    "                                logger, \n",
    "                                expected_output)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our aggregate method, let's run a dataset with some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import Dataset, Example\n",
    "\n",
    "dataset = Dataset(\n",
    "    name=\"Keyword extraction dataset\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=input,\n",
    "            expected_output=expected_output\n",
    "        ), \n",
    "        Example(\n",
    "            input=KeywordExtractionInput(\n",
    "                text=\"Clinical psychology is an integration of human science, behavioral science, theory, and clinical knowledge for the purpose of understanding, preventing, and relieving psychologically-based distress or dysfunction and to promote subjective well-being and personal development.\"\n",
    "            ),\n",
    "            expected_output=KeywordExtractionExpectedOutput(\n",
    "                keywords={\"clinical psychology\", \"well-being\", \"personal development\"}\n",
    "            )\n",
    "        ),\n",
    "        Example(\n",
    "            input=KeywordExtractionInput(\n",
    "                text=\"Prospect theory is a theory of behavioral economics, judgment and decision making that was developed by Daniel Kahneman and Amos Tversky in 1979.[1] The theory was cited in the decision to award Kahneman the 2002 Nobel Memorial Prize in Economics.[2]Based on results from controlled studies, it describes how individuals assess their loss and gain perspectives in an asymmetric manner (see loss aversion).\"\n",
    "            ),\n",
    "            expected_output=KeywordExtractionExpectedOutput(\n",
    "                keywords={\"prospect theory\", \"behavioural economics\", \"decision making\", \"losses and gains\"}\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "logger = JsonDebugLogger(name=\"Evaluate dataset debug logger\")\n",
    "\n",
    "aggregated_evaluations = evaluator.evaluate_dataset(dataset, logger)\n",
    "print(aggregated_evaluations)\n",
    "logger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-XJoCsS19-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
