{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your own custom task\n",
    "\n",
    "If the tasks we have set up don't fit your use case, this guide will go into how to set up your own task from scratch.\n",
    "To do this, we will be setting up a simple keyword extraction task.\n",
    "\n",
    "Keyword extraction is blabla.\n",
    "An example use case could be blabla.\n",
    "A full implementation can be found in blabla.\n",
    "\n",
    "Let's start with the task interface.\n",
    "The full Task interface can be found in [task.py](task.py).\n",
    "However, to implement a Task there are only a few parts relevant to us.\n",
    "The simplified interface for this guide is basically:\n",
    "\n",
    "```python\n",
    "Input = TypeVar(\"Input\", bound=PydanticSerializable)\n",
    "Output = TypeVar(\"Output\", bound=PydanticSerializable)\n",
    "\n",
    "class Task(ABC, Generic[Input, Output]):\n",
    "    @abstractmethod\n",
    "    def run(self, input: Input, logger: DebugLogger) -> Output:\n",
    "        \"\"\"Executes the process for this use-case.\"\"\"\n",
    "        ...\n",
    "```\n",
    "\n",
    "To create our own task, we have to define our Input, Output and how we would like to run it.\n",
    "Since tasks can vary so much, no assumptions are done about the implementation of the task. \n",
    "The only requirement is the fact that the input and output have to be PydanticSerializable.\n",
    "For our keyword extraction our input and output will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class KeywordExtractionInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class KeywordExtractionOutput(BaseModel):\n",
    "    keywords: Sequence[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input and output defined, we can make the task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import DebugLogger, Task\n",
    "\n",
    "class KeywordExtractionTask(Task[KeywordExtractionInput, KeywordExtractionOutput]):\n",
    "    def run(self, input: KeywordExtractionInput, logger: DebugLogger) -> KeywordExtractionOutput:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can run the task like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok very cool.\n",
    "Now that it works, we can start evaluating the performance of our task.\n",
    "\n",
    "To do evaluation, we will have to set up an evaluator.\n",
    "The full interface for an evaluator can be found in [task.py](task.py).\n",
    "We will go over it step by step, so for now all we have to worry about is this part of the interface:\n",
    "\n",
    "```python\n",
    "class Evaluator(ABC, Generic[Input, ExpectedOutput, Evaluation, AggregatedEvaluation]):\n",
    "    @abstractmethod\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: Input,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: ExpectedOutput,\n",
    "    ) -> Evaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "First of all, let's create our KeywordExtractionEvaluator.\n",
    "The first generic the evaluator takes is the same as the input for the task, so we can plug this one right in.\n",
    "\n",
    "```python\n",
    "class KeywordExtractionEvaluator(Evaluator[KeywordExtractionInput, ExpectedOutput, Evaluation, AggregatedEvaluation]):\n",
    "    @abstractmethod\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: Input,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: ExpectedOutput,\n",
    "    ) -> Evaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "Now that we have our evaluator, we can start evaluating actual cases.\n",
    "To evaluate a case, we need an interface for our ExpectedOutput, Evaluation and an implementation of the \"evaluate\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the expected output for an example run. This is used to compare the output of the task with.\n",
    "\n",
    "We will be evaluating our keyword extraction based on the expected keywords. \"\"\"\n",
    "class KeywordExtractionExpectedOutput(BaseModel):\n",
    "    expected_keywords: Sequence[str]\n",
    "\n",
    "\"\"\"This is the interface for the metrics that are generated for each evaluation case\"\"\"\n",
    "class KeywordExtractionEvaluation(BaseModel):\n",
    "    correct: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluate function will take an input for the task to process, run the task and calculate any metrics we deem interesting to measure.\n",
    "Finally, it will return the KeywordExtractionEvaluation class. \n",
    "\n",
    "```python\n",
    "def evaluate(\n",
    "        self,\n",
    "        input: KeywordExtractionInput,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: KeywordExtractionExpectedOutput,\n",
    "    ) -> KeywordExtractionEvaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        return  \n",
    "```\n",
    "\n",
    "However, to evaluate the performance of a task, we will need to try out lots of different cases. \n",
    "To do this we can use the \"evaluate_dataset\" function, provided by the Evaluator base class.\n",
    "This will take a dataset, run all the cases in it and aggregate the metrics generated from the evaluation.\n",
    "To set this up, we will need to implement the Dataset class, create an interface for the aggregated metrics and implement the \"aggregate\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the interface for the aggregated metrics that are generated from running a number of examples\"\"\"\n",
    "class KeywordExtractionAggregatedEvaluation(BaseModel):\n",
    "    percentage_correct: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate method takes a sequence of KeywordExtractionEvaluations, and aggregated the metrics we deem important.\n",
    "\n",
    "```python\n",
    "def aggregate(self, evaluations: Sequence[KeywordExtractionEvaluation]) -> KeywordExtractionAggregatedEvaluation:\n",
    "        \"\"\"`Evaluator`-specific method for aggregating individual `Evaluations` into report-like `Aggregated Evaluation`.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "If we would be interested in what the percentage of correct answers is, the aggregated function would be responsible for doing this calculation. \n",
    "So if we would have 10 examples and half of them would be correct, the aggregated function will return an KeywordExtractionAggregatedEvaluation class with a percentage_correct of 50%.\n",
    "\n",
    "Now that we have discussed all of the parts that make up an evaluator, the full class is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import Evaluator\n",
    "\n",
    "class KeywordExtractionEvaluator(Evaluator[KeywordExtractionInput, KeywordExtractionExpectedOutput, KeywordExtractionEvaluation, KeywordExtractionAggregatedEvaluation]):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        input: KeywordExtractionInput,\n",
    "        logger: DebugLogger,\n",
    "        expected_output: KeywordExtractionExpectedOutput,\n",
    "    ) -> KeywordExtractionEvaluation:\n",
    "        \"\"\"Executes the evaluation for this use-case.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aggregate(self, evaluations: Sequence[KeywordExtractionEvaluation]) -> KeywordExtractionAggregatedEvaluation:\n",
    "        \"\"\"`Evaluator`-specific method for aggregating individual `Evaluations` into report-like `Aggregated Evaluation`.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = KeywordExtractionEvaluator()\n",
    "\n",
    "dataset = Dataset()\n",
    "\n",
    "evaluation = evaluator.run_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-XJoCsS19-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
