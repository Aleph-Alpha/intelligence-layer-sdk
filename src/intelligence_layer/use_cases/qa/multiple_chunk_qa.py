from typing import Iterable, Mapping, Optional, Sequence

from pydantic import BaseModel

from intelligence_layer.core.chunk import Chunk
from intelligence_layer.core.detect_language import Language, language_config
from intelligence_layer.core.model import (
    CompleteInput,
    CompleteOutput,
    ControlModel,
    LuminousControlModel,
)
from intelligence_layer.core.task import Task
from intelligence_layer.core.tracer import TaskSpan
from intelligence_layer.use_cases.qa.single_chunk_qa import (
    SingleChunkQa,
    SingleChunkQaInput,
    SingleChunkQaOutput,
)


class MultipleChunkQaInput(BaseModel):
    """The input for a `MultipleChunkQa` task.

    Attributes:
        chunks: The list of chunks that will be used to answer the question.
            Can be arbitrarily long list of chunks.
        question: The question that will be answered based on the chunks.
        language: The desired language of the answer. ISO 619 str with language e.g. en, fr, etc.
    """

    chunks: Sequence[Chunk]
    question: str
    language: Language = Language("en")


class Subanswer(BaseModel):
    """Individual answer based on just one of the multiple chunks.

    Attributes:
        answer: The answer generated by the task. Can be a string or None (if no answer was found).
        chunk: Piece of the original text that answer is based on.
        highlights: The specific sentences that explain the answer the most.
            These are generated by the `TextHighlight` Task.
    """

    answer: Optional[str]
    chunk: Chunk
    highlights: Sequence[str]


class MultipleChunkQaOutput(BaseModel):
    """The output of a `MultipleChunkQa` task.

    Attributes:
        answer: The answer generated by the task. Can be a string or None (if no answer was found).
        subanswers: All the subanswers used to generate the answer.
    """

    answer: Optional[str]
    subanswers: Sequence[Subanswer]


class MergeAnswersInstructConfig(BaseModel):
    instruction: str
    question_label: str
    answers_label: str
    final_answer_label: str
    maximum_tokens: int = 128


MERGE_ANSWERS_INSTRUCT_CONFIGS = {
    Language("en"): MergeAnswersInstructConfig(
        instruction="You are tasked with combining multiple answers into a single answer. "
        "If conflicting answers arise, acknowledge the discrepancies by presenting them collectively",
        question_label="Question",
        answers_label="Answers",
        final_answer_label="Final answer:",
    ),
    Language("it"): MergeAnswersInstructConfig(
        instruction="Il compito è quello di combinare più risposte in un'unica risposta. "
        "Se emergono risposte contrastanti, riconoscete le discrepanze presentandole collettivamente.",
        question_label="Domanda",
        answers_label="Risposte",
        final_answer_label="Risposta finale:",
    ),
    Language("fr"): MergeAnswersInstructConfig(
        instruction="Vous devez combiner plusieurs réponses en une seule. "
        "Si des réponses contradictoires apparaissent, reconnaissez les divergences en les présentant collectivement",
        question_label="Question",
        answers_label="Réponses",
        final_answer_label="Réponse finale:",
    ),
    Language("de"): MergeAnswersInstructConfig(
        instruction="Fasse alle Antworten zu einer einzigen Antwort zusammen. Falls es Widersprüche gibt, präsentiere diese.",
        question_label="Frage",
        answers_label="Antworten",
        final_answer_label="Endgültige Antwort:",
    ),
    Language("es"): MergeAnswersInstructConfig(
        instruction="Su tarea consiste en combinar varias respuestas en una sola. "
        "Si surgen respuestas contradictorias, reconozca las discrepancias presentándolas colectivamente",
        question_label="Pregunta",
        answers_label="Respuestas",
        final_answer_label="Respuesta final:",
    ),
}


class MultipleChunkQa(Task[MultipleChunkQaInput, MultipleChunkQaOutput]):
    """Answer a question on the basis of a list of text chunks.

    Uses Aleph Alpha models to generate a natural language answer based on multiple text chunks.
    Best for longer texts that are already split into smaller units (chunks).
    Relies on SingleChunkQa to generate answers for each chunk and then merges the answers into a single final answer.
    Includes logic to return 'answer = None' if the language model determines that the question cannot be
    reliably answered on the basis of the chunks.

    Note:
        `model` provided should be a control-type model.

    Args:
        model: The model used throughout the task for model related API calls.
        merge_answers_instruct_configs: Mapping language used to prompt parameters.

    Example:
        >>> import os
        >>> from intelligence_layer.connectors import (
        ...     LimitedConcurrencyClient,
        ... )
        >>> from intelligence_layer.core import Language
        >>> from intelligence_layer.core import InMemoryTracer
        >>> from intelligence_layer.core.chunk import Chunk
        >>> from intelligence_layer.use_cases import (
        ...     MultipleChunkQa,
        ...     MultipleChunkQaInput,
        ... )


        >>> task = MultipleChunkQa()
        >>> input = MultipleChunkQaInput(
        ...     chunks=[Chunk("Tina does not like pizza."), Chunk("Mike is a big fan of pizza.")],
        ...     question="Who likes pizza?",
        ...     language=Language("en"),
        ... )
        >>> tracer = InMemoryTracer()
        >>> output = task.run(input, tracer)
        >>> print(output.answer)
        Mike likes pizza.
    """

    def __init__(
        self,
        model: ControlModel = LuminousControlModel("luminous-supreme-control-20240215"),
        merge_answers_instruct_configs: Mapping[
            Language, MergeAnswersInstructConfig
        ] = MERGE_ANSWERS_INSTRUCT_CONFIGS,
    ):
        super().__init__()
        self._single_chunk_qa = SingleChunkQa(model)
        self._model = model
        self._merge_answers_instruct_configs = merge_answers_instruct_configs

    def do_run(
        self, input: MultipleChunkQaInput, task_span: TaskSpan
    ) -> MultipleChunkQaOutput:
        instruct_config = language_config(
            input.language, self._merge_answers_instruct_configs
        )

        qa_outputs = self._single_chunk_qa.run_concurrently(
            (
                SingleChunkQaInput(
                    question=input.question, chunk=chunk, language=input.language
                )
                for chunk in input.chunks
            ),
            task_span,
        )
        final_answer = self._merge_answers(
            input.question, qa_outputs, instruct_config, task_span
        )

        return MultipleChunkQaOutput(
            answer=final_answer,
            subanswers=[
                Subanswer(
                    answer=qa_output.answer,
                    chunk=chunk,
                    highlights=qa_output.highlights,
                )
                for qa_output, chunk in zip(qa_outputs, input.chunks)
                if qa_output.answer
            ],
        )

    def _merge_answers(
        self,
        question: str,
        qa_outputs: Iterable[SingleChunkQaOutput],
        instruction_config: MergeAnswersInstructConfig,
        task_span: TaskSpan,
    ) -> Optional[str]:
        answers = [output.answer for output in qa_outputs if output.answer]
        if len(answers) == 0:
            return None
        elif len(answers) == 1:
            return answers[0]

        joined_answers = "\n".join(answers)
        return self._instruct(
            f"""{instruction_config.question_label}: {question}

{instruction_config.answers_label}:
{joined_answers}""",
            instruction_config,
            task_span,
        ).completion

    def _instruct(
        self,
        input: str,
        instruction_config: MergeAnswersInstructConfig,
        task_span: TaskSpan,
    ) -> CompleteOutput:
        prompt = self._model.to_instruct_prompt(
            instruction_config.instruction,
            input=input,
            response_prefix=f" {instruction_config.final_answer_label}",
        )
        return self._model.complete(
            CompleteInput(
                prompt=prompt, maximum_tokens=instruction_config.maximum_tokens
            ),
            task_span,
        )
