from statistics import mean
from typing import Sequence

from pydantic import BaseModel

from intelligence_layer.core.chunk import Chunk
from intelligence_layer.core.detect_language import Language
from intelligence_layer.core.evaluator import Evaluator, calculate_bleu, calculate_rouge
from intelligence_layer.core.task import Task
from intelligence_layer.core.tracer import Tracer


class LongContextSummarizeInput(BaseModel):
    """The input for a summarize-task for a text of any length.

    Attributes:
        text: A text of any length.
        language: The desired language of the summary. ISO 619 str with language e.g. en, fr, etc.
    """

    text: str
    language: Language = Language("en")


class PartialSummary(BaseModel):
    summary: str
    chunk: Chunk


class LongContextSummarizeOutput(BaseModel):
    """The output of a summarize-task for a text of any length.

    Attributes:
        partial_summaries: Chunk-wise summaries.
    """

    partial_summaries: Sequence[PartialSummary]


class SingleChunkSummarizeInput(BaseModel):
    """The input for a summarize-task that only deals with a single chunk.

    Attributes:
        chunk: The text chunk to be summarized.
        language: The desired language of the summary. ISO 619 str with language e.g. en, fr, etc.
    """

    chunk: Chunk
    language: Language = Language("en")


class SingleChunkSummarizeOutput(BaseModel):
    """The input of a summarize-task that only takes a single chunk.

    Attributes:
        summary: The summary generated by the task.
    """

    summary: str


class SingleChunkSummarizeEvaluation(BaseModel):
    """The evaluation of a single chunk summarization run.

    Attributes:
        bleu: TODO
        rouge: TODO
        output: The actual output from the task run
    """

    bleu: float
    rouge: float
    output: SingleChunkSummarizeOutput


class AggregatedSingleChunkSummarizeEvaluation(BaseModel):
    """The aggregated evaluation of a single chunk summarization implementation against a dataset.

    Attributes:
        aggregate_bleu: TODO
        aggregate_rouge: TODO
        evaluation: The actual evaluations
    """

    aggregate_bleu: float
    aggregate_rouge: float
    evaluations: Sequence[SingleChunkSummarizeEvaluation]


class SingleChunkSummarizeEvaluator(
    Evaluator[
        SingleChunkSummarizeInput,
        str,
        SingleChunkSummarizeEvaluation,
        AggregatedSingleChunkSummarizeEvaluation,
    ]
):
    def __init__(
        self, task: Task[SingleChunkSummarizeInput, SingleChunkSummarizeOutput]
    ) -> None:
        self.task = task

    def evaluate(
        self,
        input: SingleChunkSummarizeInput,
        tracer: Tracer,
        expected_output: str,
    ) -> SingleChunkSummarizeEvaluation:
        summary = self.task.run(input, tracer)
        bleu_score = calculate_bleu(summary.summary, expected_output)
        rouge_score = calculate_rouge(summary.summary, expected_output)

        return SingleChunkSummarizeEvaluation(
            bleu=bleu_score, rouge=rouge_score.recall, output=summary
        )

    def aggregate(
        self, evaluations: Sequence[SingleChunkSummarizeEvaluation]
    ) -> AggregatedSingleChunkSummarizeEvaluation:
        if len(evaluations) != 0:
            bleu_avg = mean(eval.bleu for eval in evaluations)
            rouge_avg = mean(eval.rouge for eval in evaluations)
        else:
            bleu_avg = 0.0
            rouge_avg = 0.0
        return AggregatedSingleChunkSummarizeEvaluation(
            aggregate_bleu=bleu_avg, aggregate_rouge=rouge_avg, evaluations=evaluations
        )
