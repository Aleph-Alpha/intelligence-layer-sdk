{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "\n",
    "Classification is a methodology that tries to match a text to the correct label. \n",
    "\n",
    "### Prompt based classification \n",
    "\n",
    "Prompt based classification is a methodology that relies purely on prompting the LLM in a specific way. \n",
    "\n",
    "### When should you use prompt based classification \n",
    "\n",
    "Some situations when you would use this methodology is when:\n",
    "- The labels are easily understood (they don't require explanation or examples)\n",
    "    \n",
    "    An example is sentiment analysis\n",
    "- The labels are not recognized by their semantic meaning\n",
    "    \n",
    "    E.g. Reasoning tasks like classifying contradictions\n",
    "- You don't have many examples\n",
    "\n",
    "### Example snippet \n",
    "Running the following code will instantiate a prompt based classifier, with a debug level for the log. \n",
    "Then it will classify the text given in \"ClassifyInput\".\n",
    "The contents of the debuglog will be shown below.\n",
    "The debuglog gives an overview of the steps taken to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from aleph_alpha_client import Client\n",
    "from intelligence_layer.classify import SingleLabelClassify, ClassifyInput\n",
    "from intelligence_layer.task import JsonDebugLogger\n",
    "\n",
    "text_to_classify = \"In the distant future, a space exploration party embarked on a thrilling journey to the uncharted regions of the galaxy. \\n\\\n",
    "    With excitement in their hearts and the cosmos as their canvas, they ventured into the unknown, discovering breathtaking celestial wonders. \\n\\\n",
    "    As they gazed upon distant stars and nebulas, they forged unforgettable memories that would forever bind them as pioneers of the cosmos.\"\n",
    "labels = [\"happy\", \"angry\", \"sad\"]\n",
    "client = Client(getenv(\"AA_TOKEN\"))\n",
    "task = SingleLabelClassify(client)\n",
    "input = ClassifyInput(\n",
    "    text=text_to_classify,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "debug_log = JsonDebugLogger(name=\"classify\")\n",
    "output = task.run(input, debug_log)\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n",
    "debug_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this implementation work\n",
    "For prompt based classification, we prompt the model multiple times with the text we want to classify and each of our classes. \n",
    "Instead of letting the model generate the class it thinks fits the text best, we ask it for the probability for each class.\n",
    "\n",
    "To further explain this, lets start with a more familiar case.\n",
    "The intuitive way to ask an LLM if it could label a text could be something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE)\n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=\"\").items[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then answer our question, and give us a class that it thinks fits the text. \n",
    "\n",
    "In the case of classification, however, we already have the classes beforehand.\n",
    "Because of this, all we are interested in is the probability the model would have guessed our specific classes.\n",
    "To get this probability, we can prompt the model with each of our classes and ask the model to return the logprobs for the text. \n",
    "\n",
    "In case of prompt based classification the prompt looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE)\n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=labels[0]).items[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have the same prompt, but with the class already filled in as a response.\n",
    "\n",
    "Our request will now not generate any tokens, but instead will just return us the logprobs that the class would be generated, given the previous tokens.\n",
    "\n",
    "```python\n",
    "CompletionRequest(\n",
    "    prompt=prompt_template.to_prompt(**kwargs),\n",
    "    maximum_tokens=0,\n",
    "    log_probs=0,\n",
    "    tokens=True,\n",
    "    echo=True,\n",
    ")\n",
    "```\n",
    "\n",
    "In the case of the classes \"Space exploration\" and \"Space party\", the logprobs per label might look something like the code snippet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import LogEntry\n",
    "result_objects = [log_entry for log_entry in debug_log.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Raw log probs per label\"]\n",
    "result_objects.pop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logprobs, we just need to do some calculations to turn them into our end score. \n",
    "\n",
    "To turn the logprobs into our end scores, first we normalize our probabilities. This will result in the following data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from intelligence_layer.classify import TreeNode\n",
    "from intelligence_layer.tree_visualizer import graph_nodes\n",
    "\n",
    "normalized_probs_logs = [log_entry.value for log_entry in debug_log.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs.pop()\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)\n",
    "\n",
    "graph = graph_nodes(root)\n",
    "display.display_svg(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the product of all the paths to get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example mentioned before is rather straightforward, but there are some situations when it isn't as obvious as a single token.\n",
    "\n",
    "What if we take some classes that have some overlap?\n",
    "In the following example some of the classes overlap in the tokens they have. This makes the calculation a bit more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.classify import SingleLabelClassify, ClassifyInput\n",
    "from intelligence_layer.task import LogEntry\n",
    "\n",
    "labels = [\"Space party\", \"Space exploration\", \"Space exploration party\"]\n",
    "task = SingleLabelClassify(client)\n",
    "input = ClassifyInput(\n",
    "    text=text_to_classify,\n",
    "    labels=labels\n",
    ")\n",
    "logger = JsonDebugLogger(name=\"classify\")\n",
    "output = task.run(input, logger)\n",
    "normalized_probs_logs = [log_entry.value for log_entry in logger.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs.pop()\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)\n",
    "\n",
    "graph = graph_nodes(root)\n",
    "display.display_svg(graph)\n",
    "print(\"End scores:\")\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the three classes have some overlapping tokens. \n",
    "In the graph above can be seen how the calculations would be done in this case. \n",
    "\n",
    "1. At the top, you can see that when there is only one token to choose from, the normalized score will always be 1.\n",
    "\n",
    "2. After that, the first choice is made between \"exploration\" and \"party\". \n",
    "\n",
    "3. If the choice of \"exploration\" is made, finally a choice has to be made between the \"endoftext\" token and \"party\".\n",
    "\n",
    "    This \"endoftext\" token is a token that large language models use internally to make their calculations.\n",
    "    As an end user, you generally shouldn't see this token.\n",
    "\n",
    "    In our case, this translates to choosing between: \"Space exploration party\" and just \"Space exploration\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10-intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
