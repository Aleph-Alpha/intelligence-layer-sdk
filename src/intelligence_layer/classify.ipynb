{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "\n",
    "Classification is a methodology that tries to match a text to the correct label. \n",
    "\n",
    "### Prompt based classification \n",
    "\n",
    "Prompt based classification is a methodology that relies purely on prompting the LLM in a specific way. \n",
    "\n",
    "### When should you use prompt based classification \n",
    "\n",
    "Some situations when you would use this methodology is when:\n",
    "- The labels are easily understood (they don't require explanation or examples)\n",
    "    \n",
    "    An example is sentiment analysis\n",
    "- The labels are not recognized by their semantic meaning\n",
    "    \n",
    "    E.g. Reasoning tasks like classifying contradictions\n",
    "- You don't have many examples\n",
    "\n",
    "### Example snippet \n",
    "Running the following code will instantiate a prompt based classifier, with a debug level for the log. \n",
    "Then it will classify the text given in \"ClassifyInput\".\n",
    "The contents of the debuglog will be shown below.\n",
    "The debuglog gives an overview of the steps taken to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from aleph_alpha_client import Client\n",
    "from intelligence_layer.classify import SingleLabelClassify, ClassifyInput\n",
    "from pprint import pprint\n",
    "\n",
    "text_to_classify = \"Beneath a canopy of stars, aliens of all kinds danced and laughed, celebrating unity in the cosmic night.\\n\\\n",
    "The asteroid throbbed with their energy, an unforgettable space party at the heart of the galaxy.\"\n",
    "labels = [\"Space party\", \"Space exploration\", \"Something else\", \"Something very different and long\", \"Something very cool and long\"]\n",
    "client = Client(getenv(\"AA_API_TOKEN\"))\n",
    "task = SingleLabelClassify(client, \"info\")\n",
    "input = ClassifyInput(\n",
    "    text=text_to_classify, \n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "output = task.run(input)\n",
    "pprint(output.debug_log.model_dump())\n",
    "pprint(output.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this implemetation work\n",
    "For prompt based classification, we prompt the model multiple times with the text we want to classify and each of our classes. \n",
    "Instead of letting the model generate the class it thinks fits the text best, we ask it for the probability for each class.\n",
    "\n",
    "To further explain this, lets start with a more familiar case.\n",
    "The intuitive way to ask an LLM if it could label a text could be something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE) \n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=\"\").items[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then answer our question, and give us a class that it thinks fits the text. \n",
    "\n",
    "In the case of classification, however, we already have the classes beforehand.\n",
    "Because of this, all we are interested in is the probability the model would have guessed our specific classes.\n",
    "To get this probability, we can prompt the model with each of our classes and ask the model to return the logprobs for the text. \n",
    "\n",
    "In case of prompt based classification the prompt looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE) \n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=labels[0]).items[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have pre-emptively filled in the class in our prompt.\n",
    "\n",
    "Our request will now not generate any tokens, but instead will just return us the logprobs that the class would be generated, given the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??SingleLabelClassify._complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the classes \"Space exploration\" and \"Space party\", the logprobs per label might look something like the code snippet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_objects = [log_entry for log_entry in output.debug_log.root if log_entry.message == \"Raw log probs per label\"]\n",
    "pprint(result_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logprobs, we just need to do some calculations to turn them into our end score. \n",
    "\n",
    "To turn the logprobs into our end scores, first we normalize our probabilities. This will result in the following data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import graphviz  \n",
    "from IPython import display\n",
    "import uuid\n",
    "\n",
    "from intelligence_layer.classify import TreeNode, Probability\n",
    "\n",
    "\n",
    "normalized_probs_logs = [log_entry.value for log_entry in output.debug_log.root if log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs.pop()\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)\n",
    "\n",
    "def add_node(graph, node: TreeNode) -> str:\n",
    "    new_node_id = str(uuid.uuid4())\n",
    "    graph.node(new_node_id, label=f\"{node.token.token}:{str(round(node.prob, 3))}\")\n",
    "    return new_node_id\n",
    "\n",
    "def graph_path(graph, node: TreeNode, parent_id: Optional[str] = None):\n",
    "    for child in node.children:\n",
    "        new_node_id = add_node(graph, child)\n",
    "        graph.edge(parent_id, new_node_id)\n",
    "        graph_path(graph, child, new_node_id) \n",
    "\n",
    "def graph_nodes(root: TreeNode):\n",
    "    graph = graphviz.Digraph('normalized probabilities', node_attr={'shape': 'plaintext'})\n",
    "    graph.graph_attr['rankdir'] = 'TB'  \n",
    "    graph.edge_attr.update(arrowhead='vee', arrowsize='1')\n",
    "    for child in root.children:\n",
    "        parent_id = add_node(graph, child)\n",
    "        graph_path(graph, child, parent_id)\n",
    "    return graph \n",
    "\n",
    "graph = graph_nodes(root)\n",
    "display.display_svg(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the product of all the paths to get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10-intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
