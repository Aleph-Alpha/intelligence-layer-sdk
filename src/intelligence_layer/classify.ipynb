{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt based classification \n",
    "\n",
    "Prompt based classification is a methodology that relies purely on prompting the LLM in a specific way. \n",
    "\n",
    "### When should you use prompt based classification \n",
    "\n",
    "Some situations when you would use this methodology is when:\n",
    "- The labels are easily understood (they don't require explanation or examples)\n",
    "    \n",
    "    An example is sentiment analysis\n",
    "- The labels are not recognized by their semantic meaning\n",
    "    \n",
    "    E.g. Reasoning tasks like classifying contradictions\n",
    "- You don't have many examples\n",
    "\n",
    "### Example snippet \n",
    "Running the following code will instantiate a prompt based classifier, with a debug level for the log. \n",
    "Then it will classify the text given in \"ClassifyInput\".\n",
    "The contents of the debuglog will be shown below.\n",
    "The debuglog gives an overview of the steps taken to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from aleph_alpha_client import Client\n",
    "from intelligence_layer.classify import SingleLabelClassify, ClassifyInput\n",
    "from pprint import pprint\n",
    "\n",
    "text_to_classify = \"In the distant future, a space exploration party embarked on a thrilling journey to the uncharted regions of the galaxy. \\n\\\n",
    "    With excitement in their hearts and the cosmos as their canvas, they ventured into the unknown, discovering breathtaking celestial wonders. \\n\\\n",
    "    As they gazed upon distant stars and nebulas, they forged unforgettable memories that would forever bind them as pioneers of the cosmos.\"\n",
    "labels = [\"happy\", \"angry\", \"sad\"]\n",
    "client = Client(getenv(\"AA_TOKEN\"))\n",
    "task = SingleLabelClassify(client, \"info\")\n",
    "input = ClassifyInput(\n",
    "    text=text_to_classify, \n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "output = task.run(input)\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n",
    "output.debug_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this implemetation work\n",
    "For prompt based classification, we prompt the model multiple times with the text we want to classify and each of our classes. \n",
    "Instead of letting the model generate the class it thinks fits the text best, we ask it for the probability for each class.\n",
    "\n",
    "To further explain this, lets start with a more familiar case.\n",
    "The intuitive way to ask an LLM if it could label a text could be something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE) \n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=\"\").items[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then answer our question, and give us a class that it thinks fits the text. \n",
    "\n",
    "In the case of classification, however, we already have the classes beforehand.\n",
    "Because of this, all we are interested in is the probability the model would have guessed our specific classes.\n",
    "To get this probability, we can prompt the model with each of our classes and ask the model to return the logprobs for the text. \n",
    "\n",
    "In case of prompt based classification the prompt looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE) \n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=labels[0]).items[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have the same prompt, but with the class already filled in as a response.\n",
    "\n",
    "Our request will now not generate any tokens, but instead we ask the model: \"How likely is it that you would have given the following response, given the text?\". \n",
    "\n",
    "```python\n",
    "CompletionRequest(\n",
    "    prompt=prompt_template.to_prompt(**kwargs),\n",
    "    maximum_tokens=0,\n",
    "    log_probs=0,\n",
    "    tokens=True,\n",
    "    echo=True,\n",
    ")\n",
    "```\n",
    "\n",
    "In the case of the classes \"Space exploration\" and \"Space party\", the logprobs per label might look something like the code snippet below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import InfoEnabledLog \n",
    "result_objects = [log_entry for log_entry in output.debug_log.root if log_entry.message == \"Raw log probs per label\"]\n",
    "InfoEnabledLog(root=result_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our logprobs, all we have to do is take the exponent and normalize the scores to get our endresults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example mentioned before is rather straightforward, but there are some situations when it isn't as obvious as normalizing a single token per class.\n",
    "\n",
    "What if we take some classes that contain multiple tokens with some overlap?\n",
    "\n",
    "The following example shows how the calculations are done in the case of three classes: \n",
    "```python \n",
    "[\"Space party\", \"Space exploration\", \"Space exploration party\"]\n",
    "```\n",
    "The overlap in the tokens makes the calculation a bit less straightforward. \n",
    "\n",
    "The following graph shows how the tokens relate to each other and their possible probabilities:\n",
    "\n",
    "![Alt text](classify_tokens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the graph:\n",
    "\n",
    "1. Starting at the top of the tree, when there's only one token to choose from, the normalized score for that token will always be 1. This means that when there's no ambiguity, the score is definitive.\n",
    "\n",
    "2. Moving down the tree, the first pivotal decision arises, where one must choose between \"exploration\" and \"party.\" This choice sets the direction for subsequent evaluations.\n",
    "\n",
    "3. If \"exploration\" is chosen, we encounter a final decision point, where we select between the \"endoftext\" token and \"party.\"\n",
    "\n",
    "* It's important to note that the \"endoftext\" token is an internal component used by large language models for their calculations. Typically, end users aren't be exposed to this token. \n",
    "\n",
    "* In our context, this choice translates into deciding between \"Space exploration party\" and \"Space exploration\" as the two possible outcomes.\n",
    "\n",
    "Now if we take the product of the paths to all th end classes the results are:\n",
    "\n",
    "Space party: 0.0\n",
    "\n",
    "Space exploration: 0.4\n",
    "\n",
    "Space exploration party: 0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10-intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
