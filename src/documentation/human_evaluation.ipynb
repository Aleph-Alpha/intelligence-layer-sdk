{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from intelligence_layer.connectors import (\n",
    "    ArgillaRatingEvaluation,\n",
    "    DefaultArgillaClient,\n",
    "    Field,\n",
    "    LimitedConcurrencyClient,\n",
    "    Question,\n",
    "    RecordData,\n",
    ")\n",
    "from intelligence_layer.core import (\n",
    "    CompleteOutput,\n",
    "    Instruct,\n",
    "    InstructInput,\n",
    "    LuminousControlModel,\n",
    ")\n",
    "from intelligence_layer.evaluation import (\n",
    "    AggregationLogic,\n",
    "    Aggregator,\n",
    "    ArgillaEvaluationLogic,\n",
    "    ArgillaEvaluator,\n",
    "    AsyncFileEvaluationRepository,\n",
    "    Example,\n",
    "    FileAggregationRepository,\n",
    "    FileDatasetRepository,\n",
    "    FileRunRepository,\n",
    "    RecordDataSequence,\n",
    "    Runner,\n",
    "    SuccessfulExampleOutput,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = LimitedConcurrencyClient.from_env()\n",
    "\n",
    "\n",
    "REPOSITORY_ROOT_PATH = Path(\"human-eval-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation using the Intelligence Layer\n",
    "\n",
    "Although there are a variety of ways to automate the evaluation of LLM-based tasks, sometimes it is still necessary to get a human opinion.\n",
    "To make this as painless as possible, we have integrated an [Argilla](https://argilla.io/)-Evaluator into the intelligence layer.\n",
    "This notebook serves as a quick start guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "This notebook expects that you have added your Aleph Alpha token to your .env file.\n",
    "Additionally you need to add the `ARGILLA_API_URL` and `ARGILLA_API_KEY` from env.sample to your .env file. \n",
    "Next, run\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "``` \n",
    "\n",
    "from the intelligence layer base directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you go to `localhost:6900` and you are prompted to enter a username and password, use:\n",
    "- username: `argilla`\n",
    "- password: `1234`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "This notebook is designed such that the creation of the dataset, the submission to Argilla and the aggregation of the Argilla evaluations do not have to be done in a single session.\n",
    "\n",
    "As a result, the data repositories are redefined for each step and we use file-based repositories that persist the data. If you run all steps in a single session, you can use InMemory-based repositories and reuse the same repository object for multiple steps.\n",
    "\n",
    "Running this notebook creates a `human-eval-data` folder, which will be deleted if you run the whole notebook to completion. It also creates the `test-human-eval` Argilla workspace, which will also be deleted afterwards.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Repository definition\n",
    "First we need to define our dataset. Here we use an [Instruction Dataset](https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=0) from Huggingface. Before we can use it for human eval, we need to make an intelligence layer dataset repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingfaceH4/instruction-dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the dataset a bit. It consists of prompts, example completions and metadata for 327 examples. Since we are doing human eval, for now we only need the prompt and corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"meta\"][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now build a single `Example` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Example(\n",
    "    input=InstructInput(instruction=dataset[\"prompt\"][0], input=None),\n",
    "    expected_output=None,\n",
    "    id=str(dataset[\"meta\"][0][\"id\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset repository, we can either use a `FileDatasetRepository` or an `InMemoryDatasetRepository`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5\n",
    "assert num_examples <= len(dataset)\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=InstructInput(instruction=dataset[\"prompt\"][i], input=None),\n",
    "            expected_output=None,\n",
    "            id=str(dataset[\"meta\"][i][\"id\"]),\n",
    "        )\n",
    "        for i in range(num_examples)\n",
    "    ],\n",
    "    dataset_name=\"human-evaluation-dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Setup\n",
    "\n",
    "We use an `Instruction` task to run the examples in our dataset.\n",
    "In addition, we define a `Runner` to generate the completions from the model for our dataset\n",
    "and a `RunRepository` to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LuminousControlModel(name=\"luminous-base-control\", client=client)\n",
    "task = Instruct(model=model)\n",
    "\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "# either remember the id from before (dataset.id) or retrieve as below\n",
    "dataset_id = [\n",
    "    dataset.id\n",
    "    for dataset in dataset_repository.datasets()\n",
    "    if dataset.name == \"human-evaluation-dataset\"\n",
    "][0]\n",
    "dataset_repository.datasets()\n",
    "run_repository = FileRunRepository(REPOSITORY_ROOT_PATH)\n",
    "runner = Runner(task, dataset_repository, run_repository, \"instruct-run\")\n",
    "\n",
    "run_overview = runner.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our evaluation we want a float score $s \\in [1,5]$ describing the model performance.\n",
    "We define this as an `InstructAggregatedEvaluation`, which will be used in our aggregation later.\n",
    "\n",
    "We also define the `InstructEvaluation`, which represents an evaluation of a single entry, which we will aggregate later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructAggregatedEvaluation(BaseModel):\n",
    "    general_rating: float | None\n",
    "    fluency: float | None\n",
    "    evaluated_examples: int\n",
    "\n",
    "\n",
    "class InstructEvaluation(BaseModel):\n",
    "    general_rating: float\n",
    "    fluency: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to define our human evaluation. This is done with `Questions` and `Fields`.  \n",
    "`Fields` define what a user has to evaluate. In our example, this will be the model input (Instruction) and output (Model Completion).  \n",
    "`Questions` are what a user has to answer in order to evaluate the `Fields`. The `name` property will later be used to access the human ratings.  \n",
    "Both of these are passed to the `ArgillaEvaluationLogic` to create `RecordData` to convert data back and forth from Argilla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(\n",
    "        name=\"general_rating\",  # name of the field in program, used for retrieval later\n",
    "        title=\"Rating\",  # name shown to the user\n",
    "        description=\"Rate this completion on a scale from 1 to 5\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "    Question(\n",
    "        name=\"fluency\",\n",
    "        title=\"Fluency\",\n",
    "        description=\"How fluent is the completion?\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "]\n",
    "\n",
    "fields = {\n",
    "    \"input\": Field(name=\"input\", title=\"Instruction\"),\n",
    "    \"output\": Field(name=\"output\", title=\"Model Completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our defined fields and questions will look like this:\n",
    "![Argilla Interface](../../assets/argilla_interface.png)\n",
    "\n",
    "We can now define our `InstructArgillaEvaluationLogic` to translate our data to specific Argilla formats .\n",
    "The logic has to implement the two abstract methods `to_record` and `from_record`.\n",
    "Lets look at the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ArgillaEvaluationLogic.to_record)\n",
    "print(\"-\" * 100)\n",
    "help(ArgillaEvaluationLogic.from_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of performing the evaluation, the `ArgillaEvaluationLogic` is responsible for converting the evaluation data to a format that is accepted by Argilla. During the evaluation, these records will simply be submitted to Argilla and retrieved later.\n",
    "We will now create everything we need to submit these evaluations to our Argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructArgillaEvaluationLogic(\n",
    "    ArgillaEvaluationLogic[\n",
    "        InstructInput,\n",
    "        CompleteOutput,\n",
    "        None,\n",
    "        InstructEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def to_record(\n",
    "        self,\n",
    "        example: Example[InstructInput, None],\n",
    "        *example_outputs: SuccessfulExampleOutput[CompleteOutput],\n",
    "    ) -> RecordDataSequence:\n",
    "        return RecordDataSequence(\n",
    "            records=[\n",
    "                RecordData(\n",
    "                    content={\n",
    "                        self.fields[\"input\"].name: example.input.instruction,\n",
    "                        self.fields[\"output\"].name: example_outputs[\n",
    "                            0\n",
    "                        ].output.completion,\n",
    "                    },\n",
    "                    example_id=example.id,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def from_record(self, argilla_evaluation: ArgillaRatingEvaluation) -> InstructEvaluation:\n",
    "        return InstructEvaluation(\n",
    "            general_rating=argilla_evaluation.responses[\"general_rating\"],\n",
    "            fluency=argilla_evaluation.responses[\"fluency\"],\n",
    "        )\n",
    "\n",
    "\n",
    "argilla_client = DefaultArgillaClient()\n",
    "workspace_id = argilla_client.ensure_workspace_exists(\"test-human-eval\")\n",
    "\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "run_repository = FileRunRepository(REPOSITORY_ROOT_PATH)\n",
    "evaluation_repository = AsyncFileEvaluationRepository(REPOSITORY_ROOT_PATH)\n",
    "\n",
    "eval_logic = InstructArgillaEvaluationLogic(fields, questions)\n",
    "evaluator = ArgillaEvaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"instruct-evaluation\",\n",
    "    eval_logic,\n",
    "    argilla_client=argilla_client,\n",
    "    workspace_id=workspace_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the `ArgillaEvaluator`, the `sumit` methods posts the records to the Argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either remember the id from before (run_overview.id) or retrieve as below\n",
    "run_id = [\n",
    "    overview.id\n",
    "    for overview in run_repository.run_overviews()\n",
    "    if overview.description == \"instruct-run\"\n",
    "][0]\n",
    "\n",
    "\n",
    "partial_eval_overview = evaluator.submit(run_id)\n",
    "print(partial_eval_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to perform an aggregation right now, it will have no evaluations, as none of the submitted records were evaluated by humans through Argilla yet.  \n",
    "The next steps fetches only results that have been evaluated already\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Sometimes it is best to split up the human evaluation effort into multiple people. To best facilitate this, it is possible to split up the dataset by giving them labels.\n",
    "Our Argilla client offers an easy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_id = partial_eval_overview.id\n",
    "argilla_client.split_dataset(eval_id, n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These splits can then be filered by, as shown below.  \n",
    "<img src=\"../../assets/argilla_splits.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "To finish the evaluation, we can retrieve the evaluated examples as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_repository = AsyncFileEvaluationRepository(REPOSITORY_ROOT_PATH)\n",
    "\n",
    "# either remember the id from before (eval_overview.id) or retrieve as below\n",
    "eval_id = [\n",
    "    overview.id\n",
    "    for overview in evaluation_repository.partial_evaluation_overviews()\n",
    "    if overview.description == \"instruct-evaluation\"\n",
    "][0]\n",
    "\n",
    "evaluation_overview = evaluator.retrieve(eval_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that all examples that are not yet evaluated in argilla are noted as `failed_examples` and not passed to the next step.\n",
    "\n",
    "---\n",
    "\n",
    "For the Aggregation, we first need to define our `AggregationLogic` that takes our previously defined types as input and output. Here, we use `InstructEvaluation` and `InstructAggregatedEvaluation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructArgillaAggregationLogic(\n",
    "    AggregationLogic[InstructEvaluation, InstructAggregatedEvaluation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self,\n",
    "        evaluations: Iterable[InstructEvaluation],\n",
    "    ) -> InstructAggregatedEvaluation:\n",
    "        evaluations = list(evaluations)\n",
    "\n",
    "        if len(evaluations) == 0:  # if no evaluations were submitted, return\n",
    "            return InstructAggregatedEvaluation(\n",
    "                general_rating=None,\n",
    "                fluency=None,\n",
    "                evaluated_examples=0,\n",
    "            )\n",
    "\n",
    "        general_rating = sum(\n",
    "            evaluation.general_rating for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        fluency = sum(evaluation.fluency for evaluation in evaluations) / len(\n",
    "            evaluations\n",
    "        )\n",
    "\n",
    "        return InstructAggregatedEvaluation(\n",
    "            general_rating=general_rating,\n",
    "            fluency=fluency,\n",
    "            evaluated_examples=len(evaluations),\n",
    "        )\n",
    "\n",
    "\n",
    "aggregation_logic = InstructArgillaAggregationLogic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can define our `Aggregator` and aggregate all evaluations. This step is the same as non-human evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_repository = FileAggregationRepository(REPOSITORY_ROOT_PATH)\n",
    "\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"instruct-aggregation\",\n",
    "    aggregation_logic,\n",
    ")\n",
    "\n",
    "output = aggregator.aggregate_evaluation(eval_id)\n",
    "print(output.statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! WARNING ! This deletes the \"test-human-eval\" argilla workspace and the \"human-eval-data\" folder.\n",
    "argilla_client.delete_workspace(workspace_id)\n",
    "\n",
    "shutil.rmtree(REPOSITORY_ROOT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
