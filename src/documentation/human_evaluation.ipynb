{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable, cast\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from intelligence_layer.connectors import (\n",
    "    ArgillaEvaluation,\n",
    "    DefaultArgillaClient,\n",
    "    Field,\n",
    "    LimitedConcurrencyClient,\n",
    "    Question,\n",
    "    RecordData,\n",
    "    ArgillaEvaluation,\n",
    ")\n",
    "from intelligence_layer.core import (\n",
    "    CompleteOutput,\n",
    "    Instruct,\n",
    "    InstructInput,\n",
    "    LuminousControlModel,\n",
    ")\n",
    "from intelligence_layer.evaluation import (\n",
    "    AggregationLogic,\n",
    "    ArgillaEvaluationLogic,\n",
    "    ArgillaEvaluator,\n",
    "    Example,\n",
    "    FileAggregationRepository,\n",
    "    FileDatasetRepository,\n",
    "    FileRunRepository,\n",
    "    RecordDataSequence,\n",
    "    Runner,\n",
    "    SuccessfulExampleOutput,\n",
    "    AsyncFileEvaluationRepository,\n",
    "    Aggregator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = LimitedConcurrencyClient.from_env()\n",
    "\n",
    "\n",
    "REPOSITORY_ROOT_PATH = Path(\"human-eval-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation using the Intelligence Layer\n",
    "\n",
    "Although there are a variety of ways to automate the evaluation of LLM-based tasks, sometimes it is still necessary to get a human opinion.\n",
    "To make this as painless as possible, we have integrated an [Argilla](https://argilla.io/)-Evaluator into the intelligence layer.\n",
    "This notebook serves as a quick start guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "This notebook expects that you have added your Aleph Alpha token to your .env file.\n",
    "Additionally you need to add the `ARGILLA_API_URL` and `ARGILLA_API_KEY` from env.sample to your .env file. \n",
    "Next, run\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "``` \n",
    "\n",
    "from the intelligence layer base directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you go to `localhost:6900` and you are prompted to enter a username and password, use:\n",
    "- username: `argilla`\n",
    "- password: `1234`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "This notebook is designed such that the creation of the dataset, the submission to Argilla and the aggregation of the Argilla evaluations do not have to be done in a single session.\n",
    "\n",
    "As a result, the data repositories are redefined for each step and we use file-based repositories that persist the data. If you run all steps in a single session, you can use InMemory-based repositories and reuse the same repository object for multiple steps.\n",
    "\n",
    "Running this notebook creates a `human-eval-data` folder, which will be deleted if you run the whole notebook to completion. It also creates the `test-human-eval` Argilla workspace, which will also be deleted afterwards.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Repository definition\n",
    "First we need to define our dataset. Here we use an [Instruction Dataset](https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=0) from Huggingface. Before we can use it for human eval, we need to make an intelligence layer dataset repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingfaceH4/instruction-dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the dataset a bit. It consists of prompts, example completions and metadata for 327 examples. Since we are doing human eval, for now we only need the prompt and corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion', 'meta'],\n",
      "    num_rows: 327\n",
      "})\n",
      "dict_keys(['id', 'motivation_app', 'prompt', 'input', 'completion', 'source', 'category', 'subcategory'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"meta\"][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now build a single `Example` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Example(\n",
    "    input=InstructInput(instruction=dataset[\"prompt\"][0], input=None),\n",
    "    expected_output=None,\n",
    "    id=str(dataset[\"meta\"][0][\"id\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset repository, we can either use a `FileDatasetRepository` or an `InMemoryDatasetRepository`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5\n",
    "assert num_examples <= len(dataset)\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=InstructInput(instruction=dataset[\"prompt\"][i], input=None),\n",
    "            expected_output=None,\n",
    "            id=str(dataset[\"meta\"][i][\"id\"]),\n",
    "        )\n",
    "        for i in range(num_examples)\n",
    "    ],\n",
    "    dataset_name=\"human-evaluation-dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human-evaluation-dataset'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Setup\n",
    "\n",
    "We use an `Instruction` task to run the examples in our dataset.\n",
    "In addition, we define a `Runner` to generate the completions from the model for our dataset\n",
    "and a `RunRepository` to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: 5it [00:05,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LuminousControlModel(name=\"luminous-base-control\", client=client)\n",
    "task = Instruct(model=model)\n",
    "\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "# either remember the id from before (dataset.id) or retrieve as below\n",
    "dataset_id = [\n",
    "    dataset.id\n",
    "    for dataset in dataset_repository.datasets()\n",
    "    if dataset.name == \"human-evaluation-dataset\"\n",
    "][0]\n",
    "dataset_repository.datasets()\n",
    "run_repository = FileRunRepository(REPOSITORY_ROOT_PATH)\n",
    "runner = Runner(task, dataset_repository, run_repository, \"instruct-run\")\n",
    "\n",
    "run_overview = runner.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our evaluation we want a float score $s \\in [1,5]$ describing the model performance.\n",
    "We define this as an `InstructAggregatedEvaluation`, which will be used in our aggregation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructAggregatedEvaluation(BaseModel):\n",
    "    general_rating: float | None\n",
    "    fluency: float | None\n",
    "    evaluated_examples: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to define our human evaluation. This is done with `Questions` and `Fields`.  \n",
    "`Fields` define what a user has to evaluate. In our example, this will be the model input (Instruction) and output (Model Completion). Note that the field names have to match the content keys from the `RecordData` which we will define later in our `InstructArgillaEvaluationLogic`.  \n",
    "`Questions` are what a user has to answer in order to evaluate the `Fields`. The `name` property will later be used to access the human ratings in the aggregation step. In our case we ask how complete and how fluent the completions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(\n",
    "        name=\"general_rating\",  # name of the field in program, used for retrieval later\n",
    "        title=\"Rating\",  # name shown to the user\n",
    "        description=\"Rate this completion on a scale from 1 to 5\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "    Question(\n",
    "        name=\"fluency\",\n",
    "        title=\"Fluency\",\n",
    "        description=\"How fluent is the completion?\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "]\n",
    "\n",
    "fields = {\n",
    "    \"input\": Field(name=\"input\", title=\"Instruction\"),\n",
    "    \"output\": Field(name=\"output\", title=\"Model Completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our defined fields and questions will look like this:\n",
    "![Argilla Interface](../../assets/argilla_interface.png)\n",
    "\n",
    "We can now define our `InstructArgillaEvaluationLogic` and `InstructArgillaAggregationLogic`.\n",
    "They have to implement the two abstract methods `_to_record` and `aggregate` respectively.\n",
    "Lets look at the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_record in module intelligence_layer.evaluation.evaluation.evaluator.argilla_evaluator:\n",
      "\n",
      "to_record(self, example: intelligence_layer.evaluation.dataset.domain.Example, *output: intelligence_layer.evaluation.run.domain.SuccessfulExampleOutput) -> intelligence_layer.evaluation.evaluation.evaluator.argilla_evaluator.RecordDataSequence\n",
      "    This method is responsible for translating the `Example` and `Output` of the task to :class:`RecordData`\n",
      "    \n",
      "    \n",
      "    Args:\n",
      "        example: The example to be translated.\n",
      "        output: The output of the example that was run.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Help on function aggregate in module intelligence_layer.evaluation.aggregation.aggregator:\n",
      "\n",
      "aggregate(self, evaluations: Iterable[+Evaluation]) -> +AggregatedEvaluation\n",
      "    `Evaluator`-specific method for aggregating individual `Evaluations` into report-like `Aggregated Evaluation`.\n",
      "    \n",
      "    This method is responsible for taking the results of an evaluation run and aggregating all the results.\n",
      "    It should create an `AggregatedEvaluation` class and return it at the end.\n",
      "    \n",
      "    Args:\n",
      "        evaluations: The results from running `eval_and_aggregate_runs` with a :class:`Task`.\n",
      "    \n",
      "    Returns:\n",
      "        The aggregated results of an evaluation run with a :class:`Dataset`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ArgillaEvaluationLogic.to_record)\n",
    "print(\"-\" * 100)\n",
    "help(AggregationLogic.aggregate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of performing the evaluation, the `ArgillaEvaluationLogic` is responsible for converting the evaluation data to a format that is accepted by Argilla. During the evaluation, these records will simply be submitted to Argilla.  \n",
    "We will now create everything we need to submit these evaluations to our Argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InstructEvaluation(BaseModel):\n",
    "    general_rating: float\n",
    "    fluency: float\n",
    "\n",
    "class InstructArgillaEvaluationLogic(\n",
    "    ArgillaEvaluationLogic[\n",
    "        InstructInput,\n",
    "        CompleteOutput,\n",
    "        None,\n",
    "        InstructEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def to_record(\n",
    "        self,\n",
    "        example: Example[InstructInput, None],\n",
    "        *example_outputs: SuccessfulExampleOutput[CompleteOutput],\n",
    "    ) -> RecordDataSequence:\n",
    "        return RecordDataSequence(\n",
    "            records=[\n",
    "                RecordData(\n",
    "                    content={\n",
    "                        \"input\": example.input.instruction,\n",
    "                        \"output\": example_outputs[0].output.completion,\n",
    "                    },\n",
    "                    example_id=example.id,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def from_record(self, argilla_evaluation: ArgillaEvaluation) -> InstructEvaluation:\n",
    "        return InstructEvaluation(general_rating=argilla_evaluation.responses[\"general_rating\"], fluency=argilla_evaluation.responses[\"fluency\"])\n",
    "\n",
    "\n",
    "argilla_client = DefaultArgillaClient()\n",
    "workspace_id = argilla_client.ensure_workspace_exists(\"test-human-eval\")\n",
    "\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "run_repository = FileRunRepository(REPOSITORY_ROOT_PATH)\n",
    "evaluation_repository = AsyncFileEvaluationRepository(\n",
    "    REPOSITORY_ROOT_PATH\n",
    ")\n",
    "\n",
    "eval_logic = InstructArgillaEvaluationLogic(fields, questions)\n",
    "evaluator = ArgillaEvaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"instruct-evaluation\",\n",
    "    eval_logic,\n",
    "    argilla_client=argilla_client,\n",
    "    workspace_id=workspace_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the `ArgillaEvaluator`, the `evaluate_runs` methods posts the records to the Argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Overview ID = 41d5a7ac-c7f1-4e22-9ef3-538bf98ecbc5\n",
      "Start time = 2024-05-13 18:22:42.271727\n",
      "Submitted Evaluations = 5\n",
      "Description = \"instruct-evaluation\"\n",
      "Run Overviews={\n",
      "Run Overview ID = 5fece010-9d8f-4eb3-bf55-4abda031ed25\n",
      "Dataset ID = 2f40e028-f4ea-4018-bdcb-24e62b38d057\n",
      "Start time = 2024-05-13 16:16:21.485698+00:00\n",
      "End time = 2024-05-13 16:16:34.460946+00:00\n",
      "Failed example count = 0\n",
      "Successful example count = 5\n",
      "Description = \"instruct-run\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# either remember the id from before (run_overview.id) or retrieve as below\n",
    "run_id = [\n",
    "    overview.id\n",
    "    for overview in run_repository.run_overviews()\n",
    "    if overview.description == \"instruct-run\"\n",
    "][0]\n",
    "\n",
    "\n",
    "partial_eval_overview = evaluator.submit(run_id)\n",
    "print(partial_eval_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the evaluation says that 5 examples were successfully evaluated, no real evaluation has happened yet.  \n",
    "If we try to perform an aggregation right now, it will have no evaluations, as none of the submitted records were evaluated by humans through Argilla yet.  \n",
    "The aggregation fetches only the results that were already evaluated.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Sometimes it is best to split up the human evaluation effort into multiple people. To best facilitate this, it is possible to split up the dataset by giving them labels.\n",
    "Our Argilla client offers an easy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_id = partial_eval_overview.id\n",
    "argilla_client.split_dataset(eval_id, n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These splits can then be filered by, as shown below.  \n",
    "<img src=\"../../assets/argilla_splits.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "For the Aggregation, we first need to define our `AggregationLogic` that has to take an `ArgillaEvaluation` as an input. As output, we use the `InstructAggregatedEvaluation` we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructArgillaAggregationLogic(\n",
    "    AggregationLogic[ArgillaEvaluation, InstructAggregatedEvaluation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self,\n",
    "        evaluations: Iterable[ArgillaEvaluation],\n",
    "    ) -> InstructAggregatedEvaluation:\n",
    "        evaluations = list(evaluations)\n",
    "\n",
    "        if len(evaluations) == 0:  # if no evaluations were submitted, return\n",
    "            return InstructAggregatedEvaluation(\n",
    "                general_rating=None,\n",
    "                fluency=None,\n",
    "                evaluated_examples=0,\n",
    "            )\n",
    "\n",
    "        general_rating = sum(\n",
    "            cast(float, evaluation.responses[\"general_rating\"])\n",
    "            for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        fluency = sum(\n",
    "            cast(float, evaluation.responses[\"fluency\"]) for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        return InstructAggregatedEvaluation(\n",
    "            general_rating=general_rating,\n",
    "            fluency=fluency,\n",
    "            evaluated_examples=len(evaluations),\n",
    "        )\n",
    "\n",
    "\n",
    "aggregation_logic = InstructArgillaAggregationLogic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can define our `ArgillaAggregator` and retrieve the aggregation of all records that have been evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Repository does not contain an evaluation with id: 41d5a7ac-c7f1-4e22-9ef3-538bf98ecbc5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# either remember the id from before (eval_overview.id) or retrieve as below\u001b[39;00m\n\u001b[1;32m      5\u001b[0m eval_id \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     overview\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m overview \u001b[38;5;129;01min\u001b[39;00m evaluation_repository\u001b[38;5;241m.\u001b[39mpartial_evaluation_overviews()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m overview\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruct-evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m ][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m evaluation_overview \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m aggregator \u001b[38;5;241m=\u001b[39m Aggregator(\n\u001b[1;32m     14\u001b[0m     evaluation_repository,\n\u001b[1;32m     15\u001b[0m     aggregation_repository,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruct-aggregation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     aggregation_logic,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m aggregator\u001b[38;5;241m.\u001b[39maggregate_evaluation(eval_id)\n",
      "File \u001b[0;32m~/intelligence-layer/src/intelligence_layer/evaluation/evaluation/evaluator/argilla_evaluator.py:190\u001b[0m, in \u001b[0;36mArgillaEvaluator.retrieve\u001b[0;34m(self, evaluation_id)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evaluation \u001b[38;5;129;01min\u001b[39;00m evaluations:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_repository\u001b[38;5;241m.\u001b[39mstore_example_evaluation(evaluation)\n\u001b[1;32m    189\u001b[0m num_failed_evaluations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_repository\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfailed_example_evaluations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_evaluation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    194\u001b[0m num_not_yet_evaluated_evals \u001b[38;5;241m=\u001b[39m partial_overview\u001b[38;5;241m.\u001b[39msubmitted_evaluation_count \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    195\u001b[0m     evaluations\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    198\u001b[0m overview \u001b[38;5;241m=\u001b[39m EvaluationOverview(\n\u001b[1;32m    199\u001b[0m     run_overviews\u001b[38;5;241m=\u001b[39mpartial_overview\u001b[38;5;241m.\u001b[39mrun_overviews,\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mpartial_evaluation_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m+\u001b[39m num_failed_evaluations,\n\u001b[1;32m    207\u001b[0m )\n",
      "File \u001b[0;32m~/intelligence-layer/src/intelligence_layer/evaluation/evaluation/evaluation_repository.py:195\u001b[0m, in \u001b[0;36mEvaluationRepository.failed_example_evaluations\u001b[0;34m(self, evaluation_id, evaluation_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfailed_example_evaluations\u001b[39m(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m, evaluation_id: \u001b[38;5;28mstr\u001b[39m, evaluation_type: \u001b[38;5;28mtype\u001b[39m[Evaluation]\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[ExampleEvaluation[Evaluation]]:\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns all failed :class:`ExampleEvaluation`s for the given evaluation overview ID sorted by their example ID.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        A :class:`Sequence` of failed :class:`ExampleEvaluation`s.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_evaluations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r\u001b[38;5;241m.\u001b[39mresult, FailedExampleEvaluation)]\n",
      "File \u001b[0;32m~/intelligence-layer/src/intelligence_layer/evaluation/evaluation/file_evaluation_repository.py:81\u001b[0m, in \u001b[0;36mFileSystemEvaluationRepository.example_evaluations\u001b[0;34m(self, evaluation_id, evaluation_type)\u001b[0m\n\u001b[1;32m     79\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_directory(evaluation_id)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepository does not contain an evaluation with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m example_evaluations: \u001b[38;5;28mlist\u001b[39m[ExampleEvaluation[Evaluation]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_names(path):\n",
      "\u001b[0;31mValueError\u001b[0m: Repository does not contain an evaluation with id: 41d5a7ac-c7f1-4e22-9ef3-538bf98ecbc5"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_repository = AsyncFileEvaluationRepository(REPOSITORY_ROOT_PATH)\n",
    "\n",
    "aggregation_repository = FileAggregationRepository(REPOSITORY_ROOT_PATH)\n",
    "# either remember the id from before (eval_overview.id) or retrieve as below\n",
    "eval_id = [\n",
    "    overview.id\n",
    "    for overview in evaluation_repository.partial_evaluation_overviews()\n",
    "    if overview.description == \"instruct-evaluation\"\n",
    "][0]\n",
    "\n",
    "evaluation_overview = evaluator.retrieve(eval_id)\n",
    "\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"instruct-aggregation\",\n",
    "    aggregation_logic,\n",
    ")\n",
    "\n",
    "output = aggregator.aggregate_evaluation(eval_id)\n",
    "print(output.statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! WARNING ! This deletes the \"test-human-eval\" argilla workspace and the \"human-eval-data\" folder.\n",
    "argilla_client.delete_workspace(workspace_id)\n",
    "\n",
    "shutil.rmtree(REPOSITORY_ROOT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
