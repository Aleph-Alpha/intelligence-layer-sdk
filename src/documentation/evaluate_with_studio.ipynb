{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from intelligence_layer.connectors.studio.studio import StudioClient\n",
    "from intelligence_layer.core import TextChunk\n",
    "from intelligence_layer.core.model import Llama3InstructModel\n",
    "from intelligence_layer.evaluation.benchmark.studio_benchmark import (\n",
    "    StudioBenchmarkRepository,\n",
    ")\n",
    "from intelligence_layer.evaluation.dataset.domain import Example\n",
    "from intelligence_layer.evaluation.dataset.studio_dataset_repository import (\n",
    "    StudioDatasetRepository,\n",
    ")\n",
    "from intelligence_layer.examples import (\n",
    "    ClassifyInput,\n",
    "    PromptBasedClassify,\n",
    ")\n",
    "from intelligence_layer.examples.classify.classify import (\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    ")\n",
    "from intelligence_layer.examples.classify.prompt_based_classify_with_definitions import (\n",
    "    LabelWithDefinition,\n",
    "    PromptBasedClassifyWithDefinitions,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with Studio\n",
    "\n",
    "This notebook shows how you can evaluate the performance of `Task`s using Studio. This notebook focuses on the `PromptBasedClassify` for demonstration purposes.\n",
    "\n",
    "First, we need to instantiate the `StudioClient`. We can either pass an existing project or let the `StudioClient` create it by setting the `create_project` flag to `True.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_client = StudioClient(project=\"Classify with Studio\", create_project=True)\n",
    "studio_dataset_repository = StudioDatasetRepository(studio_client)\n",
    "studio_benchmark_repository = StudioBenchmarkRepository(studio_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our evaluation dataset from some pre-defined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"data/classify_examples.json\").open() as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Finance',\n",
       " 'message': 'I just traveled to Paris for a conference, where can I get the train ride refunded?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't quite yet the format we need, therefore we translate it into the interface of our `Example`.\n",
    "\n",
    "This is the target structure:\n",
    "\n",
    "``` python\n",
    "class Example(BaseModel, Generic[Input, ExpectedOutput]):\n",
    "    input: Input\n",
    "    expected_output: ExpectedOutput\n",
    "    id: Optional[str] = Field(default_factory=lambda: str(uuid4()))\n",
    "    metadata: Optional[SerializableDict]\n",
    "```\n",
    "\n",
    "We want the `input` in each `Example` to contain the input of an actual task.\n",
    "The `expected_output` shall correspond to anything we wish to compare our generated output to (i.e., the expected label in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: 5e3ce59d-f87c-448d-b811-c02ecae1588d\n"
     ]
    }
   ],
   "source": [
    "all_labels = list(set(item[\"label\"] for item in data))\n",
    "dataset = studio_dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(item[\"message\"]), labels=all_labels),\n",
    "            expected_output=item[\"label\"],\n",
    "        )\n",
    "        for item in data\n",
    "    ],\n",
    "    dataset_name=\"Single Label Classify Dataset\",\n",
    ")\n",
    "print(f\"Dataset ID: {dataset.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also automatically uploads the created dataset to you **Studio** instance.\n",
    "We can inspect the dataset and the individual examples in **Studio** under **Evaluate/Datasets**. Do not forget to select the correct project!\n",
    "\n",
    "After we have checked our `Dataset`, we can create our first `Benchmark`. To this end, we need the `EvaluationLogic` and the `AggregationLogic` of our Classify use-case. After creating the `Benchmark`, make sure to copy the ID of the `Benchmark` into the `get_benchmark` method, so you don't have to create the `Benchmark` again every time you run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark ID: ec40c09a-0472-4c70-bd48-7427e4abc87f\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "evaluation_logic = SingleLabelClassifyEvaluationLogic()\n",
    "aggregation_logic = SingleLabelClassifyAggregationLogic()\n",
    "\n",
    "rand_str = \"\".join(\n",
    "    random.choice(string.ascii_uppercase + string.ascii_lowercase + string.digits)\n",
    "    for _ in range(16)\n",
    ")\n",
    "\n",
    "benchmark = studio_benchmark_repository.create_benchmark(\n",
    "    dataset.id,\n",
    "    evaluation_logic,\n",
    "    aggregation_logic,\n",
    "    f\"Single Label Classify Benchmark {rand_str}\",  # Benchmark names need to be unique, therefore we add a random string to the name\n",
    ")\n",
    "print(f\"Benchmark ID: {benchmark.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we are ready to `execute` our first `Benchmark`. We pass it a meaningful name and execute it. After about two minutes we can take a look at the results in **Studio** in the **Evaluate/Benchmarks** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Task: 100%|██████████| 24/24 [00:56<00:00,  2.37s/it]\n",
      "Evaluating: 24it [00:00, 92691.80it/s]\n",
      "Submitting traces to Studio: 100%|██████████| 24/24 [00:01<00:00, 21.70it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'model_dump_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPromptBasedClassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClassify v0.0 with Luminous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/intelligence-layer-sdk/src/intelligence_layer/evaluation/benchmark/studio_benchmark.py:161\u001b[0m, in \u001b[0;36mStudioBenchmark.execute\u001b[0;34m(self, task, name, description, labels, metadata)\u001b[0m\n\u001b[1;32m    155\u001b[0m     trace_ids\u001b[38;5;241m.\u001b[39mappend(trace_id)\n\u001b[1;32m    157\u001b[0m benchmark_lineages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_benchmark_lineages(\n\u001b[1;32m    158\u001b[0m     eval_lineages\u001b[38;5;241m=\u001b[39mevaluation_lineages,\n\u001b[1;32m    159\u001b[0m     trace_ids\u001b[38;5;241m=\u001b[39mtrace_ids,\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_benchmark_lineages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_lineages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark_lineages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m benchmark_execution_id\n",
      "File \u001b[0;32m~/Documents/GitHub/intelligence-layer-sdk/src/intelligence_layer/connectors/studio/studio.py:501\u001b[0m, in \u001b[0;36mStudioClient.submit_benchmark_lineages\u001b[0;34m(self, benchmark_lineages, benchmark_id, execution_id)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit_benchmark_lineages\u001b[39m(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    489\u001b[0m     benchmark_lineages: PostBenchmarkLineagesRequest,\n\u001b[1;32m    490\u001b[0m     benchmark_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    491\u001b[0m     execution_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    493\u001b[0m     url \u001b[38;5;241m=\u001b[39m urljoin(\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/api/projects/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/evaluation/benchmarks/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/executions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecution_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/lineages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    498\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    499\u001b[0m         url,\n\u001b[1;32m    500\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers,\n\u001b[0;32m--> 501\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[43mbenchmark_lineages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump_json\u001b[49m(),\n\u001b[1;32m    502\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_for_status(response)\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'model_dump_json'"
     ]
    }
   ],
   "source": [
    "benchmark.execute(PromptBasedClassify(), \"Classify v0.0 with Luminous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve our results and run this again using a `Llama` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m      2\u001b[0m     PromptBasedClassify(model\u001b[38;5;241m=\u001b[39mLlama3InstructModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-8b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassify v0.1 with Llama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'benchmark' is not defined"
     ]
    }
   ],
   "source": [
    "benchmark.execute(\n",
    "    PromptBasedClassify(model=Llama3InstructModel(\"llama-3.1-8b-instruct\")),\n",
    "    \"Classify v0.1 with Llama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further comparisons we also `execute` the `PromptBasedClassifyWithDefinitions` task on the same `Benchmark`. This is possible because both `Task` have the exact same input and output format and can thus be compared to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_with_definitions = [\n",
    "    LabelWithDefinition(\n",
    "        name=\"Finance\",\n",
    "        definition=\"Handles reimbursements, salary payments, and financial planning.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Sales\",\n",
    "        definition=\"Manages client inquiries, builds relationships, and drives revenue.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Communications\",\n",
    "        definition=\"Oversees media inquiries, partnerships, and public documentation.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Research\",\n",
    "        definition=\"Collaborates on innovative projects and explores market applications.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"IT Support\",\n",
    "        definition=\"Provides technical assistance for devices and platform access issues.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Human Resources\",\n",
    "        definition=\"Manages onboarding, leave requests, and career development.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Product\",\n",
    "        definition=\"Addresses customer issues, ensures compliance, and demonstrates product use.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Customer\",\n",
    "        definition=\"Schedules meetings and ensures customer needs are effectively met.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Security\",\n",
    "        definition=\"Maintains physical and digital safety, including badge and certificate issues.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"Marketing\",\n",
    "        definition=\"Manages strategic initiatives and promotes the company's offerings.\",\n",
    "    ),\n",
    "    LabelWithDefinition(\n",
    "        name=\"CEO Office\",\n",
    "        definition=\"Handles executive engagements and key stakeholder follow-ups.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "classify_with_definitions = PromptBasedClassifyWithDefinitions(\n",
    "    labels_with_definitions=labels_with_definitions,\n",
    "    model=Llama3InstructModel(\"llama-3.1-8b-instruct\"),\n",
    ")\n",
    "benchmark.execute(classify_with_definitions, \"Classify v1.0 with definitions and Llama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-ZqHLMTHE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
