{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import string\n",
    "from typing import Iterable\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from intelligence_layer.core import Input, Task, TaskSpan\n",
    "from intelligence_layer.evaluation import (\n",
    "    Evaluation,\n",
    "    Evaluator,\n",
    "    InMemoryEvaluationRepository,\n",
    "    SingleOutputEvaluationLogic,\n",
    ")\n",
    "from intelligence_layer.evaluation.aggregation.aggregator import (\n",
    "    AggregationLogic,\n",
    "    Aggregator,\n",
    ")\n",
    "from intelligence_layer.evaluation.aggregation.in_memory_aggregation_repository import (\n",
    "    InMemoryAggregationRepository,\n",
    ")\n",
    "from intelligence_layer.evaluation.dataset.domain import Example, ExpectedOutput\n",
    "from intelligence_layer.evaluation.dataset.in_memory_dataset_repository import (\n",
    "    InMemoryDatasetRepository,\n",
    ")\n",
    "from intelligence_layer.evaluation.run.in_memory_run_repository import (\n",
    "    InMemoryRunRepository,\n",
    ")\n",
    "from intelligence_layer.evaluation.run.runner import Runner\n",
    "\n",
    "\n",
    "class DummyTask(Task[str, str]):\n",
    "    def __init__(self, model: str, prompt: str):\n",
    "        self.model = model\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def do_run(self, input: str, task_span: TaskSpan) -> str:\n",
    "        wordlist = [\n",
    "            \"apple\",\n",
    "            \"banana\",\n",
    "            \"car\",\n",
    "            \"dog\",\n",
    "            \"elephant\",\n",
    "            \"fish\",\n",
    "            \"goat\",\n",
    "            \"hat\",\n",
    "            \"igloo\",\n",
    "            \"jacket\",\n",
    "        ]\n",
    "        sentences = [\n",
    "            \"Once upon a time,\",\n",
    "            \"In a land far, far away,\",\n",
    "            \"Suddenly,\",\n",
    "            \"One day,\",\n",
    "            \"In the morning,\",\n",
    "        ]\n",
    "\n",
    "        random.seed(hash(input) + hash(self.model))  # Set the seed based on the prompt\n",
    "\n",
    "        story = self.prompt + \" \"\n",
    "        for _ in range(10):\n",
    "            sentence = random.choice(sentences)\n",
    "            word = random.choice(wordlist)\n",
    "            story += sentence + \" \" + word + \" \"\n",
    "        return story\n",
    "\n",
    "\n",
    "class DummyEvaluation(BaseModel):\n",
    "    text_length: int\n",
    "    normalized_capital_count: float\n",
    "\n",
    "\n",
    "class DummyEvaluationLogic(\n",
    "    SingleOutputEvaluationLogic[str, str, None, DummyEvaluation]\n",
    "):\n",
    "    def do_evaluate_single_output(\n",
    "        self, example: Example[Input, ExpectedOutput], output: str\n",
    "    ) -> Evaluation:\n",
    "        return DummyEvaluation(\n",
    "            text_length=len(output),\n",
    "            normalized_capital_count=sum(c.isupper() for c in output) / len(output),\n",
    "        )\n",
    "\n",
    "\n",
    "class DummyAggregatedEvaluation(BaseModel):\n",
    "    avg_length: float\n",
    "    avg_normalized_capital_count: float\n",
    "\n",
    "\n",
    "class DummyAggregationLogic(\n",
    "    AggregationLogic[DummyEvaluation, DummyAggregatedEvaluation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self, evaluations: Iterable[DummyEvaluation]\n",
    "    ) -> DummyAggregatedEvaluation:\n",
    "        eval_list = list(evaluations)\n",
    "        avg_length = sum([s.text_length for s in eval_list]) / len(eval_list)\n",
    "        avg_normalized_capital_count = sum(\n",
    "            [s.normalized_capital_count for s in eval_list]\n",
    "        ) / len(eval_list)\n",
    "        return DummyAggregatedEvaluation(\n",
    "            avg_length=avg_length,\n",
    "            avg_normalized_capital_count=avg_normalized_capital_count,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"compare-tasks\"\n",
    "random.seed(42)\n",
    "examples = [\n",
    "    Example(\n",
    "        input=\"\".join(random.choices(string.ascii_letters, k=random.randint(1, 50))),\n",
    "        expected_output=None,\n",
    "    )\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "dataset = dataset_repository.create_dataset(\n",
    "    examples=examples, dataset_name=\"my-dataset\"\n",
    ")\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    EXPERIMENT_NAME,\n",
    "    DummyEvaluationLogic(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"model a\", \"model b\", \"model c\"]\n",
    "prompt_list = [\n",
    "    \"A nice story starts with:\",\n",
    "    \"Some kind of prompt\",\n",
    "    \"No prompt at all\",\n",
    "    \"OPTIMIZING PROMPTS IS HARD TO DO\",\n",
    "]\n",
    "for model, prompt in itertools.product(model_list, prompt_list):\n",
    "    dummy_task = DummyTask(model=model, prompt=prompt)\n",
    "\n",
    "    description = f\"|{model}|{prompt}|\"\n",
    "    runner = Runner(dummy_task, dataset_repository, run_repository, EXPERIMENT_NAME)\n",
    "    run_overview = runner.run_dataset(dataset.id, description=description)\n",
    "\n",
    "    eval_overview = evaluator.evaluate_runs(run_overview.id, description=description)\n",
    "\n",
    "    aggregator = Aggregator(\n",
    "        evaluation_repository,\n",
    "        aggregation_repository,\n",
    "        EXPERIMENT_NAME + \":\" + description,\n",
    "        DummyAggregationLogic(),\n",
    "    )\n",
    "    aggregator.aggregate_evaluation(eval_overview.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.evaluation.infrastructure.repository_navigator import (\n",
    "    aggregation_overviews_to_pandas,\n",
    ")\n",
    "\n",
    "aggregations_of_interest = [\n",
    "    overview\n",
    "    for overview in aggregation_repository.aggregation_overviews(\n",
    "        aggregation_type=DummyAggregatedEvaluation\n",
    "    )\n",
    "    if overview.description.startswith(EXPERIMENT_NAME)\n",
    "]\n",
    "formated_aggregations = aggregation_overviews_to_pandas(aggregations_of_interest)\n",
    "\n",
    "aggregation_fields = list(DummyAggregatedEvaluation.model_fields.keys())\n",
    "formated_aggregations = formated_aggregations[[\"description\"] + aggregation_fields]\n",
    "formated_aggregations[[\"model\", \"prompt\"]] = formated_aggregations[\n",
    "    \"description\"\n",
    "].str.split(\"|\", expand=True)[[1, 2]]\n",
    "formated_aggregations.drop(columns=\"description\", inplace=True)\n",
    "\n",
    "display(\n",
    "    formated_aggregations.sort_values(\n",
    "        by=\"avg_normalized_capital_count\", ascending=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_aggregations.pivot(\n",
    "    index=\"model\", columns=\"prompt\", values=\"avg_normalized_capital_count\"\n",
    ").plot(kind=\"box\", rot=90, title=\"avg_normalized_capital_count\")\n",
    "formated_aggregations.pivot(index=\"prompt\", columns=\"model\", values=\"avg_length\").plot(\n",
    "    kind=\"box\", title=\"avg_length\"\n",
    ")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
