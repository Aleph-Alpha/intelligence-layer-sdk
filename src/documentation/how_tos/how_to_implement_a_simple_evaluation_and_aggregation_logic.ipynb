{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from intelligence_layer.evaluation import (\n",
    "    AggregationLogic,\n",
    "    Example,\n",
    "    SingleOutputEvaluationLogic,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement logic for the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine the data types you need for the evaluation:\n",
    "   - An `Example` of the dataset you are using defines \"`Input`\" and \"`ExpectedOutput`\" data types\n",
    "   - The task you are using defines the `Output` data type.\n",
    "2. Create an `Evaluation` type that will contain the domain-specific evaluation result for a single `Example`.\n",
    "3. Decide if you want to use a single `Output` per `Example`, or multiple outputs per example, during your evaluation to generate your evaluation results.\n",
    "   - For a single output, we recommend to implement a `SingleOutputEvaluationLogic`.\n",
    "   - For multiple outputs, implement an `EvaluationLogic`.\n",
    "4. Implement the evaluation logic in the `do_evaluate_single_output` method for `SingleOutputEvaluationLogic` or in the `do_evaluate` method for `EvaluationLogic`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "In the following example we want to evaluate a story-generating task that generates a story of a topic with a targeted word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - This is only redefined here for completeness. Normally these would be imported.\n",
    "# Note that we do not have an ExpectedOutput here.\n",
    "\n",
    "\n",
    "class StoryTaskInput(BaseModel):\n",
    "    topic: str\n",
    "    targeted_word_count: int\n",
    "\n",
    "\n",
    "class StoryTaskOutput(BaseModel):\n",
    "    story: str\n",
    "\n",
    "\n",
    "# Step 2 - We want to analyze if the word count is accurate\n",
    "class StoryEvaluation(BaseModel):\n",
    "    word_count_off_by: int\n",
    "\n",
    "\n",
    "class StoryEvaluationLogic(\n",
    "    # Step 3 - We only need a single output to analyze the word count\n",
    "    SingleOutputEvaluationLogic[\n",
    "        StoryTaskInput, StoryTaskOutput, None, StoryEvaluation\n",
    "    ]  # We pass None here as we do not have an ExpectedOutput\n",
    "):\n",
    "    def do_evaluate_single_output(\n",
    "        self, example: Example[StoryTaskInput, None], output: StoryTaskOutput\n",
    "    ) -> StoryEvaluation:\n",
    "        # Step 4 - Implement the domain specific logic\n",
    "        output_word_count = len(output.story.split())\n",
    "        word_count_off_by = output_word_count - example.input.targeted_word_count\n",
    "        return StoryEvaluation(word_count_off_by=word_count_off_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement a logic for an aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Implement the evaluation logic for your use-case. (see [above](#how-to-implement-logic-for-the-evaluation))\n",
    "1. Create an `AggregatedEvaluation` type that will contain the domain specific data aggregated from evaluations.\n",
    "2. Implement an `AggregationLogic` for your data types\n",
    "   1. Implement the domain-specific logic in the `aggregate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "In the following example, we calculate basic statistics on the word count differences of the previous evaluation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - See the example above\n",
    "\n",
    "\n",
    "# Step 1\n",
    "class StoryAggregation(BaseModel):\n",
    "    wc_off_mean: float\n",
    "    wc_off_median: int\n",
    "    wc_off_std: float\n",
    "\n",
    "\n",
    "# Step 2\n",
    "class StoryAggregationLogic(AggregationLogic[StoryEvaluation, StoryAggregation]):\n",
    "    def aggregate(self, evaluations: Iterable[StoryEvaluation]) -> StoryAggregation:\n",
    "        # Step 2.1\n",
    "        word_counts = np.array(\n",
    "            [evaluation.word_count_off_by for evaluation in evaluations]\n",
    "        )\n",
    "        wc_off_mean = np.mean(word_counts)\n",
    "        wc_off_median = np.median(word_counts)\n",
    "        wc_off_std = np.std(word_counts)\n",
    "        return StoryAggregation(\n",
    "            wc_off_mean=wc_off_mean, wc_off_median=wc_off_median, wc_off_std=wc_off_std\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-d3iSWYpm-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
