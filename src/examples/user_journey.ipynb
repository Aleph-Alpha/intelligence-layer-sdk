{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Effectiveness of LLM-based Email Classification Systems\n",
    "\n",
    "In the fast-paced world of business, effectively managing incoming support emails is crucial. The ability to quickly and accurately classify these emails into the appropriate department and determine their urgency is not just a matter of operational efficiency; it directly impacts customer satisfaction and overall business success. Given the high stakes, it's essential to rigorously evaluate any solution designed to automate this process. This tutorial focuses on the evaluation of a LLM-based program developed to automate the classification of support emails.\n",
    "\n",
    "In an environment brimming with various methodologies and tools, understanding the comparative effectiveness of different approaches is vital. Systematic evaluation allows us to identify which techniques are best suited for specific tasks, understand their strengths and weaknesses, and optimize their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we are only given a few anecdotal examples. Let's see how far we can get with these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Hi, my laptop crashed and I can't start it anymore. Do you need the serial number or sth?\",\n",
    "    \"Hello,\\n\\nI am writing my Master's Thesis and would like to investigate the model's performance. Could I get some free credits?\\n\\nCheers, Niklas\",\n",
    "]\n",
    "\n",
    "labels = {\n",
    "    \"Product\",\n",
    "    \"Customer\",\n",
    "    \"CEO Office\",\n",
    "    \"Research\",\n",
    "    \"Finance\",\n",
    "    \"Accounting\",\n",
    "    \"Legal\",\n",
    "    \"Communication Department\",\n",
    "    \"Infrastructure\",\n",
    "    \"People & Culture\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core import TextChunk, InMemoryTracer\n",
    "from intelligence_layer.use_cases import PromptBasedClassify, ClassifyInput\n",
    "\n",
    "\n",
    "prompt_based_classify = PromptBasedClassify()\n",
    "\n",
    "classify_inputs = [\n",
    "    ClassifyInput(chunk=TextChunk(example), labels=labels) for example in examples\n",
    "]\n",
    "\n",
    "\n",
    "outputs = prompt_based_classify.run_concurrently(classify_inputs, InMemoryTracer())\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sorted(list(o.scores.items()), key=lambda i: i[1], reverse=True)[0] for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_examples = [\n",
    "    {\n",
    "        \"label\": \"Finance\",\n",
    "        \"message\": \"I just traveled to Paris for a conference, where can I get the train ride refunded?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Customer\",\n",
    "        \"message\": \"Hello, we would like to get in contact with your sales team, because we are interested in your solution.\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Communication Department\",\n",
    "        \"message\": \"We are working on a documentation on AI and would like to film a piece about you. Would you be interested?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Research\",\n",
    "        \"message\": \"I am working with Stanford and was hoping to win you over for a research collaboration.\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"IT Support\",\n",
    "        \"message\": \"My laptop is broken\"},\n",
    "    {\n",
    "        \"label\": \"Communications\",\n",
    "        \"message\": \"I already tried to call many times. Can I get a meeting with Jonas?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Communications\",\n",
    "        \"message\": \"Can you send your models via email?\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Research\",\n",
    "        \"message\": \"We should do a research collaboration.\"},\n",
    "    {\n",
    "        \"label\": \"Research\",\n",
    "        \"message\": \"H100 cluster available right now. Would you like to procure at low prices?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Research\",\n",
    "        \"message\": \"My company has been working on time series and signal processing for a long time. It would make sense to define a joint go to market.\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"People & Culture\",\n",
    "        \"message\": \"Full stack developer in your area available now.\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Product\",\n",
    "        \"message\": \"Hi,\\n\\nI am having trouble running your docker container in my environment. It fails to start. Can you help?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Product\",\n",
    "        \"message\": \"Hello,\\n\\nI am getting strange errors from your API. It is saying the queue is full, but I am only sending one task at a time. Why is this happening?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Customer\",\n",
    "        \"message\": \"Can you show me a demo of different use cases your product can solve?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"People & Culture\",\n",
    "        \"message\": \"Hey, I did not get a t-shirt in the onboarding. Could I still get one?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Customer\",\n",
    "        \"message\": \"Hi, can you name me a couple of timeslots for a first call? Would be really interested in learning more about the product?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Product\",\n",
    "        \"message\": \"Hi Jan, is your tool ISO 37301 compliant?\"},\n",
    "    {\n",
    "        \"label\": \"I can’t login to Mattermost or Sharepoint, how can I gain access?\",\n",
    "        \"message\": \"IT Support\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Ignore\",\n",
    "        \"message\": \"Hi, Jonas here. I need something really urgently right now. Could you share your number with me?\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Finance\",\n",
    "        \"message\": \"I did not get paid last month, when do I get paid? What is going on?\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Security\",\n",
    "        \"message\": \"Hi, I want to get a new badge, the photo of me looks ugly and I just got new glasses so it does not look like me. \"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Marketing\",\n",
    "        \"message\": \"Let us celebrate AI day in style, we want to invite you and the CEO to join us.\"\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Sales\",\n",
    "        \"message\": \"Jonas, we have met each other at the event in Nürnberg, can we meet for a follow up in your Office in Heidelberg?\"\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Security\",\n",
    "        \"message\": \"Your hTTPs Certificate is not valid on your www.aleph-alpha.de\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"HR\",\n",
    "        \"message\": \"I want to take a week off immediatly\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"HR\",\n",
    "        \"message\": \"I want to take a sabbatical\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"HR\",\n",
    "        \"message\": \"How can I work more, I want to work weekends, can I get paid overtime?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.evaluation import InMemoryDatasetRepository, Example\n",
    "\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(example[\"message\"]), labels=labels),\n",
    "            expected_output=example[\"label\"],\n",
    "        )\n",
    "        for example in labeled_examples\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from intelligence_layer.evaluation import (\n",
    "    Evaluator,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    InMemoryAggregationRepository,\n",
    "    Runner,\n",
    "    Aggregator,\n",
    ")\n",
    "from intelligence_layer.use_cases import (\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyEvaluationLogic(),\n",
    ")\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyAggregationLogic(),\n",
    ")\n",
    "runner = Runner(\n",
    "    prompt_based_classify, dataset_repository, run_repository, \"prompt-based-classify\"\n",
    ")\n",
    "run_overview = runner.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overview = evaluator.evaluate_runs(run_overview.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview = aggregator.aggregate_evaluation(eval_overview.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases import (\n",
    "    SingleLabelClassifyOutput,\n",
    "    SingleLabelClassifyEvaluation,\n",
    ")\n",
    "\n",
    "\n",
    "overview = [\n",
    "    {\n",
    "        \"input\": example.input,\n",
    "        \"expected_output\": example.expected_output,\n",
    "        \"result\": next(\n",
    "            e\n",
    "            for e in run_repository.example_outputs(\n",
    "                run_overview.id, SingleLabelClassifyOutput\n",
    "            )\n",
    "            if e.example_id == example.id\n",
    "        ).output,\n",
    "        \"eval\": evaluation_repository.example_evaluation(\n",
    "            evaluation_id=eval_overview.id,\n",
    "            example_id=example.id,\n",
    "            evaluation_type=SingleLabelClassifyEvaluation,\n",
    "        ).result,\n",
    "    }\n",
    "    for example in dataset_repository.examples(\n",
    "        dataset_id=dataset_id, input_type=ClassifyInput, expected_output_type=str\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in overview if not e[\"eval\"].correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_adjusted_classify_task = PromptBasedClassify(\n",
    "    instruction=\"\"\"Identify teh department that would be responsible for handling the given request.\n",
    "Reply with only the department name.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_prompt_adjusted = Runner(\n",
    "    prompt_adjusted_classify_task,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt\",\n",
    ")\n",
    "run_overview_prompt_adjusted = runner_prompt_adjusted.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overview_prompt_adjusted = evaluator.evaluate_runs(run_overview_prompt_adjusted.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview_prompt_adjusted = aggregator.aggregate_evaluation(\n",
    "    eval_overview_prompt_adjusted.id\n",
    ")\n",
    "aggregation_overview_prompt_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = [\n",
    "    {\n",
    "        \"input\": example.input,\n",
    "        \"expected_output\": example.expected_output,\n",
    "        \"result\": next(\n",
    "            e\n",
    "            for e in run_repository.example_outputs(\n",
    "                run_overview_prompt_adjusted.id, SingleLabelClassifyOutput\n",
    "            )\n",
    "            if e.example_id == example.id\n",
    "        ).output,\n",
    "        \"eval\": evaluation_repository.example_evaluation(\n",
    "            evaluation_id=eval_overview_prompt_adjusted.id,\n",
    "            example_id=example.id,\n",
    "            evaluation_type=SingleLabelClassifyEvaluation,\n",
    "        ).result,\n",
    "    }\n",
    "    for example in dataset_repository.examples(\n",
    "        dataset_id=dataset_id, input_type=ClassifyInput, expected_output_type=str\n",
    "    )\n",
    "]\n",
    "[e for e in overview if not e[\"eval\"].correct]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
