{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from intelligence_layer.core import InMemoryTracer, LuminousControlModel, TextChunk\n",
    "from intelligence_layer.evaluation import (\n",
    "    Aggregator,\n",
    "    Evaluator,\n",
    "    Example,\n",
    "    InMemoryAggregationRepository,\n",
    "    InMemoryDatasetRepository,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    Runner,\n",
    ")\n",
    "from intelligence_layer.use_cases import (\n",
    "    ClassifyInput,\n",
    "    PromptBasedClassify,\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    "    SingleLabelClassifyEvaluation,\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    "    SingleLabelClassifyOutput,\n",
    ")\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Effectiveness of LLM-based Email Classification Systems\n",
    "\n",
    "In the fast-paced world of business, effectively managing incoming support emails is crucial. The ability to quickly and accurately classify these emails into the appropriate department and determine their urgency is not just a matter of operational efficiency; it directly impacts customer satisfaction and overall business success. Given the high stakes, it's essential to rigorously evaluate any solution designed to automate this process. This tutorial focuses on the evaluation of a LLM-based program developed to automate the classification of support emails.\n",
    "\n",
    "In an environment brimming with various methodologies and tools, understanding the comparative effectiveness of different approaches is vital. Systematic evaluation allows us to identify which techniques are best suited for specific tasks, understand their strengths and weaknesses, and optimize their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we are only given a few anecdotal examples. Let's see how far we can get with these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Hi, my laptop crashed and I can't start it anymore. Do you need the serial number or sth?\",\n",
    "    \"Hello,\\n\\nI am writing my Master's Thesis and would like to investigate the model's performance. Could I get some free credits?\\n\\nCheers, Niklas\",\n",
    "]\n",
    "\n",
    "labels = {\n",
    "    \"Product\",\n",
    "    \"Customer\",\n",
    "    \"CEO Office\",\n",
    "    \"Research\",\n",
    "    \"Finance and Accounting\",\n",
    "    \"Legal\",\n",
    "    \"Communications\",\n",
    "    \"Infrastructure\",\n",
    "    \"Human Resources\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the Intelligence Layer provides some classification tasks out of the box.\n",
    "\n",
    "Let's run it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Luckily, the Intelligence provides some classification tasks out of the box.\n",
    "\n",
    "Let's import it and run!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the default task\n",
    "prompt_based_classify = PromptBasedClassify()\n",
    "\n",
    "classify_inputs = [\n",
    "    ClassifyInput(chunk=TextChunk(example), labels=labels) for example in examples\n",
    "]\n",
    "\n",
    "\n",
    "outputs = prompt_based_classify.run_concurrently(classify_inputs, InMemoryTracer())\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Hmm, we have some results, but they aren't really legible (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sorted(list(o.scores.items()), key=lambda i: i[1], reverse=True)[0] for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It appears that the Finance Department can fix my laptop and the Comms people can reward free credits...\n",
    "We probably have to do some finetuning of our classification approach.\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Sales\",\n",
    "        \"message\": \"Jonas, we have met each other at the event in NÃ¼rnberg, can we meet for a follow up in your Office in Heidelberg?\"\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Security\",\n",
    "        \"message\": \"Your hTTPs Certificate is not valid on your www.aleph-alpha.de\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"HR\",\n",
    "        \"message\": \"I want to take a week off immediatly\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"HR\",\n",
    "        \"message\": \"I want to take a sabbatical\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"HR\",\n",
    "        \"message\": \"How can I work more, I want to work weekends, can I get paid overtime?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/classify_examples.json\", \"r\") as file:\n",
    "    labeled_examples: list[dict[str, str]] = json.load(file)\n",
    "\n",
    "labeled_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intelligence Layer offers support to run task evaluations.\n",
    "\n",
    "First, we have to create a dataset inside a repository.\n",
    "There are different repositories (that persist datasets in different ways), but an `InMemoryDatasetRepository` will do for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_repository = InMemoryDatasetRepository()\n",
    "\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(example[\"message\"]), labels=labels),\n",
    "            expected_output=example[\"label\"],\n",
    "        )\n",
    "        for example in labeled_examples\n",
    "    ],\n",
    "    dataset_name=\"MyDataset\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dataset is created, we generate a unique ID. We'll need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a few repositories to store runs, evals and aggregated evaluations\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "\n",
    "\n",
    "# each repository is used by a class that has a dedicated responsibility\n",
    "runner = Runner(\n",
    "    prompt_based_classify, dataset_repository, run_repository, \"prompt-based-classify\"\n",
    ")\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyEvaluationLogic(),\n",
    ")\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyAggregationLogic(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating, we must generate predictions for each sample in our datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overview = evaluator.evaluate_runs(run_overview.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "eval_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's aggregate all individual evaluations to get some eval statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview = aggregator.aggregate_evaluation(eval_overview.id)\n",
    "aggregation_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we only predicted around 25% of classes correctly.\n",
    "\n",
    "However, a closer look at the overview suggests that we have a bunch of incorrect labels in our test dataset.\n",
    "We will fix this later.\n",
    "\n",
    "First, let's have a look at a few failed examples in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_examples(run_id: str, eval_id: str, dataset_id: str, first_n: int):\n",
    "    overview = [\n",
    "        {\n",
    "            \"input\": example.input,\n",
    "            \"expected_output\": example.expected_output,\n",
    "            \"result\": sorted(\n",
    "                list(\n",
    "                    next(\n",
    "                        example_output\n",
    "                        for example_output in run_repository.example_outputs(\n",
    "                            run_id, SingleLabelClassifyOutput\n",
    "                        )\n",
    "                        if example_output.example_id == example.id\n",
    "                    ).output.scores.items()\n",
    "                ),\n",
    "                key=lambda i: i[1],\n",
    "                reverse=True,\n",
    "            )[0],\n",
    "            \"eval\": evaluation_repository.example_evaluation(\n",
    "                evaluation_id=eval_id,\n",
    "                example_id=example.id,\n",
    "                evaluation_type=SingleLabelClassifyEvaluation,\n",
    "            ).result,\n",
    "        }\n",
    "        for example in dataset_repository.examples(\n",
    "            dataset_id=dataset_id, input_type=ClassifyInput, expected_output_type=str\n",
    "        )\n",
    "    ]\n",
    "    return [example for example in overview if not example[\"eval\"].correct][:first_n]\n",
    "\n",
    "\n",
    "get_failed_examples(run_overview.id, eval_overview.id, dataset_id, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms it: some expected labels are missing. Let's try fixing this.\n",
    "\n",
    "We can do this two ways: Adjust our set of labels or adjust the eval set. In this case, we'll do the latter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's translate the other labels into the correct department\n",
    "label_map = {\n",
    "    \"IT Support\": \"Infrastructure\",\n",
    "    \"Sales\": \"Customer\",\n",
    "    \"Marketing\": \"Customer\",\n",
    "    \"Security\": \"Infrastructure\",\n",
    "    \"Finance\": \"Finance and Accounting\",\n",
    "}\n",
    "\n",
    "for example in labeled_examples:\n",
    "    label = example[\"label\"]\n",
    "    if label in label_map.keys():\n",
    "        example[\"label\"] = label_map[label]\n",
    "\n",
    "# datasets in the IL are immutable, so we must create a new one\n",
    "cleaned_dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(example[\"message\"]), labels=labels),\n",
    "            expected_output=example[\"label\"],\n",
    "        )\n",
    "        for example in labeled_examples\n",
    "    ],\n",
    "    dataset_name=\"CleanedDataset\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt used for the `PromptBasedClassify`-task looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_based_classify.instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably improve this task by making the prompt more specific, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_prompt = \"\"\"Identify the department that would be responsible for handling the given request.\n",
    "Reply with only the department name.\"\"\"\n",
    "prompt_adjusted_classify = PromptBasedClassify(instruction=adjusted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the cleaned dataset using this task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_prompt_adjusted = Runner(\n",
    "    prompt_adjusted_classify,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt\",\n",
    ")\n",
    "run_overview_prompt_adjusted = runner_prompt_adjusted.run_dataset(cleaned_dataset_id)\n",
    "eval_overview_prompt_adjusted = evaluator.evaluate_runs(run_overview_prompt_adjusted.id)\n",
    "aggregation_overview_prompt_adjusted = aggregator.aggregate_evaluation(\n",
    "    eval_overview_prompt_adjusted.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview_prompt_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, this already got us up to 58%!\n",
    "\n",
    "So far, we only used the `luminous-base-control` model. Let's see if we can improve our classifications by upgrading to a bigger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_with_extended = PromptBasedClassify(\n",
    "    instruction=adjusted_prompt, model=LuminousControlModel(\"luminous-supreme-control\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run it again and see if we improved!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_with_extended = Runner(\n",
    "    classify_with_extended,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt & better model\",\n",
    ")\n",
    "run_overview_with_extended = runner_with_extended.run_dataset(cleaned_dataset_id)\n",
    "eval_overview_with_extended = evaluator.evaluate_runs(run_overview_with_extended.id)\n",
    "aggregation_overview_with_extended = aggregator.aggregate_evaluation(\n",
    "    eval_overview_with_extended.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview_with_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using a bigger model further improved our results to 66.66%.\n",
    "\n",
    "As you can see there are plenty of option on how to further enhance the accuracy of our classify task. Notice, for instance, that so far we did not tell our classification task what each class means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_failed_examples(\n",
    "    run_overview_prompt_adjusted.id,\n",
    "    eval_overview_prompt_adjusted.id,\n",
    "    cleaned_dataset_id,\n",
    "    3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model had to 'guess' what we mean by each class purely from the given labels. In order to tackle this issue you could use the `PromptBasedClassifyWithDefinitions` task. This task allows you to also provide a short description for each class.\n",
    "\n",
    "Feel free to further play around and improve our classification example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
