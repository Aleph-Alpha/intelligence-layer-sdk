{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from intelligence_layer.core import InMemoryTracer, LuminousControlModel, TextChunk\n",
    "from intelligence_layer.evaluation import (\n",
    "    Aggregator,\n",
    "    Evaluator,\n",
    "    Example,\n",
    "    InMemoryAggregationRepository,\n",
    "    InMemoryDatasetRepository,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    Runner,\n",
    ")\n",
    "from intelligence_layer.use_cases import (\n",
    "    ClassifyInput,\n",
    "    PromptBasedClassify,\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    "    SingleLabelClassifyEvaluation,\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    "    SingleLabelClassifyOutput,\n",
    ")\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Effectiveness of LLM-based Email Classification Systems\n",
    "\n",
    "In the fast-paced world of business, effectively managing incoming support emails is crucial. The ability to quickly and accurately classify these emails into the appropriate department and determine their urgency is not just a matter of operational efficiency; it directly impacts customer satisfaction and overall business success. Given the high stakes, it's essential to rigorously evaluate any solution designed to automate this process. This tutorial focuses on the evaluation of a LLM-based program developed to automate the classification of support emails.\n",
    "\n",
    "In an environment brimming with various methodologies and tools, understanding the comparative effectiveness of different approaches is vital. Systematic evaluation allows us to identify which techniques are best suited for specific tasks, understand their strengths and weaknesses, and optimize their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we are only given a few anecdotal examples.\n",
    "Firstly, there are two e-mails, and secondly a number of potential departments to which they should be sent.\n",
    "\n",
    "Let's have a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Hi, my laptop crashed and I can't start it anymore. Do you need the serial number or sth?\",\n",
    "    \"Hello,\\n\\nI am writing my Master's Thesis and would like to investigate the model's performance. Could I get some free credits?\\n\\nCheers, Niklas\",\n",
    "]\n",
    "\n",
    "labels = {\n",
    "    \"Product\",\n",
    "    \"Customer\",\n",
    "    \"CEO Office\",\n",
    "    \"Research\",\n",
    "    \"Finance and Accounting\",\n",
    "    \"Legal\",\n",
    "    \"Communications\",\n",
    "    \"Infrastructure\",\n",
    "    \"Human Resources\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the Intelligence Layer provides some classification tasks out of the box.\n",
    "\n",
    "Let's run it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Luckily, the Intelligence provides some classification tasks out of the box.\n",
    "\n",
    "Let's import it and run!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SingleLabelClassifyOutput(scores={'People & Culture': 0.00540895946326583, 'Product': 0.0012847404821690297, 'Legal': 0.011450767273591046, 'Infrastructure': 7.63933851429347e-06, 'Accounting': 0.0037175198485175764, 'Communication Department': 0.4868987707545542, 'Research': 0.0001194994797172138, 'Finance': 0.4868987707545542, 'CEO Office': 8.307395237093146e-07, 'Customer': 0.004212501865592915}),\n",
       " SingleLabelClassifyOutput(scores={'People & Culture': 0.0002471222567108989, 'Product': 1.034420307682754e-06, 'Legal': 5.253219782981775e-06, 'Infrastructure': 1.797552960512546e-07, 'Accounting': 0.0024014826120226144, 'Communication Department': 0.9688271358793473, 'Research': 0.02010736772998545, 'Finance': 0.008381997922340201, 'CEO Office': 2.756636746614045e-08, 'Customer': 2.839863783934064e-05})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating the default task\n",
    "prompt_based_classify = PromptBasedClassify()\n",
    "\n",
    "# building the input object for each example\n",
    "classify_inputs = [\n",
    "    ClassifyInput(chunk=TextChunk(example), labels=labels) for example in examples\n",
    "]\n",
    "\n",
    "# running the tasks concurrently\n",
    "outputs = prompt_based_classify.run_concurrently(classify_inputs, InMemoryTracer())\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, we have some results, but they aren't really legible (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Hmm, we have some results, but they aren't really legible (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Communication Department', 0.4868987707545542),\n",
       " ('Communication Department', 0.9688271358793473)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sorted(list(o.scores.items()), key=lambda i: i[1], reverse=True)[0] for o in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Finance Department can fix my laptop and the Comms people can reward free credits...\n",
    "We probably have to do some finetuning of our classification approach.\n",
    "\n",
    "However, let's first make sure that this evidence is not anecdotal.\n",
    "For this, we need to do some eval. Luckily, we have by now got access to a few more examples...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Finance',\n",
       "  'message': 'I just traveled to Paris for a conference, where can I get the train ride refunded?'},\n",
       " {'label': 'Customer',\n",
       "  'message': 'Hello, we would like to get in contact with your sales team, because we are interested in your solution.'},\n",
       " {'label': 'Communication Department',\n",
       "  'message': 'We are working on a documentation on AI and would like to film a piece about you. Would you be interested?'},\n",
       " {'label': 'Research',\n",
       "  'message': 'I am working with Stanford and was hoping to win you over for a research collaboration.'},\n",
       " {'label': 'IT Support', 'message': 'My laptop is broken'},\n",
       " {'label': 'Communications',\n",
       "  'message': 'I already tried to call many times. Can I get a meeting with Jonas?'},\n",
       " {'label': 'Communications', 'message': 'Can you send your models via email?'},\n",
       " {'label': 'Research', 'message': 'We should do a research collaboration.'},\n",
       " {'label': 'Research',\n",
       "  'message': 'H100 cluster available right now. Would you like to procure at low prices?'},\n",
       " {'label': 'Research',\n",
       "  'message': 'My company has been working on time series and signal processing for a long time. It would make sense to define a joint go to market.'},\n",
       " {'label': 'People & Culture',\n",
       "  'message': 'Full stack developer in your area available now.'},\n",
       " {'label': 'Product',\n",
       "  'message': 'Hi,\\n\\nI am having trouble running your docker container in my environment. It fails to start. Can you help?'},\n",
       " {'label': 'Product',\n",
       "  'message': 'Hello,\\n\\nI am getting strange errors from your API. It is saying the queue is full, but I am only sending one task at a time. Why is this happening?'},\n",
       " {'label': 'Customer',\n",
       "  'message': 'Can you show me a demo of different use cases your product can solve?'},\n",
       " {'label': 'People & Culture',\n",
       "  'message': 'Hey, I did not get a t-shirt in the onboarding. Could I still get one?'},\n",
       " {'label': 'Customer',\n",
       "  'message': 'Hi, can you name me a couple of timeslots for a first call? Would be really interested in learning more about the product?'},\n",
       " {'label': 'Product', 'message': 'Hi Jan, is your tool ISO 37301 compliant?'},\n",
       " {'label': 'I can’t login to Mattermost or Sharepoint, how can I gain access?',\n",
       "  'message': 'IT Support'},\n",
       " {'label': 'Ignore',\n",
       "  'message': 'Hi, Jonas here. I need something really urgently right now. Could you share your number with me?'},\n",
       " {'label': 'Finance',\n",
       "  'message': 'I did not get paid last month, when do I get paid? What is going on?'},\n",
       " {'label': 'Security',\n",
       "  'message': 'Hi, I want to get a new badge, the photo of me looks ugly and I just got new glasses so it does not look like me. '},\n",
       " {'label': 'Marketing',\n",
       "  'message': 'Let us celebrate AI day in style, we want to invite you and the CEO to join us.'},\n",
       " {'label': 'Sales',\n",
       "  'message': 'Jonas, we have met each other at the event in Nürnberg, can we meet for a follow up in your Office in Heidelberg?'},\n",
       " {'label': 'Security',\n",
       "  'message': 'Your hTTPs Certificate is not valid on your www.aleph-alpha.de'},\n",
       " {'label': 'HR', 'message': 'I want to take a week off immediatly'},\n",
       " {'label': 'HR', 'message': 'I want to take a sabbatical'},\n",
       " {'label': 'HR',\n",
       "  'message': 'How can I work more, I want to work weekends, can I get paid overtime?'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"data/classify_examples.json\", 'r') as file:\n",
    "    labeled_examples = json.load(file)\n",
    "\n",
    "labeled_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intelligence layer offers support to run task evaluations.\n",
    "\n",
    "First, we have to create a dataset inside a repository.\n",
    "There are different repositories (that persist datasets in different ways), but an `InMemoryDatasetRepository` will do for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/classify_examples.json\", \"r\") as file:\n",
    "    labeled_examples: list[dict[str, str]] = json.load(file)\n",
    "\n",
    "labeled_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intelligence Layer offers support to run task evaluations.\n",
    "\n",
    "First, we have to create a dataset inside a repository.\n",
    "There are different repositories (that persist datasets in different ways), but an `InMemoryDatasetRepository` will do for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_repository = InMemoryDatasetRepository()\n",
    "\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(example[\"message\"]), labels=labels),\n",
    "            expected_output=example[\"label\"],\n",
    "        )\n",
    "        for example in labeled_examples\n",
    "    ],\n",
    "    dataset_name=\"MyDataset\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dataset is created, we generate a unique ID. We'll need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2521c77d-114d-4e65-8f81-449e53108f7f'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, let's actually run an evaluation on it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a few repositories to store runs, evals and aggregated evaluations\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "\n",
    "\n",
    "# each repository is used by a class that has a dedicated responsibility\n",
    "runner = Runner(\n",
    "    prompt_based_classify, dataset_repository, run_repository, \"prompt-based-classify\"\n",
    ")\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyEvaluationLogic(),\n",
    ")\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyAggregationLogic(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating, we must generate predictions for each sample in our datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_overview = runner.run_dataset(dataset_id)\n",
    "run_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "eval_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's aggregate all individual evaluations to get some eval statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview = aggregator.aggregate_evaluation(eval_overview.id)\n",
    "aggregation_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we only predicted around 25% of classes correctly.\n",
    "\n",
    "However, a closer look at the overview suggests that we have a bunch of incorrect labels in our test dataset.\n",
    "We will fix this later.\n",
    "\n",
    "First, let's have a look at a few failed examples in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_examples(run_id: str, eval_id: str, dataset_id: str, first_n: int):\n",
    "    overview = [\n",
    "        {\n",
    "            \"input\": example.input,\n",
    "            \"expected_output\": example.expected_output,\n",
    "            \"result\": sorted(\n",
    "                list(\n",
    "                    next(\n",
    "                        example_output\n",
    "                        for example_output in run_repository.example_outputs(\n",
    "                            run_id, SingleLabelClassifyOutput\n",
    "                        )\n",
    "                        if example_output.example_id == example.id\n",
    "                    ).output.scores.items()\n",
    "                ),\n",
    "                key=lambda i: i[1],\n",
    "                reverse=True,\n",
    "            )[0],\n",
    "            \"eval\": evaluation_repository.example_evaluation(\n",
    "                evaluation_id=eval_id,\n",
    "                example_id=example.id,\n",
    "                evaluation_type=SingleLabelClassifyEvaluation,\n",
    "            ).result,\n",
    "        }\n",
    "        for example in dataset_repository.examples(\n",
    "            dataset_id=dataset_id, input_type=ClassifyInput, expected_output_type=str\n",
    "        )\n",
    "    ]\n",
    "    return [example for example in overview if not example[\"eval\"].correct][:first_n]\n",
    "\n",
    "\n",
    "get_failed_examples(run_overview.id, eval_overview.id, dataset_id, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms it: some expected labels are missing. Let's try fixing this.\n",
    "\n",
    "We can do this two ways: Adjust our set of labels or adjust the eval set. In this case, we'll do the latter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's translate the other labels into the correct department\n",
    "label_map = {\n",
    "    \"IT Support\": \"Infrastructure\",\n",
    "    \"Sales\": \"Customer\",\n",
    "    \"Marketing\": \"Customer\",\n",
    "    \"Security\": \"Infrastructure\",\n",
    "    \"Finance\": \"Finance and Accounting\",\n",
    "}\n",
    "\n",
    "for example in labeled_examples:\n",
    "    label = example[\"label\"]\n",
    "    if label in label_map.keys():\n",
    "        example[\"label\"] = label_map[label]\n",
    "\n",
    "# datasets in the IL are immutable, so we must create a new one\n",
    "cleaned_dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(example[\"message\"]), labels=labels),\n",
    "            expected_output=example[\"label\"],\n",
    "        )\n",
    "        for example in labeled_examples\n",
    "    ],\n",
    "    dataset_name=\"CleanedDataset\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt used for the `PromptBasedClassify`-task looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_based_classify.instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably improve this task by making the prompt more specific, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_prompt = \"\"\"Identify the department that would be responsible for handling the given request.\n",
    "Reply with only the department name.\"\"\"\n",
    "prompt_adjusted_classify = PromptBasedClassify(instruction=adjusted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the cleaned dataset using this task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 27it [00:32,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "runner_prompt_adjusted = Runner(\n",
    "    prompt_adjusted_classify,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt\",\n",
    ")\n",
    "run_overview_prompt_adjusted = runner_prompt_adjusted.run_dataset(cleaned_dataset_id)\n",
    "eval_overview_prompt_adjusted = evaluator.evaluate_runs(run_overview_prompt_adjusted.id)\n",
    "aggregation_overview_prompt_adjusted = aggregator.aggregate_evaluation(\n",
    "    eval_overview_prompt_adjusted.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview_prompt_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, this already got us up to 58%!\n",
    "\n",
    "So far, we only used the `luminous-base-control` model. Let's see if we can improve our classifications by upgrading to a bigger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': ClassifyInput(chunk='Hey, I did not get a t-shirt in the onboarding. Could I still get one?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'People & Culture',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.11905821963385722, 'Product': 0.0007529779112762469, 'Legal': 0.0026281511497473755, 'Infrastructure': 9.573039863173891e-05, 'Accounting': 0.00976476612874965, 'Communication Department': 0.028255287665935772, 'Research': 1.1433354228584936e-05, 'Finance': 0.19613051178613103, 'CEO Office': 0.00021573178351041137, 'Customer': 0.6430871901879318}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Jonas, we have met each other at the event in Nürnberg, can we meet for a follow up in your Office in Heidelberg?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Sales',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.03466694356346471, 'Product': 0.0006973523989065594, 'Legal': 0.014227284972224445, 'Infrastructure': 0.002131281832159999, 'Accounting': 0.004840585080623741, 'Communication Department': 0.4357360185096552, 'Research': 0.024582544007061204, 'Finance': 0.47856239422605007, 'CEO Office': 0.001123106602569485, 'Customer': 0.003432488807284528}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='My laptop is broken', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'IT Support',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.00011538318333471, 'Product': 0.0010283991187241372, 'Legal': 7.449698229461483e-05, 'Infrastructure': 6.574335612689703e-05, 'Accounting': 0.00010839246965362872, 'Communication Department': 0.0005504623813759523, 'Research': 8.809614760325034e-07, 'Finance': 0.002795481432311151, 'CEO Office': 7.303427156261302e-07, 'Customer': 0.9952600297719872}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Hi, Jonas here. I need something really urgently right now. Could you share your number with me?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Ignore',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.0027921150030431386, 'Product': 0.00015610958884849715, 'Legal': 0.0022234342816152253, 'Infrastructure': 0.00020681160399791868, 'Accounting': 0.0007218434255355227, 'Communication Department': 0.44111224825666556, 'Research': 0.00010729244673203881, 'Finance': 0.4106753395570174, 'CEO Office': 7.920617423034184e-05, 'Customer': 0.1419255996623142}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Hi Jan, is your tool ISO 37301 compliant?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Product',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.0006679883017215352, 'Product': 0.11117125853631647, 'Legal': 0.0950898165199237, 'Infrastructure': 0.023302761713130622, 'Accounting': 0.03443924930051577, 'Communication Department': 0.009087200473199403, 'Research': 0.004482289128038741, 'Finance': 0.6600538892118525, 'CEO Office': 0.0003109469009137648, 'Customer': 0.061394599914387366}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Hi,\\n\\nI am having trouble running your docker container in my environment. It fails to start. Can you help?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Product',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 1.2885345777895933e-05, 'Product': 0.0029619131690842015, 'Legal': 0.00024312863840838438, 'Infrastructure': 0.9906245476696584, 'Accounting': 4.787486747910973e-05, 'Communication Department': 0.00016709970654512957, 'Research': 0.0003537500815320124, 'Finance': 0.0007035160420105038, 'CEO Office': 1.9152356682796523e-06, 'Customer': 0.004883369243835948}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Hi, I want to get a new badge, the photo of me looks ugly and I just got new glasses so it does not look like me. ', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Security',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.035447679019574685, 'Product': 1.4556878890486365e-05, 'Legal': 0.017260719640241097, 'Infrastructure': 1.7835460845620018e-05, 'Accounting': 0.007659397910420652, 'Communication Department': 0.004364193377137608, 'Research': 3.895622804027345e-05, 'Finance': 0.15384327338233697, 'CEO Office': 7.277981555304867e-05, 'Customer': 0.7812806082869597}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='My company has been working on time series and signal processing for a long time. It would make sense to define a joint go to market.', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Research',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 3.888249056491584e-05, 'Product': 0.11590710557768708, 'Legal': 0.000325559107583116, 'Infrastructure': 0.000105693567547841, 'Accounting': 0.0021229120535255904, 'Communication Department': 0.003966121861983323, 'Research': 0.3570190195097128, 'Finance': 0.5194596082420401, 'CEO Office': 0.0002237532842546323, 'Customer': 0.0008313443051006192}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Can you send your models via email?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Communications',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.006060842594286994, 'Product': 0.09480756753345904, 'Legal': 0.021154427727023792, 'Infrastructure': 0.05074683403236992, 'Accounting': 0.02891467444111507, 'Communication Department': 0.13794419680201822, 'Research': 0.016475084879233565, 'Finance': 0.49675613537002145, 'CEO Office': 0.0002994034848488399, 'Customer': 0.1468408331356231}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='H100 cluster available right now. Would you like to procure at low prices?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Research',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.00030471647711875397, 'Product': 0.0013763560758355033, 'Legal': 0.00011520645285504865, 'Infrastructure': 0.07514649552749639, 'Accounting': 0.0118899021620508, 'Communication Department': 0.007440510648774454, 'Research': 0.0033017065860232947, 'Finance': 0.8873053577432958, 'CEO Office': 6.12454622557398e-05, 'Customer': 0.013058502864294285}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='I want to take a sabbatical', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'HR',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.8386597459457881, 'Product': 1.204947209028871e-06, 'Legal': 0.0005955949107618142, 'Infrastructure': 4.5927347973219604e-05, 'Accounting': 0.00440088420785664, 'Communication Department': 0.009917545656902184, 'Research': 0.017405834690762557, 'Finance': 0.12861268897875777, 'CEO Office': 0.00021910712293561128, 'Customer': 0.0001414661910532324}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Your hTTPs Certificate is not valid on your www.aleph-alpha.de', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Security',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.00010525045695257107, 'Product': 4.121667115293105e-05, 'Legal': 0.004337670857013201, 'Infrastructure': 0.7294848026308586, 'Accounting': 0.0001847203046218093, 'Communication Department': 0.1844426426428915, 'Research': 0.00017352866712861211, 'Finance': 0.07688713252695714, 'CEO Office': 5.364385410410433e-06, 'Customer': 0.004337670857013201}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='I already tried to call many times. Can I get a meeting with Jonas?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Communications',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.07100219760312608, 'Product': 0.0016184377445183124, 'Legal': 0.005475101578707347, 'Infrastructure': 0.0005953899730241662, 'Accounting': 0.001147644210527974, 'Communication Department': 0.6736502967544478, 'Research': 0.0013843231623117025, 'Finance': 0.10997035491732951, 'CEO Office': 0.0025066854151630505, 'Customer': 0.13264956864084415}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Hello,\\n\\nI am getting strange errors from your API. It is saying the queue is full, but I am only sending one task at a time. Why is this happening?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Product',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.0002629620382111627, 'Product': 0.02223692957948374, 'Legal': 0.012670220549140852, 'Infrastructure': 0.0729120850776688, 'Accounting': 0.008180509227069187, 'Communication Department': 0.009269731375952349, 'Research': 0.0009178276981501435, 'Finance': 0.03902702681497116, 'CEO Office': 8.808124968285771e-05, 'Customer': 0.8344346263896697}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Can you show me a demo of different use cases your product can solve?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Customer',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.00041603343224054723, 'Product': 0.7177605689549155, 'Legal': 0.004267957365339802, 'Infrastructure': 0.001697676147785359, 'Accounting': 0.0016454441438113435, 'Communication Department': 0.00531155876025379, 'Research': 0.018539180251167547, 'Finance': 0.04447318147492857, 'CEO Office': 0.0002464917743120061, 'Customer': 0.20564190769524557}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='I want to take a week off immediatly', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'HR',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.46299612684321056, 'Product': 1.9808579812054106e-05, 'Legal': 0.009055387538445772, 'Infrastructure': 4.463932369175321e-05, 'Accounting': 0.007991351454381538, 'Communication Department': 0.03812460555218306, 'Research': 0.0002129623027568396, 'Finance': 0.4098781370054803, 'CEO Office': 0.0004508411984741028, 'Customer': 0.07122614020156406}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='Let us celebrate AI day in style, we want to invite you and the CEO to join us.', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Marketing',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.01767441050909784, 'Product': 0.0006437903557333712, 'Legal': 0.0009971220089282922, 'Infrastructure': 0.0002725979880200259, 'Accounting': 0.0001873536745861537, 'Communication Department': 0.7060017099733116, 'Research': 0.0014061674169881045, 'Finance': 0.06566840156870157, 'CEO Office': 0.19001782043943738, 'Customer': 0.017130626065195588}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='I just traveled to Paris for a conference, where can I get the train ride refunded?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'Finance',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.00032538852910948184, 'Product': 9.322537443314816e-05, 'Legal': 1.2228507943008445e-05, 'Infrastructure': 0.00014439028452357174, 'Accounting': 0.002724444798046206, 'Communication Department': 0.0005364749891846305, 'Research': 0.00019735811922369747, 'Finance': 0.08475503576237196, 'CEO Office': 8.946567529803128e-06, 'Customer': 0.9112025070676344}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='How can I work more, I want to work weekends, can I get paid overtime?', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'HR',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.25633979462850376, 'Product': 6.291336782180687e-05, 'Legal': 0.07344258078722231, 'Infrastructure': 7.5888007359612e-05, 'Accounting': 0.03258993137477255, 'Communication Department': 0.04184630021331723, 'Research': 0.005663280891773613, 'Finance': 0.5776706445424228, 'CEO Office': 0.0003195004448393767, 'Customer': 0.01198916574196698}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)},\n",
       " {'input': ClassifyInput(chunk='IT Support', labels=frozenset({'People & Culture', 'Product', 'Legal', 'Infrastructure', 'Accounting', 'Communication Department', 'Research', 'Finance', 'CEO Office', 'Customer'})),\n",
       "  'expected_output': 'I can’t login to Mattermost or Sharepoint, how can I gain access?',\n",
       "  'result': SingleLabelClassifyOutput(scores={'People & Culture': 0.020056602767552136, 'Product': 0.008368983358924628, 'Legal': 0.017717137909874635, 'Infrastructure': 0.27714266636556395, 'Accounting': 0.03309997127739246, 'Communication Department': 0.054519498843658365, 'Research': 0.01563531932812848, 'Finance': 0.115530251640236, 'CEO Office': 0.0009985594532152073, 'Customer': 0.45693100905545425}),\n",
       "  'eval': SingleLabelClassifyEvaluation(correct=False)}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_with_extended = PromptBasedClassify(\n",
    "    instruction=adjusted_prompt, model=LuminousControlModel(\"luminous-supreme-control\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run it again and see if we improved!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_with_extended = Runner(\n",
    "    classify_with_extended,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt & better model\",\n",
    ")\n",
    "run_overview_with_extended = runner_with_extended.run_dataset(cleaned_dataset_id)\n",
    "eval_overview_with_extended = evaluator.evaluate_runs(run_overview_with_extended.id)\n",
    "aggregation_overview_with_extended = aggregator.aggregate_evaluation(\n",
    "    eval_overview_with_extended.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview_with_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using a bigger model further improved our results to 66.66%.\n",
    "\n",
    "As you can see there are plenty of option on how to further enhance the accuracy of our classify task. Notice, for instance, that so far we did not tell our classification task what each class means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_failed_examples(\n",
    "    run_overview_prompt_adjusted.id,\n",
    "    eval_overview_prompt_adjusted.id,\n",
    "    cleaned_dataset_id,\n",
    "    3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model had to 'guess' what we mean by each class purely from the given labels. In order to tackle this issue you could use the `PromptBasedClassifyWithDefinitions` task. This task allows you to also provide a short description for each class.\n",
    "\n",
    "Feel free to further play around and improve our classification example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
