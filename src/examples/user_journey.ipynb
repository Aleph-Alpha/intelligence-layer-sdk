{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from intelligence_layer.core import InMemoryTracer, LuminousControlModel, TextChunk\n",
    "from intelligence_layer.evaluation import (\n",
    "    Aggregator,\n",
    "    Evaluator,\n",
    "    Example,\n",
    "    InMemoryAggregationRepository,\n",
    "    InMemoryDatasetRepository,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    Runner,\n",
    "    evaluation_lineages_to_pandas,\n",
    ")\n",
    "from intelligence_layer.evaluation.evaluation.domain import FailedExampleEvaluation\n",
    "from intelligence_layer.use_cases import (\n",
    "    ClassifyInput,\n",
    "    PromptBasedClassify,\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Effectiveness of LLM-based Email Classification Systems\n",
    "\n",
    "In the fast-paced world of business, effectively managing incoming support emails is crucial. The ability to quickly and accurately classify these emails into the appropriate department and determine their urgency is not just a matter of operational efficiency; it directly impacts customer satisfaction and overall business success. Given the high stakes, it's essential to rigorously evaluate any solution designed to automate this process. This tutorial focuses on the evaluation of a LLM-based program developed to automate the classification of support emails.\n",
    "\n",
    "In an environment brimming with various methodologies and tools, understanding the comparative effectiveness of different approaches is vital. Systematic evaluation allows us to identify which techniques are best suited for specific tasks, understand their strengths and weaknesses, and optimize their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Evaluation\n",
    "\n",
    "To start off, we are only given a few anecdotal examples.\n",
    "Firstly, there are two e-mails, and secondly a number of potential departments to which they should be sent.\n",
    "\n",
    "Let's have a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Hi, my laptop crashed and I can't start it anymore. Do you need the serial number or sth?\",\n",
    "    \"Hello,\\n\\nI am writing my Master's Thesis and would like to investigate the model's performance. Could I get some free credits?\\n\\nCheers, Niklas\",\n",
    "]\n",
    "\n",
    "labels = {\n",
    "    \"Product\",\n",
    "    \"Customer\",\n",
    "    \"CEO Office\",\n",
    "    \"Research\",\n",
    "    \"Finance and Accounting\",\n",
    "    \"Legal\",\n",
    "    \"Communications\",\n",
    "    \"Infrastructure\",\n",
    "    \"Human Resources\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the Intelligence Layer provides some classification tasks out of the box.\n",
    "\n",
    "Let's run it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the default task\n",
    "prompt_based_classify = PromptBasedClassify()\n",
    "\n",
    "# building the input object for each example\n",
    "classify_inputs = [\n",
    "    ClassifyInput(chunk=TextChunk(example), labels=labels) for example in examples\n",
    "]\n",
    "\n",
    "# running the tasks concurrently\n",
    "outputs = prompt_based_classify.run_concurrently(classify_inputs, InMemoryTracer())\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, we have some results, but they aren't really legible (yet).\n",
    "So let's look at the sorted individual results for more clarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.sorted_scores for o in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first example 'Communications' gets the highest score, while for the second example the 'Communications' is the clear winner.\n",
    "This suggests that the Finance Department can fix my laptop and the Comms people can reward free credits ... Not very likely.\n",
    "We probably have to do some fine-tuning of our classification approach.\n",
    "\n",
    "However, let's first make sure that this evidence is not anecdotal.\n",
    "For this, we need to do some eval. Luckily, we have by now got access to a few more examples...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intelligence layer offers support to run task evaluations.\n",
    "\n",
    "First, we have to create a dataset inside a repository.\n",
    "There are different repositories (that persist datasets in different ways), but an `InMemoryDatasetRepository` will do for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/classify_examples.json\", \"r\") as file:\n",
    "    labeled_examples: list[dict[str, str]] = json.load(file)\n",
    "\n",
    "labeled_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intelligence Layer offers support to run task evaluations.\n",
    "\n",
    "First, we have to create a dataset inside a repository.\n",
    "There are different repositories (that persist datasets in different ways), but an `InMemoryDatasetRepository` will do for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_repository = InMemoryDatasetRepository()\n",
    "\n",
    "examples = [\n",
    "    Example(\n",
    "        input=ClassifyInput(chunk=TextChunk(example[\"message\"]), labels=labels),\n",
    "        expected_output=example[\"label\"],\n",
    "    )\n",
    "    for example in labeled_examples\n",
    "]\n",
    "\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=examples,\n",
    "    dataset_name=\"MyDataset\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dataset is created, we generate a unique ID. We'll need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, let's actually run an evaluation on it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a few repositories to store runs, evals and aggregated evaluations\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "\n",
    "\n",
    "# each repository is used by a class that has a dedicated responsibility\n",
    "runner = Runner(\n",
    "    prompt_based_classify, dataset_repository, run_repository, \"prompt-based-classify\"\n",
    ")\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyEvaluationLogic(),\n",
    ")\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyAggregationLogic(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating, we must generate predictions for each sample in our datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_overview = runner.run_dataset(dataset_id)\n",
    "run_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's evaluate this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "eval_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation throws many warnings and we will take care of them below.\n",
    "\n",
    "Finally, let's aggregate all individual evaluations to get some eval statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_overview = aggregator.aggregate_evaluation(eval_overview.id)\n",
    "aggregation_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we only predicted around 25% of classes correctly.\n",
    "\n",
    "Again, we get warnings that there are examples for which the expected labels are not part of the labels that the model can predict.\n",
    "\n",
    "## Fixing the Data\n",
    "\n",
    "Let's have a look at a few failed examples in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed_lineages = [\n",
    "    lineage\n",
    "    for lineage in evaluator.evaluation_lineages(eval_overview.id)\n",
    "    if not isinstance(lineage.evaluation.result, FailedExampleEvaluation)\n",
    "]\n",
    "\n",
    "\n",
    "lineages = [\n",
    "    lineage for lineage in passed_lineages if not lineage.evaluation.result.correct\n",
    "][:2]\n",
    "\n",
    "\n",
    "for lineage in lineages:\n",
    "    display(lineage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms it: The first example has an expected label \"IT Support\". However, this label is not listed in the set of labels our model can predict for that example.\n",
    "\n",
    "Let's see how often this is the case and which are the invalid expected labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = [\n",
    "    lineage\n",
    "    for lineage in passed_lineages\n",
    "    if lineage.evaluation.result.expected_label_missing\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Number of examples with invalid expected label: {len(lineages)} out of {len(passed_lineages)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Invalid expected labels: {set([lineage.example.expected_output for lineage in lineages])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this in two ways: Add the missing labels to the set of allowed labels, or change the expected label to the closes matching available label. In this case, we'll do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's translate the other labels into the correct department\n",
    "label_map = {\n",
    "    \"IT Support\": \"Infrastructure\",\n",
    "    \"Sales\": \"Customer\",\n",
    "    \"Marketing\": \"Customer\",\n",
    "    \"Security\": \"Infrastructure\",\n",
    "    \"Finance\": \"Finance and Accounting\",\n",
    "}\n",
    "\n",
    "# we update the existing examples inplace with the correct labels\n",
    "for example in examples:\n",
    "    if example.expected_output in label_map.keys():\n",
    "        example.expected_output = label_map[example.expected_output]\n",
    "\n",
    "# datasets in the IL are immutable, so we must create a new one\n",
    "cleaned_dataset_id = dataset_repository.create_dataset(\n",
    "    examples=examples,\n",
    "    dataset_name=\"CleanedDataset\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Prompt\n",
    "\n",
    "The prompt used for the `PromptBasedClassify`-task looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_based_classify.instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably improve this task by making the prompt more specific, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_prompt = \"\"\"Identify the department that would be responsible for handling the given request.\n",
    "Reply with only the department name.\"\"\"\n",
    "prompt_adjusted_classify = PromptBasedClassify(instruction=adjusted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the cleaned dataset using this task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "runner_prompt_adjusted = Runner(\n",
    "    prompt_adjusted_classify,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt\",\n",
    ")\n",
    "run_overview_prompt_adjusted = runner_prompt_adjusted.run_dataset(cleaned_dataset_id)\n",
    "eval_overview_prompt_adjusted = evaluator.evaluate_runs(run_overview_prompt_adjusted.id)\n",
    "aggregation_overview_prompt_adjusted = aggregator.aggregate_evaluation(\n",
    "    eval_overview_prompt_adjusted.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregation_overview_prompt_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our adjustments improved the accuracy to 58%!\n",
    "\n",
    "So far, we only used the `luminous-base-control` model. Let's see if we can improve our classifications by upgrading to a bigger model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "classify_with_extended = PromptBasedClassify(\n",
    "    instruction=adjusted_prompt, model=LuminousControlModel(\"luminous-supreme-control\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run it again and see if we improved!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "runner_with_extended = Runner(\n",
    "    classify_with_extended,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"running for adjusted prompt & better model\",\n",
    ")\n",
    "run_overview_with_extended = runner_with_extended.run_dataset(cleaned_dataset_id)\n",
    "eval_overview_with_extended = evaluator.evaluate_runs(run_overview_with_extended.id)\n",
    "aggregation_overview_with_extended = aggregator.aggregate_evaluation(\n",
    "    eval_overview_with_extended.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "aggregation_overview_with_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using a bigger model further improved our results to 67%. But there are still wrongly predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "incorrect_predictions_lineages = [\n",
    "    lineage\n",
    "    for lineage in evaluator.evaluation_lineages(eval_overview_prompt_adjusted.id)\n",
    "    if not isinstance(lineage.evaluation.result, FailedExampleEvaluation)\n",
    "    and not lineage.evaluation.result.correct\n",
    "]\n",
    "\n",
    "df = evaluation_lineages_to_pandas(incorrect_predictions_lineages)\n",
    "df[\"input\"] = [i.chunk for i in df[\"input\"]]\n",
    "df[\"predicted\"] = [r.predicted for r in df[\"result\"]]\n",
    "df.reset_index()[[\"example_id\", \"input\", \"expected_output\", \"predicted\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's analyze this in more depth by visualizing how often each label was expected or predicted in a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "by_labels = aggregation_overview_with_extended.statistics.by_label\n",
    "\n",
    "expected_counts_by_labels = {\n",
    "    label: by_labels[label].expected_count for label in by_labels.keys()\n",
    "}\n",
    "predicted_counts_by_labels = {\n",
    "    label: by_labels[label].predicted_count for label in by_labels.keys()\n",
    "}\n",
    "\n",
    "x_axis = numpy.arange(len(expected_counts_by_labels.keys()))\n",
    "pyplot.bar(\n",
    "    x_axis - 0.2, expected_counts_by_labels.values(), width=0.4, label=\"expected counts\"\n",
    ")\n",
    "pyplot.bar(\n",
    "    x_axis + 0.2,\n",
    "    predicted_counts_by_labels.values(),\n",
    "    width=0.4,\n",
    "    label=\"predicted counts\",\n",
    ")\n",
    "pyplot.ylabel(\"Classification count\")\n",
    "pyplot.xlabel(\"Labels\")\n",
    "pyplot.legend()\n",
    "_ = pyplot.xticks(x_axis, by_labels.keys(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our task tends to overpredict the `Customer` label while it underpredicts `Infrastructure`, `CEO Office` and `Product`.\n",
    "\n",
    "We can get even more insight into the classification behaviour of our task by analysing its cross-matrix. From the off-diagonal cells in the cross-matrix we can see the explicit misslabeling for each class. This helps us to see if a specific class is frequently misslabeld as a particular other class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = aggregation_overview_with_extended.statistics.confusion_matrix\n",
    "\n",
    "data = []\n",
    "for (predicted_label, expected_label), count in confusion_matrix.items():\n",
    "    data.append(\n",
    "        {\n",
    "            \"Expected Label\": expected_label,\n",
    "            \"Predicted Label\": predicted_label,\n",
    "            \"Count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pandas.DataFrame(data)\n",
    "df = df.pivot(index=\"Expected Label\", columns=\"Predicted Label\", values=\"Count\")\n",
    "df = df.fillna(0)\n",
    "df = df.reindex(\n",
    "    index=labels, columns=labels, fill_value=0\n",
    ")  # this will add any labels that were neither expected nor predicted\n",
    "df = df.style.background_gradient(cmap=\"grey\", vmin=df.min().min(), vmax=df.max().max())\n",
    "df = df.format(\"{:.0f}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we can see that the bias towards the `Customer` class does not come at the cost of one particular other class, but is caused by a more general mislabeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is plenty of room for further improvements of our classification task. \n",
    "\n",
    "Notice, for instance, that so far we did not tell our classification task what each class means.\n",
    "\n",
    "The model had to 'guess' what we mean by each class purely from the given labels. In order to tackle this issue you could use the `PromptBasedClassifyWithDefinitions` task. This task allows you to also provide a short description for each class.\n",
    "\n",
    "Feel free to further play around and improve our classification example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
