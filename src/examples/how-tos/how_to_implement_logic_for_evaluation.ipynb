{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from intelligence_layer.evaluation.dataset.domain import Example\n",
    "from intelligence_layer.evaluation.evaluation.evaluator import (\n",
    "    SingleOutputEvaluationLogic,\n",
    ")\n",
    "from typing import Iterable\n",
    "from intelligence_layer.evaluation.aggregation.aggregator import AggregationLogic\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement logic for the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check the dataset you are using for the `Example` data types \"`Input`\" and \"`ExpectedOutput`\" and the `Task` for the `Output` data type.\n",
    "2. Create an `Evaluation` type that contains the domain-specific evaluation result for a single `Example`.\n",
    "3. Decide if you want to use a single `Output` per `Example`, or multiple, during your evaluation to generate your evaluation results.\n",
    "   - For single, we recommend to implement a `SingleOutputEvaluationLogic`.\n",
    "   - For multiple, implement an `EvaluationLogic`.'\n",
    "4. Implement the evaluation logic in the `do_evaluate_single_output` or `do_evaluate` methods for `SingleOutputEvaluationLogic` or `EvaluationLogic` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "In the following example we want to evaluate a story-generating task that generates a story to a topic with a targeted word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - This is only redefined here for completeness. Normally these would be imported.\n",
    "# Note that we do not have an ExpectedOutput here\n",
    "class StoryTaskInput(BaseModel):\n",
    "    topic: str\n",
    "    targeted_word_count: int\n",
    "\n",
    "\n",
    "class StoryTaskOutput(BaseModel):\n",
    "    story: str\n",
    "\n",
    "\n",
    "# Step 2 - We want to analyze the if the word count is accurate\n",
    "class StoryEvaluation(BaseModel):\n",
    "    word_count_off_by: int\n",
    "\n",
    "\n",
    "class StoryEvaluationLogic(\n",
    "    # Step 3 - We only need a single output to analyze the word count\n",
    "    SingleOutputEvaluationLogic[StoryTaskInput, StoryTaskOutput, None, StoryEvaluation]\n",
    "):\n",
    "    def do_evaluate_single_output(\n",
    "        self, example: Example[StoryTaskInput, None], output: StoryTaskOutput\n",
    "    ) -> StoryEvaluation:\n",
    "        # Step 4 - Implement the domain specific logic\n",
    "        output_word_count = len(output.story.split())\n",
    "        word_count_off_by = output_word_count - example.input.targeted_word_count\n",
    "        return StoryEvaluation(word_count_off_by=word_count_off_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement logic for the aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Implement the evaluation logic for your use-case.\n",
    "1. Create an `AggregatedEvaluation` type that contains the domain specific aggregated data for the evaluation.\n",
    "2. Implement an `AggregationLogic` for your data types\n",
    "3. Implement the domain-specific logic in the `aggregate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "In the following example, we calculate basic statistics on how much off the word count is based on the abovementioned example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - See the example above\n",
    "\n",
    "\n",
    "# Step 1\n",
    "class StoryAggregation(BaseModel):\n",
    "    wc_off_mean: float\n",
    "    wc_off_median: int\n",
    "    wc_off_std: float\n",
    "\n",
    "\n",
    "# Step 2\n",
    "class StoryAggregationLogic(AggregationLogic[StoryEvaluation, StoryAggregation]):\n",
    "    def aggregate(self, evaluations: Iterable[StoryEvaluation]) -> StoryAggregation:\n",
    "        # Step 3\n",
    "        word_counts = np.array(\n",
    "            [evaluation.word_count_off_by for evaluation in evaluations]\n",
    "        )\n",
    "        wc_off_mean = np.mean(word_counts)\n",
    "        wc_off_median = np.median(word_counts)\n",
    "        wc_off_std = np.std(word_counts)\n",
    "        return StoryAggregation(\n",
    "            wc_off_mean=wc_off_mean, wc_off_median=wc_off_median, wc_off_std=wc_off_std\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-d3iSWYpm-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
