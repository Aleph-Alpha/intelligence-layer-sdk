{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.connectors import (\n",
    "    DefaultArgillaClient,\n",
    "    Field,\n",
    "    Question,\n",
    "    RecordData,\n",
    "    ArgillaEvaluation,\n",
    ")\n",
    "from intelligence_layer.evaluation import (\n",
    "    ArgillaEvaluationRepository,\n",
    "    InMemoryEvaluationRepository,\n",
    "    ArgillaEvaluationLogic,\n",
    "    SuccessfulExampleOutput,\n",
    "    ArgillaAggregator,\n",
    "    Example,\n",
    "    RecordDataSequence,\n",
    "    AggregationLogic,\n",
    "    InMemoryAggregationRepository,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate with human evaluation via argilla\n",
    "1. Initialize an argilla client with the correct settings for your setup\n",
    "   - By default, the url and api key are read from the environment variables `ARGILLA_API_URL` and `ARGILLA_API_KEY`\n",
    "2. Create `Question`s and `Field`s to structure the data that will be displayed in argilla\n",
    "3. Choose an argilla workspace and get its ID\n",
    "4. Create a repository\n",
    "5. Implement ArgillaEvaluationLogic\n",
    "6. Submit tasks to the argilla instance by running the `ArgillaEvaluator`\n",
    "   - Make sure to save the `EvaluationOverview.id`, as it is needed to retrieve the results later\n",
    "7. **Use the argilla web platform to evaluate** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "client = DefaultArgillaClient(\n",
    "    # api_url=\"<<your url here>>\" ...\n",
    ")\n",
    "# Step 2\n",
    "questions = [\n",
    "    Question(\n",
    "        name=\"rating\",\n",
    "        title=\"Funniness\",\n",
    "        description=\"How funny do you think is the joke? Rate it from 1-5.\",\n",
    "        options=range(1, 6),\n",
    "    )\n",
    "]\n",
    "fields = [\n",
    "    Field(name=\"input\", title=\"Topic\"),\n",
    "    Field(name=\"output\", title=\"Joke\"),\n",
    "]\n",
    "# Step 3\n",
    "workspace_id = client.ensure_workspace_exists(\"my-workspace-name\")\n",
    "\n",
    "# Step 4\n",
    "data_storage = (\n",
    "    InMemoryEvaluationRepository()\n",
    ")  # Use FileEvaluationRepository for persistent results\n",
    "evaluation_repository = ArgillaEvaluationRepository(\n",
    "    data_storage, client, workspace_id, fields, questions\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5\n",
    "# redefined for completeness\n",
    "class StoryTaskInput(BaseModel):\n",
    "    topic: str\n",
    "    targeted_word_count: int\n",
    "\n",
    "\n",
    "class StoryTaskOutput(BaseModel):\n",
    "    story: str\n",
    "\n",
    "\n",
    "class CustomArgillaEvaluationLogic(\n",
    "    ArgillaEvaluationLogic[StoryTaskInput, StoryTaskOutput, None]\n",
    "):\n",
    "    def _to_record(\n",
    "        self,\n",
    "        example: Example[StoryTaskInput, None],\n",
    "        *output: SuccessfulExampleOutput[StoryTaskOutput],\n",
    "    ) -> RecordDataSequence:\n",
    "        return RecordDataSequence(\n",
    "            records=[\n",
    "                RecordData(\n",
    "                    content={\n",
    "                        \"input\": example.input.topic,\n",
    "                        \"output\": sample.output.story,\n",
    "                    },\n",
    "                    example_id=example.id,\n",
    "                )\n",
    "                for sample in output\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "evaluation_logic = CustomArgillaEvaluationLogic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# we skip this as we do not have a dataset or run in this example\n",
    "\n",
    "# Step 6\n",
    "runs_to_evaluate = \"your_run_ids_of_interest\"\n",
    "\n",
    "evaluator = ArgillaEvaluator(\n",
    "    ..., evaluation_repository, description=\"My evaluation description\", evaluation_logic=evaluation_logic\n",
    ")\n",
    "evaluation_overview = evaluator.evaluate_runs(runs_to_evaluate)\n",
    "print(\"ID to retrieve results later: \", evaluation_overview.id)\n",
    "\n",
    "# Step 7\n",
    "\n",
    "####################################\n",
    "# Evaluate via the argilla UI here #\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to aggregate an argilla evaluation\n",
    "0. Submit tasks to argilla and perform an evaluation (see [here](#how-to-evaluate-with-human-evaluation-via-argilla))\n",
    "1. Implement an `AggregationLogic` that takes an `ArgillaEvaluation`s as input\n",
    "2. Remember the ID of the evaluation and the name of the argilla workspace that you want to aggregate\n",
    "3. Initialize all necessary repositories\n",
    "4. Aggregate the results with an ArgillaAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "class CustomArgillaAggregation(BaseModel):\n",
    "    avg_funniness: float\n",
    "\n",
    "\n",
    "class CustomArgillaAggregationLogic(\n",
    "    AggregationLogic[ArgillaEvaluation, CustomArgillaAggregation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self, evaluations: Iterable[ArgillaEvaluation]\n",
    "    ) -> CustomArgillaAggregation:\n",
    "        evaluation_list = list(evaluations)\n",
    "        total_score = sum(\n",
    "            evaluation.metadata[\n",
    "                \"rating\"\n",
    "            ]  # This name is defined by the `Question`s given to the argilla repository during submission\n",
    "            for evaluation in evaluation_list\n",
    "        )\n",
    "        return CustomArgillaAggregation(\n",
    "            avg_funniness=total_score / len(evaluation_list)\n",
    "        )\n",
    "\n",
    "\n",
    "# Step 2 - See the first example for more info\n",
    "eval_id = \"my-previous-eval-id\"\n",
    "client = DefaultArgillaClient()\n",
    "workspace_id = client.ensure_workspace_exists(\"my-workspace-name\")\n",
    "\n",
    "# Step3\n",
    "evaluation_repository = ArgillaEvaluationRepository(\n",
    "    InMemoryEvaluationRepository(), client, workspace_id, None, None\n",
    ")\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "aggregation_logic = CustomArgillaAggregationLogic()\n",
    "\n",
    "# Step 4\n",
    "aggregator = ArgillaAggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"My aggregation description\",\n",
    "    aggregation_logic,\n",
    ")\n",
    "aggregation = aggregator.aggregate_evaluation(eval_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-d3iSWYpm-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
