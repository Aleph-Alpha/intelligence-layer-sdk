{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Effectiveness of LLM-based Email Classification Systems\n",
    "\n",
    "In the fast-paced world of business, effectively managing incoming support emails is crucial. The ability to quickly and accurately classify these emails into the appropriate department and determine their urgency is not just a matter of operational efficiency; it directly impacts customer satisfaction and overall business success. Given the high stakes, it's essential to rigorously evaluate any solution designed to automate this process. This tutorial focuses on the evaluation of a LLM-based program developed to automate the classification of support emails.\n",
    "\n",
    "### The Importance of Evaluating LLM-Based Solutions\n",
    "\n",
    "In an environment brimming with various methodologies and tools, understanding the comparative effectiveness of different approaches is vital. Systematic evaluation allows us to identify which techniques are best suited for specific tasks, understand their strengths and weaknesses, and optimize their performance.\n",
    "\n",
    "### Business Impact and Evaluation Challenges\n",
    "\n",
    "For a business, the deployment of an LLM-based email classification system can be transformative. It can streamline workflows, expedite response times, and ensure that critical issues are swiftly addressed. However, evaluating these models presents unique challenges, particularly due to the generative nature of LLMs.\n",
    "\n",
    "Unlike traditional models, LLMs generate outputs based on complex and often less transparent internal processes. This complexity can make it challenging to understand why a model categorizes an email in a certain way, which is crucial for business stakeholders who need to trust and rely on the system's decisions. Moreover, in the context of email classification, the nuances of language and context can lead to misclassifications, which can have significant business ramifications.\n",
    "\n",
    "### Addressing Evaluation Challenges with Traceability\n",
    "\n",
    "One way to mitigate these challenges is through traceability of model outputs. By implementing mechanisms that allow us to trace back how and why a particular classification was made, we can gain insights into the model's decision-making process. This transparency is invaluable for fine-tuning the implementation, addressing misclassifications, and building trust among stakeholders (and users).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: import task and try out two examples that product manager gave me\n",
    "# looks good, let's do eval..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from intelligence_layer.evaluation import (\n",
    "    Evaluator,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    InMemoryDatasetRepository,\n",
    "    InMemoryAggregationRepository,\n",
    "    Runner,\n",
    "    Aggregator,\n",
    ")\n",
    "from intelligence_layer.use_cases import (\n",
    "    PromptBasedClassify,\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt_based_classify = PromptBasedClassify()\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyEvaluationLogic(),\n",
    ")\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"single-label-classify\",\n",
    "    SingleLabelClassifyAggregationLogic(),\n",
    ")\n",
    "runner = Runner(prompt_based_classify, dataset_repository, run_repository, \"prompt-based-classify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a single example and see what comes of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core import TextChunk, NoOpTracer\n",
    "from intelligence_layer.use_cases import ClassifyInput\n",
    "from intelligence_layer.evaluation import Example\n",
    "\n",
    "\n",
    "classify_input = ClassifyInput(\n",
    "    chunk=TextChunk(\"This is good\"),\n",
    "    labels=frozenset({\"positive\", \"negative\"}),\n",
    ")\n",
    "\n",
    "single_example_dataset = dataset_repository.create_dataset(\n",
    "    examples=[Example(input=classify_input, expected_output=\"positive\")]\n",
    ")\n",
    "\n",
    "run_overview = runner.run_dataset(single_example_dataset, NoOpTracer())\n",
    "evaluation_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "aggregation_overview = aggregator.aggregate_evaluation(evaluation_overview.id)\n",
    "\n",
    "print(\"Statistics: \", aggregation_overview.statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!\n",
    "\n",
    "Let's have a look at this [dataset](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) for more elaborate evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "all_data = list(dataset[test_set_name])\n",
    "data = all_data[:25]  # this has 573 datapoints, let's take a look at 25 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`.\n",
    "\n",
    "This is the target structure:\n",
    "\n",
    "``` python\n",
    "class Example(BaseModel, Generic[Input, ExpectedOutput]):\n",
    "    input: Input\n",
    "    expected_output: ExpectedOutput\n",
    "    id: Optional[str] = Field(default_factory=lambda: str(uuid4()))\n",
    "\n",
    "```\n",
    "\n",
    "We want the `input` in each `Example` to mimic the input of an actual task, therefore we must every time include the text (chunk) and all possible labels.\n",
    "The `expected_output` shall correspond to anything we wish to compare our generated output to.\n",
    "In this case, that means the correct class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = list(set(c for d in data for c in d[\"label_name\"]))\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(d[\"text\"]), labels=all_labels),\n",
    "            expected_output=d[\"label_name\"],\n",
    "        )\n",
    "        for d in data\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!\n",
    "\n",
    "Note that this may take a while as we parallelise the tasks in a way that accommodates the inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_overview = runner.run_dataset(dataset_id)\n",
    "evaluation_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "aggregation_overview = aggregator.aggregate_evaluation(evaluation_overview.id)\n",
    "aggregation_overview.raise_on_evaluation_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases import SingleLabelClassifyEvaluation\n",
    "\n",
    "print(\"Percentage correct:\", aggregation_overview.statistics.percentage_correct)\n",
    "print(\n",
    "    \"First example:\",\n",
    "    evaluation_repository.example_evaluations(\n",
    "        evaluation_id=next(iter(aggregation_overview.evaluation_overviews)).id,\n",
    "        evaluation_type=SingleLabelClassifyEvaluation,\n",
    "    )[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, let's see if we can achieve a better result with our EmbeddingBasedClassifier.\n",
    "Here, we have to provide some example for each class.\n",
    "\n",
    "We can even reuse our data repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Any, Mapping, Sequence\n",
    "from intelligence_layer.use_cases import (\n",
    "    MultiLabelClassifyEvaluationLogic,\n",
    "    MultiLabelClassifyAggregationLogic,\n",
    "    EmbeddingBasedClassify,\n",
    "    LabelWithExamples,\n",
    ")\n",
    "\n",
    "\n",
    "def build_labels_and_examples(hf_data: Any) -> Mapping[str, Sequence[str]]:\n",
    "    examples = defaultdict(list)\n",
    "    for d in hf_data:\n",
    "        labels = d[\"label_name\"]\n",
    "        for label in labels:\n",
    "            if len(examples[label]) < 20:\n",
    "                examples[label].append(d[\"text\"])\n",
    "    return examples\n",
    "\n",
    "\n",
    "client = LimitedConcurrencyClient.from_env()\n",
    "embedding_based_classify = EmbeddingBasedClassify(\n",
    "    client=client,\n",
    "    labels_with_examples=[\n",
    "        LabelWithExamples(name=name, examples=examples)\n",
    "        for name, examples in build_labels_and_examples(all_data[25:]).items()\n",
    "    ],\n",
    ")\n",
    "eval_logic = MultiLabelClassifyEvaluationLogic(threshold=0.6)\n",
    "aggregation_logic = MultiLabelClassifyAggregationLogic()\n",
    "\n",
    "embedding_based_classify_evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"multi-label-classify\",\n",
    "    eval_logic,\n",
    ")\n",
    "embedding_based_classify_aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"multi-label-classify\",\n",
    "    aggregation_logic,\n",
    ")\n",
    "embedding_based_classify_runner = Runner(\n",
    "    embedding_based_classify,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"embedding-based-classify\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_based_classify_run_result = embedding_based_classify_runner.run_dataset(\n",
    "    dataset_id\n",
    ")\n",
    "embedding_based_classify_evaluation_result = (\n",
    "    embedding_based_classify_evaluator.evaluate_runs(\n",
    "        embedding_based_classify_run_result.id\n",
    "    )\n",
    ")\n",
    "embedding_based_classify_aggregation_result = (\n",
    "    embedding_based_classify_aggregator.aggregate_evaluation(\n",
    "        embedding_based_classify_evaluation_result.id\n",
    "    )\n",
    ")\n",
    "embedding_based_classify_aggregation_result.raise_on_evaluation_failure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_based_classify_aggregation_result.statistics.macro_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our method has a great recall value, but we tend to falsely predict labels at times.\n",
    "\n",
    "Note, that the evaluation criteria for the multiple label approach are a lot harsher; we evaluate whether we correctly predict all labels & not just one of the correct ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Wrap up\n",
    "\n",
    "There you go, this is how to evaluate any task using the 'Intelligence Layer'-framework.\n",
    "Simply define an `Evaluator` that takes the target `Task` as input and customize the `do_evaluate` as well as `aggregate` methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
