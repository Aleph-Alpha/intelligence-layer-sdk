{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM-based tasks\n",
    "\n",
    "Evaluating LLM-based use cases is pivotal for several reasons.\n",
    "First, with the myriad of methods available, comparability becomes essential.\n",
    "By systematically evaluating different approaches, we can discern which techniques are more effective or suited for specific tasks, fostering a deeper understanding of their strengths and weaknesses.\n",
    "Secondly, optimization plays a significant role. Without proper evaluation metrics and rigorous testing, it becomes challenging to fine-tune methods and/or models to achieve their maximum potential.\n",
    "Moreover, drawing comparisons with state-of-the-art (SOTA) and open-source methods is crucial.\n",
    "Such comparisons not only provide benchmarks but also enable users to determine the value-added by proprietary or newer models over freely available counterparts.\n",
    "\n",
    "However, evaluating LLMs, especially in the domain of text generation, presents unique challenges.\n",
    "Text generation is inherently subjective, and what one evaluator deems coherent and relevant, another might find disjointed or off-topic. This subjectivity complicates the establishment of universal evaluation standards, making it imperative to approach LLM evaluation with a multifaceted and comprehensive strategy.\n",
    "\n",
    "### Evaluating classification use-cases\n",
    "\n",
    "To (at least for now) evade the elusive issue described in the last paragraph, let's have a look at an easier to evaluate methodology: classification.\n",
    "Why is this easier?\n",
    "Well, unlike other tasks such as QA, the result of a classification task is more or less binary (true/false).\n",
    "There are very few grey areas, as it is unlikely that a classification result is somewhat or \"half\" correct.\n",
    "\n",
    "Make sure that you have familiarized yourself with the `PromptBasedClassify` prior to starting this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate our task, an evaluator for it and a repository that stores the evaluation results along with tracing information for the evaluated examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from intelligence_layer.connectors.limited_concurrency_client import LimitedConcurrencyClient\n",
    "from intelligence_layer.core import InMemoryEvaluationRepository\n",
    "from intelligence_layer.core.evaluation.dataset_repository import InMemoryDatasetRepository\n",
    "from intelligence_layer.use_cases.classify.classify import SingleLabelClassifyEvaluator\n",
    "from intelligence_layer.use_cases.classify.prompt_based_classify import PromptBasedClassify\n",
    "\n",
    "\n",
    "client = LimitedConcurrencyClient.from_token(os.getenv(\"AA_TOKEN\"))\n",
    "task = PromptBasedClassify(client)\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "\n",
    "\n",
    "evaluator = SingleLabelClassifyEvaluator(task, evaluation_repository, dataset_repository)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a single example and see what comes of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core.tracer import InMemoryTracer\n",
    "from intelligence_layer.core.chunk import Chunk\n",
    "from intelligence_layer.use_cases.classify.classify import ClassifyInput\n",
    "\n",
    "\n",
    "classify_input = ClassifyInput(\n",
    "        chunk=Chunk(\"This is good\"),\n",
    "        labels=frozenset({\"positive\", \"negative\"}),\n",
    "    )\n",
    "evaluation_tracer = InMemoryTracer()\n",
    "expected_output = \"positive\"\n",
    "evaluation = evaluator.run_and_evaluate(\n",
    "    input=classify_input, tracer=evaluation_tracer, expected_output=[expected_output]\n",
    ")\n",
    "\n",
    "print(\"The task result:\", evaluation)\n",
    "print(\"The expected output:\", expected_output)\n",
    "print(\"The eval result:\", evaluation.correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!\n",
    "\n",
    "Let's have a look at this [dataset](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) for more elaborate evaluaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "all_data = list(dataset[test_set_name])\n",
    "data = all_data[:25] # this has 573 datapoints, let's take a look at 25 for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`.\n",
    "\n",
    "This is the target structure:\n",
    "\n",
    "``` python\n",
    "class Example(BaseModel, Generic[Input, ExpectedOutput]):\n",
    "    input: Input\n",
    "    expected_output: ExpectedOutput\n",
    "    id: Optional[str] = Field(default_factory=lambda: str(uuid4()))\n",
    "\n",
    "```\n",
    "\n",
    "We want the `input` in each `Example` to mimic the input of an actual task, therefore we must every time include the text (chunk) and all possible labels.\n",
    "The `expected_output` shall correspond to anything we wish to compare our generated output to.\n",
    "In this case, that means the correct class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core import Example\n",
    "\n",
    "\n",
    "all_labels = list(set(c for d in data for c in d[\"label_name\"]))\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(\n",
    "                chunk=Chunk(d[\"text\"]),\n",
    "                labels=all_labels\n",
    "            ),\n",
    "            expected_output=d[\"label_name\"]\n",
    "        ) for d in data\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!\n",
    "\n",
    "Note that this may take a while as we parallelise the tasks in a way that accommodates the inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = evaluator.evaluate_dataset(dataset_id)\n",
    "overview.raise_on_evaluation_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", overview.statistics.percentage_correct)\n",
    "print(\"First example:\", overview.statistics.percentage_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, let's see if we can achieve a better result with our EmbeddingBasedClassifier.\n",
    "Here, we have to provide some example for each class.\n",
    "\n",
    "We can even reuse our evaluation and dataset repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Any, Mapping, Sequence\n",
    "from intelligence_layer.use_cases.classify.classify import MultiLabelClassifyEvaluator\n",
    "from intelligence_layer.use_cases.classify.embedding_based_classify import EmbeddingBasedClassify, LabelWithExamples\n",
    "\n",
    "\n",
    "def build_labels_and_examples(hf_data: Any) -> Mapping[str, Sequence[str]]:\n",
    "    examples = defaultdict(list)\n",
    "    for d in hf_data:\n",
    "        labels = d[\"label_name\"]\n",
    "        for label in labels:\n",
    "            if len(examples[label]) < 20:\n",
    "                examples[label].append(d[\"text\"])\n",
    "    return examples\n",
    "\n",
    "\n",
    "embedding_based_classify = EmbeddingBasedClassify(\n",
    "    client,\n",
    "    labels_with_examples=[\n",
    "        LabelWithExamples(\n",
    "            name=name,\n",
    "            examples=examples\n",
    "        ) for name, examples in build_labels_and_examples(all_data[25:]).items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "embedding_based_classify_evaluator = MultiLabelClassifyEvaluator(embedding_based_classify, evaluation_repository, dataset_repository, threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_based_classify_result = embedding_based_classify_evaluator.evaluate_dataset(dataset_id)\n",
    "embedding_based_classify_result.raise_on_evaluation_failure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_based_classify_result.statistics.macro_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our method has a great recall value, but we tend to falsely predict labels at times.\n",
    "\n",
    "Note, that the evaluation criteria for the multiple label approach are a lot harsher; we evaluate whether we correctly predict all labels & not just one of the correct ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Wrap up\n",
    "\n",
    "There you go, this is how to evaluate any task using the 'Intelligence Layer'-framework.\n",
    "Simply define an `Evaluator` that takes the target `Task` as input and customize the `do_evaluate` as well as `aggregate` methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-tfT-HG2V-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
