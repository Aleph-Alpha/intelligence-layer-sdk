{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM-based tasks\n",
    "\n",
    "Evaluating LLM-based use cases is pivotal for several reasons.\n",
    "First, with the myriad of methods available, comparability becomes essential.\n",
    "By systematically evaluating different approaches, we can discern which techniques are more effective or suited for specific tasks, fostering a deeper understanding of their strengths and weaknesses.\n",
    "Secondly, optimization plays a significant role. Without proper evaluation metrics and rigorous testing, it becomes challenging to fine-tune methods and/or models to achieve their maximum potential.\n",
    "Moreover, drawing comparisons with state-of-the-art (SOTA) and open-source methods is crucial.\n",
    "Such comparisons not only provide benchmarks but also enable users to determine the value-added by proprietary or newer models over freely available counterparts.\n",
    "\n",
    "However, evaluating LLMs, especially in the domain of text generation, presents unique challenges.\n",
    "Text generation is inherently subjective, and what one evaluator deems coherent and relevant, another might find disjointed or off-topic. This subjectivity complicates the establishment of universal evaluation standards, making it imperative to approach LLM evaluation with a multifaceted and comprehensive strategy.\n",
    "\n",
    "### Evaluating classification use-cases\n",
    "\n",
    "To (at least for now) evade the elusive issue described in the last paragraph, let's have a look at an easier to evaluate methodology: classification.\n",
    "Why is this easier?\n",
    "Well, unlike other tasks such as QA, the result of a classification task is more or less binary (true/false).\n",
    "There are very few grey areas, as it is unlikely that a classification result is somewhat or \"half\" correct.\n",
    "\n",
    "Make sure that you have familiarized yourself with the `PromptBasedClassify` and `EmbeddingBasedClassify` prior to starting this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate our task and an evaluator for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from intelligence_layer.connectors.limited_concurrency_client import LimitedConcurrencyClient\n",
    "from intelligence_layer.use_cases.classify.classify import ClassifyEvaluator\n",
    "from intelligence_layer.use_cases.classify.prompt_based_classify import PromptBasedClassify\n",
    "\n",
    "\n",
    "client = LimitedConcurrencyClient.from_token(os.getenv(\"AA_TOKEN\"))\n",
    "task = PromptBasedClassify(client)\n",
    "evaluator = ClassifyEvaluator(task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a single example and see what comes of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core.tracer import InMemoryTracer\n",
    "from intelligence_layer.core.chunk import Chunk\n",
    "from intelligence_layer.use_cases.classify.classify import ClassifyInput\n",
    "\n",
    "\n",
    "classify_input = ClassifyInput(\n",
    "        chunk=Chunk(\"This is good\"),\n",
    "        labels=frozenset({\"positive\", \"negative\"}),\n",
    "    )\n",
    "evaluation_tracer = InMemoryTracer()\n",
    "expected_output = \"positive\"\n",
    "evaluation = evaluator.evaluate(\n",
    "    input=classify_input, tracer=evaluation_tracer, expected_output=[expected_output]\n",
    ")\n",
    "\n",
    "print(\"The task result:\", evaluation.output.scores)\n",
    "print(\"The expected output:\", expected_output)\n",
    "print(\"The eval result:\", evaluation.correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!\n",
    "\n",
    "Let's have a look at this [dataset](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) for more elaborate evaluaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "all_data = list(dataset[test_set_name])\n",
    "data, all_data = all_data[:10], all_data[10:] # this has 573 datapoints, let's take a look at 10 for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`.\n",
    "\n",
    "This is the target structure:\n",
    "\n",
    "``` python\n",
    "class Example(BaseModel, Generic[Input, ExpectedOutput]):\n",
    "    input: Input\n",
    "    expected_output: ExpectedOutput\n",
    "    id: Optional[str] = Field(default_factory=lambda: str(uuid4()))\n",
    "\n",
    "\n",
    "class Dataset(Protocol, Generic[Input, ExpectedOutput]):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def examples(self) -> Iterable[Example[Input, ExpectedOutput]]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "We want the `input` in each `Example` to mimic the input of an actual task, therefore we must every time include the text (chunk) and all possible labels.\n",
    "The `expected_output` shall correspond to anything we wish to compare our generated output to.\n",
    "In this case, that means the correct class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core.evaluator import Example, SequenceDataset\n",
    "\n",
    "\n",
    "all_labels = list(set(c for d in data for c in d[\"label_name\"]))\n",
    "dataset = SequenceDataset(\n",
    "    name=\"tweet topics\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(\n",
    "                chunk=Chunk(d[\"text\"]),\n",
    "                labels=all_labels\n",
    "            ),\n",
    "            expected_output=d[\"label_name\"]\n",
    "        ) for d in data\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!\n",
    "\n",
    "Note that this may take a while as we parallelise the tasks in a way that accommodates the inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_tracer = InMemoryTracer()\n",
    "result = evaluator.evaluate_dataset(dataset=dataset, tracer=evaluation_tracer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", result.percentage_correct)\n",
    "print(\"First example:\", result.evaluations[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "Because we designed the `ClassifyEvaluator` in a way that allows it to evaluate any `Task` with `ClassifyInput` and `ClassifyOutput` (both single & multi label), it can even evaluate different classifier implementations, such as the `EmbeddingBasedClassifier`.\n",
    "\n",
    "To achieve this, let's first find some examples for the different labels within our eval set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Mapping, Sequence\n",
    "\n",
    "from intelligence_layer.use_cases.classify.embedding_based_classify import LabelWithExamples\n",
    "\n",
    "labels_with_examples_dict: Mapping[str, Sequence[str]] = {}\n",
    "for d in all_data:\n",
    "    for label in d[\"label_name\"]:\n",
    "        if label in labels_with_examples_dict:\n",
    "            labels_with_examples_dict[label].append(d[\"text\"])\n",
    "        else:\n",
    "            labels_with_examples_dict[label] = [d[\"text\"]]\n",
    "labels_with_examples = [LabelWithExamples(name=k, examples=v) for k, v in labels_with_examples_dict.items()]\n",
    "pprint({k: v[:1] for k, v in labels_with_examples_dict.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's instantiate our `EmbeddingBasedClassify`-task with these examples.\n",
    "Again, this may take a few seconds, because we embed all examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases.classify.embedding_based_classify import EmbeddingBasedClassify\n",
    "\n",
    "\n",
    "ebc = EmbeddingBasedClassify(labels_with_examples, client)\n",
    "ebc_evaluator = ClassifyEvaluator(ebc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebc_evaluation_tracer = InMemoryTracer()\n",
    "ebc_result = ebc_evaluator.evaluate_dataset(dataset=dataset, tracer=ebc_evaluation_tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's print part of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", ebc_result.percentage_correct)\n",
    "print(\"First example:\", ebc_result.evaluations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our `EmbeddingBasedClassify` outperformed the prompt-based approach here.\n",
    "However, also note the small sample size of 10.\n",
    "To achieve statistical signifance in evaluation, we generally recommend evaluating on at least 100, if not 1000, examples.\n",
    "\n",
    "In the case at hand, we can note that the embedding-based approach likely benefitted from the large examples we were able to provide on the basis of the extensive dataset.\n",
    "Generally, we recommend using this approach once you can provide around 10 or more examples per label.\n",
    "\n",
    "### Wrap up\n",
    "\n",
    "There you go, this is how to evaluate any task using the Intelligence Layer framework.\n",
    "Simply define an `Evaluator` that takes the target `Task` as input and customize the `evaluate` as well as `aggregate` methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-tfT-HG2V-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
