{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, Mapping, Sequence\n",
    "\n",
    "from intelligence_layer.connectors import LimitedConcurrencyClient\n",
    "from intelligence_layer.core import NoOpTracer, TextChunk\n",
    "from intelligence_layer.evaluation import (\n",
    "    Aggregator,\n",
    "    Evaluator,\n",
    "    Example,\n",
    "    InMemoryAggregationRepository,\n",
    "    InMemoryDatasetRepository,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    Runner,\n",
    "    RepositoryNavigator,\n",
    "    evaluation_lineages_to_pandas,\n",
    ")\n",
    "from intelligence_layer.use_cases import (\n",
    "    ClassifyInput,\n",
    "    EmbeddingBasedClassify,\n",
    "    LabelWithExamples,\n",
    "    MultiLabelClassifyAggregationLogic,\n",
    "    MultiLabelClassifyEvaluationLogic,\n",
    "    PromptBasedClassify,\n",
    "    SingleLabelClassifyAggregationLogic,\n",
    "    SingleLabelClassifyEvaluation,\n",
    "    SingleLabelClassifyEvaluationLogic,\n",
    "    SingleLabelClassifyOutput,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM-based tasks\n",
    "\n",
    "Evaluating LLM-based use cases is pivotal for several reasons.\n",
    "First, with the myriad of methods available, comparability becomes essential.\n",
    "By systematically evaluating different approaches, we can discern which techniques are more effective or suited for specific tasks, fostering a deeper understanding of their strengths and weaknesses.\n",
    "Secondly, optimization plays a significant role. Without proper evaluation metrics and rigorous testing, it becomes challenging to fine-tune methods and/or models to achieve their maximum potential.\n",
    "Moreover, drawing comparisons with state-of-the-art (SOTA) and open-source methods is crucial.\n",
    "Such comparisons not only provide benchmarks but also enable users to determine the value-added by proprietary or newer models over freely available counterparts.\n",
    "\n",
    "However, evaluating LLMs, especially in the domain of text generation, presents unique challenges.\n",
    "Text generation is inherently subjective, and what one evaluator deems coherent and relevant, another might find disjointed or off-topic. This subjectivity complicates the establishment of universal evaluation standards, making it imperative to approach LLM evaluation with a multifaceted and comprehensive strategy.\n",
    "\n",
    "### Evaluating classification use-cases\n",
    "\n",
    "To (at least for now) evade the elusive issue described in the last paragraph, let's have a look at an easier to evaluate methodology: classification.\n",
    "Why is this easier?\n",
    "Well, unlike other tasks such as QA, the result of a classification task is more or less binary (true/false).\n",
    "There are very few grey areas, as it is unlikely that a classification result is somewhat or \"half\" correct.\n",
    "\n",
    "Make sure that you have familiarized yourself with the [PromptBasedClassify](classification.ipynb#prompt-based-single-label-classification) prior to starting this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate our task, as well as, a runner, an evaluator and an aggregator for it. Furthermore, we need the corresponding repositories that store the results of each step along with tracing information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = PromptBasedClassify()\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "run_repository = InMemoryRunRepository()\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "evaluation_logic = SingleLabelClassifyEvaluationLogic()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "aggregation_logic = SingleLabelClassifyAggregationLogic()\n",
    "\n",
    "\n",
    "runner = Runner(task, dataset_repository, run_repository, \"prompt-based-classify\")\n",
    "evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"single-label-classify\",\n",
    "    evaluation_logic,\n",
    ")\n",
    "aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"single-label-classify\",\n",
    "    aggregation_logic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a single example and see what comes of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_input = ClassifyInput(\n",
    "    chunk=TextChunk(\"This is good\"),\n",
    "    labels=frozenset({\"positive\", \"negative\"}),\n",
    ")\n",
    "\n",
    "single_example_dataset = dataset_repository.create_dataset(\n",
    "    examples=[Example(input=classify_input, expected_output=\"positive\")],\n",
    "    dataset_name=\"ClassifyDataset\",\n",
    ")\n",
    "\n",
    "run_overview = runner.run_dataset(single_example_dataset.id, NoOpTracer())\n",
    "evaluation_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "aggregation_overview = aggregator.aggregate_evaluation(evaluation_overview.id)\n",
    "\n",
    "print(\"Statistics: \", aggregation_overview.statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! The example was classified correctly.\n",
    "\n",
    "Next, we will have a look at this pre-defined [dataset of tweets](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) for more elaborate evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "all_data = list(dataset[test_set_name])\n",
    "data = all_data[:25]  # this has 573 datapoints, let's take a look at 25 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`.\n",
    "\n",
    "This is the target structure:\n",
    "\n",
    "``` python\n",
    "class Example(BaseModel, Generic[Input, ExpectedOutput]):\n",
    "    input: Input\n",
    "    expected_output: ExpectedOutput\n",
    "    id: Optional[str] = Field(default_factory=lambda: str(uuid4()))\n",
    "\n",
    "```\n",
    "\n",
    "We want the `input` in each `Example` to mimic the input of an actual task. Therefore, we have to always include the text (chunk) and all possible labels.\n",
    "The `expected_output` shall correspond to anything we wish to compare our generated output to.\n",
    "In this case, that means the correct class(es), i.e., the label name(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = list(set(label_name for item in data for label_name in item[\"label_name\"]))\n",
    "dataset = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(chunk=TextChunk(item[\"text\"]), labels=all_labels),\n",
    "            expected_output=item[\"label_name\"][0],\n",
    "        )\n",
    "        for item in data\n",
    "    ],\n",
    "    dataset_name=\"tweet_topic_multi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!\n",
    "\n",
    "Note that this may take a while as we parallelise the tasks in a way that accommodates the inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_overview = runner.run_dataset(dataset.id)\n",
    "evaluation_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "aggregation_overview = aggregator.aggregate_evaluation(evaluation_overview.id)\n",
    "aggregation_overview.raise_on_evaluation_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", aggregation_overview.statistics.percentage_correct)\n",
    "\n",
    "# You can also use evaluator.evaluation_lineages for an easier use, but that only works if the evaluator is still in memory.\n",
    "navigator = RepositoryNavigator(\n",
    "    dataset_repository=dataset_repository,\n",
    "    run_repository=run_repository,\n",
    "    evaluation_repository=evaluation_repository,\n",
    ")\n",
    "lineages = navigator.evaluation_lineages(\n",
    "    next(iter(aggregation_overview.evaluation_overviews)).id,\n",
    "    input_type=ClassifyInput,\n",
    "    expected_output_type=SingleLabelClassifyOutput,\n",
    "    output_type=Sequence[str],\n",
    "    evaluation_type=SingleLabelClassifyEvaluation,\n",
    ")\n",
    "evaluation_lineages_to_pandas(lineages).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to the `PromptBasedClassify` we now gonne use the `EmbeddingBasedClassify` for multi label classifications.\n",
    "In this case, we have to provide some example for each class.\n",
    "\n",
    "We can even reuse our data repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels_and_examples(hf_data: Any) -> Mapping[str, Sequence[str]]:\n",
    "    examples = defaultdict(list)\n",
    "    for item in hf_data:\n",
    "        labels = item[\"label_name\"]\n",
    "        for label in labels:\n",
    "            if len(examples[label]) < 20:\n",
    "                examples[label].append(item[\"text\"])\n",
    "    return examples\n",
    "\n",
    "\n",
    "client = LimitedConcurrencyClient.from_env()\n",
    "embedding_based_classify = EmbeddingBasedClassify(\n",
    "    client=client,\n",
    "    labels_with_examples=[\n",
    "        LabelWithExamples(name=name, examples=examples)\n",
    "        for name, examples in build_labels_and_examples(all_data[25:]).items()\n",
    "    ],\n",
    ")\n",
    "eval_logic = MultiLabelClassifyEvaluationLogic(threshold=0.60)\n",
    "aggregation_logic = MultiLabelClassifyAggregationLogic()\n",
    "\n",
    "embedding_based_classify_runner = Runner(\n",
    "    embedding_based_classify,\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    \"embedding-based-classify\",\n",
    ")\n",
    "embedding_based_classify_evaluator = Evaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    evaluation_repository,\n",
    "    \"multi-label-classify\",\n",
    "    eval_logic,\n",
    ")\n",
    "embedding_based_classify_aggregator = Aggregator(\n",
    "    evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"multi-label-classify\",\n",
    "    aggregation_logic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_based_classify_run_result = embedding_based_classify_runner.run_dataset(\n",
    "    dataset.id\n",
    ")\n",
    "embedding_based_classify_evaluation_result = (\n",
    "    embedding_based_classify_evaluator.evaluate_runs(\n",
    "        embedding_based_classify_run_result.id\n",
    "    )\n",
    ")\n",
    "embedding_based_classify_aggregation_result = (\n",
    "    embedding_based_classify_aggregator.aggregate_evaluation(\n",
    "        embedding_based_classify_evaluation_result.id\n",
    "    )\n",
    ")\n",
    "embedding_based_classify_aggregation_result.raise_on_evaluation_failure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_based_classify_aggregation_result.statistics.macro_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our method has a great recall value, i.e. all relevant labels are retrieved. However, the low precision value indicates that we tend to falsely predict labels at times.\n",
    "\n",
    "Note, that the evaluation criteria for the multiple label approach are a lot harsher; we evaluate whether we correctly predict all labels & not just one of the correct ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Wrap up\n",
    "\n",
    "There you go, this is how to evaluate any task using the 'Intelligence Layer'-framework.\n",
    "Simply define an `Evaluator` that takes the target `Task` as input and customize the `do_evaluate` as well as `aggregate` methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
