{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM-based tasks\n",
    "\n",
    "Evaluating LLM-based use cases is pivotal for several reasons.\n",
    "First, with the myriad of methods available, comparability becomes essential.\n",
    "By systematically evaluating different approaches, we can discern which techniques are more effective or suited for specific tasks, fostering a deeper understanding of their strengths and weaknesses.\n",
    "Secondly, optimization plays a significant role. Without proper evaluation metrics and rigorous testing, it becomes challenging to fine-tune methods and/or models to achieve their maximum potential.\n",
    "Moreover, drawing comparisons with state-of-the-art (SOTA) and open-source methods is crucial.\n",
    "Such comparisons not only provide benchmarks but also enable users to determine the value-added by proprietary or newer models over freely available counterparts.\n",
    "\n",
    "However, evaluating LLMs, especially in the domain of text generation, presents unique challenges.\n",
    "Text generation is inherently subjective, and what one evaluator deems coherent and relevant, another might find disjointed or off-topic. This subjectivity complicates the establishment of universal evaluation standards, making it imperative to approach LLM evaluation with a multifaceted and comprehensive strategy.\n",
    "\n",
    "### Evaluating classification use-cases\n",
    "\n",
    "To (at least for now) evade the elusive issue described in the last paragraph, let's have a look at an easier to evaluate methodology: classification.\n",
    "Make sure that you have familiarized yourself with the `SingleLabelClassify` and `EmbeddingBasedClassify` prior to starting this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to instantiate an evaluator that takes our classify methodology (`task`) and some datapoints and returns some evaluation metrics.\n",
    "\n",
    "First, let's evaluate a single example and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases.classify.classify import ClassifyEvaluator\n",
    "\n",
    "evaluator = ClassifyEvaluator(task)\n",
    "classify_input = ClassifyInput(\n",
    "        chunk=Chunk(\"This is good\"),\n",
    "        labels=frozenset({\"positive\", \"negative\"}),\n",
    "    )\n",
    "evaluation_logger = InMemoryDebugLogger(name=\"evaluation logger\")\n",
    "expected_output = \"positive\"\n",
    "evaluation = evaluator.evaluate(\n",
    "    input=classify_input, logger=evaluation_logger, expected_output=[expected_output]\n",
    ")\n",
    "\n",
    "print(\"The task result:\", evaluation.output.scores)\n",
    "print(\"The expected output:\", expected_output)\n",
    "print(\"The eval result:\", evaluation.correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!\n",
    "Let's now try to find a dataset to use.\n",
    "We found this [dataset](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) on huggingface, let's see if we can get an evaluation going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "data = list(dataset[test_set_name])[:10] # this has 573 datapoints, let's take a look at 20 for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from intelligence_layer.core.evaluator import Example, Dataset\n",
    "\n",
    "\n",
    "all_labels = list(set(c for d in data for c in d[\"label_name\"]))\n",
    "dataset = Dataset(\n",
    "    name=\"tweet topics\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(\n",
    "                chunk=d[Chunk(\"text\")],\n",
    "                labels=all_labels\n",
    "            ),\n",
    "            expected_output=d[\"label_name\"]\n",
    "        ) for d in data\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_logger = InMemoryDebugLogger(name=\"evaluation logger\")\n",
    "result = evaluator.evaluate_dataset(dataset=dataset, logger=evaluation_logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", result.percentage_correct)\n",
    "print(\"First example:\", result.evaluations[0])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
