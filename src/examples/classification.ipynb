{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Language models offer unprecedented capabilities in understanding and generating human-like text.\n",
    "One of the pressing issues in their application is the classification of vast amounts of data.\n",
    "Traditional methods often require manual labeling and can be time-consuming and prone to errors.\n",
    "LLMs, on the other hand, can swiftly process and categorize enormous datasets with minimal human intervention.\n",
    "By leveraging LLMs for classification tasks, organizations can unlock insights from their data more efficiently, streamline their workflows, and harness the full potential of their information assets.\n",
    "\n",
    "In this notebook, we present to alternative ways for classifying text using Aleph Alpha's Luminous models.\n",
    "First, let's have a look at single-label classification using prompting.\n",
    "\n",
    "### Prompt-based single-label classification\n",
    "\n",
    "Single-label classification refers to the task of categorizing data points into one of n distinct categories or classes.\n",
    "In this type of classification, each input is assigned to only one class, ensuring that no overlap exists between categories.\n",
    "Common applications of single-label classification include email spam detection, where emails are classified as either \"spam\" or \"not spam\", or sentiment classification, where a text can be \"positive\", \"negative\" or \"neutral\".\n",
    "When trying to solve this issue in a prompt-based manner, our primary goal is to construct a prompt that instructs the model to accurately predict the correct class for any given input.\n",
    "\n",
    "### When should you use prompt-based classification?\n",
    "\n",
    "We recommend using this type of classification when...\n",
    "- ...the labels are easily understood (they don't require explanation or examples).\n",
    "- ...the labels cannot be recognized purely by their semantic meaning.\n",
    "- ...many examples for each label aren't readily available.\n",
    "\n",
    "### Example snippet\n",
    "\n",
    "Running the following code will instantiate a `PromptBasedClassify` that leverages a prompt for classification.\n",
    "We can now enter any `ClassifyInput` so that the task returns each label along with its probability.\n",
    "In addition, note the `tracer`, which will give a comprehensive overview of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.connectors import LimitedConcurrencyClient\n",
    "from intelligence_layer.core import TextChunk, InMemoryTracer\n",
    "from intelligence_layer.use_cases import ClassifyInput, PromptBasedClassify\n",
    "\n",
    "\n",
    "text_to_classify = TextChunk(\n",
    "    \"In the distant future, a space exploration party embarked on a thrilling journey to the uncharted regions of the galaxy. \\n\\\n",
    "With excitement in their hearts and the cosmos as their canvas, they ventured into the unknown, discovering breathtaking celestial wonders. \\n\\\n",
    "As they gazed upon distant stars and nebulas, they forged unforgettable memories that would forever bind them as pioneers of the cosmos.\"\n",
    ")\n",
    "labels = [\"happy\", \"angry\", \"sad\"]\n",
    "input = ClassifyInput(chunk=text_to_classify, labels=labels)\n",
    "\n",
    "task = PromptBasedClassify()\n",
    "tracer = InMemoryTracer()\n",
    "output = task.run(input, tracer)\n",
    "\n",
    "# Let's see the results:\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this implementation work?\n",
    "\n",
    "We prompt the model multiple times, each time supplying the text, or chunk, and one label at a time.\n",
    "Note that we also supply each label, rather than letting the model generate it.\n",
    "\n",
    "To further explain this, let's start with a more familiar case.\n",
    "Intuitively, one would probably prompt a model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptBasedClassify.INSTRUCTION\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then complete our instruction, thus generating a matching label.\n",
    "\n",
    "In the case of single-label classification, however, we already know all possible classes beforehand.\n",
    "Because of this, all we are interested in is the probability that the model would have generated our specific classes.\n",
    "To get this probability, we can prompt the model with each of our classes and ask it to return the \"logprobs\" for the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our request will not generate any tokens, but instead return the log probability of this completion given the previous tokens.\n",
    "This is called an `EchoTask`.\n",
    "Let's have a look at just one of these tasks triggered by our classification run.\n",
    "\n",
    "In particular, note the `expected_completion` in the `Input` and the `prob` for the token \" angry\" in the `Output`.\n",
    "Feel free to ignore the big `Complete` task dump in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer.entries[-1].entries[0].entries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logprobs, we just need to do some calculations to turn them into a final score.\n",
    "\n",
    "To turn the logprobs into our end scores, we first normalize our probabilities.\n",
    "For this, we utilize a probability tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases import TreeNode\n",
    "from intelligence_layer.core import LogEntry\n",
    "\n",
    "task_log = tracer.entries[-1]\n",
    "normalized_probs_logs = [\n",
    "    log_entry.value\n",
    "    for log_entry in task_log.entries\n",
    "    if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"\n",
    "]\n",
    "log = normalized_probs_logs[-1]\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the product of all the paths to get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding-based multi-label classification\n",
    "\n",
    "Large language model embeddings offer a powerful approach to text classification.\n",
    "In particular, such embeddings can be seen as a numerical representation of the meaning of a text.\n",
    "Utilizing this, we can provide textual examples for each label and embed them to create a representations for each label in vector space.\n",
    "\n",
    "**Or, in more detail**:\n",
    "In this method, each example from various classes is transformed into a vector representation using the embeddings from the language model.\n",
    "These embedded vectors capture the semantic essence of the text.\n",
    "Once this is done, clusters of embeddings are formed for each class, representing the centroid or the average meaning of the examples within that class.\n",
    "When a new piece of text needs to be classified, it is first embedded using the same language model.\n",
    "This new embedded vector is then compared to the pre-defined clusters for each class using a cosine similarity.\n",
    "The class whose cluster is closest to the new text's embedding is then assigned to the text, thereby achieving classification.\n",
    "This method leverages the deep semantic understanding of large language models to classify texts with high accuracy and nuance.\n",
    "\n",
    "### When should you use embedding-based classification?\n",
    "\n",
    "We recommend using this type of classification when...\n",
    "- ...proper classification requires fine-grained control over the classes' definitions.\n",
    "- ...the labels can be defined mostly or purely by the semantic meaning of the examples.\n",
    "- ...examples for each label are readily available.\n",
    "\n",
    "### Example snippet\n",
    "\n",
    "Let's start by instantiating a classifier for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases import EmbeddingBasedClassify, LabelWithExamples\n",
    "\n",
    "\n",
    "labels_with_examples = [\n",
    "    LabelWithExamples(\n",
    "        name=\"positive\",\n",
    "        examples=[\n",
    "            \"I really like this.\",\n",
    "            \"Wow, your hair looks great!\",\n",
    "            \"We're so in love.\",\n",
    "            \"That truly was the best day of my life!\",\n",
    "            \"What a great movie.\",\n",
    "        ],\n",
    "    ),\n",
    "    LabelWithExamples(\n",
    "        name=\"negative\",\n",
    "        examples=[\n",
    "            \"I really dislike this.\",\n",
    "            \"Ugh, Your hair looks horrible!\",\n",
    "            \"We're not in love anymore.\",\n",
    "            \"My day was very bad, I did not have a good time.\",\n",
    "            \"They make terrible food.\",\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = LimitedConcurrencyClient.from_env()\n",
    "classify = EmbeddingBasedClassify(labels_with_examples, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several things to note here, in particular:\n",
    "- This time, we instantiated our classification task with a number of `LabelWithExamples`.\n",
    "- The examples provided should reflect the spectrum of texts expected in the intended usage domain of this classifier.\n",
    "- This cell took some time to run.\n",
    "This is because we instantiate a retriever in the background, which also requires us to embed the provided examples.\n",
    "\n",
    "With that being said, let's run an unknown example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_input = ClassifyInput(\n",
    "    chunk=\"It was very awkward with him, I did not enjoy it.\",\n",
    "    labels=frozenset(l.name for l in labels_with_examples),\n",
    ")\n",
    "tracer = InMemoryTracer()\n",
    "result = classify.run(classify_input, tracer)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we correctly identified the new example.\n",
    "\n",
    "Again, let's appreciate the difference of this result compared to `PromptBasedClassify`'s result.\n",
    "- The probabilities do not add up to 1.\n",
    "In fact, we have no way of predicting what the sum of all scores will be.\n",
    "In some cases, individual scores may even be negative.\n",
    "All we know is that the highest score is likely to correspond to the best fitting label, provided we delivered good examples.\n",
    "- We were much quicker to obtain a result.\n",
    "\n",
    "Because all examples are pre-embedded, this classifier is much cheaper to operate as it only requires a single embedding-task to be sent to the Aleph Alpha API.\n",
    "\n",
    "Let's try another example. This time, we expect the outcome to be positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_input = ClassifyInput(\n",
    "    chunk=\"We used to be not like each other, but this changed a lot.\",\n",
    "    labels=frozenset(l.name for l in labels_with_examples),\n",
    ")\n",
    "tracer = InMemoryTracer()\n",
    "result = classify.run(classify_input, tracer)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we wrongly classify this text as negative.\n",
    "To be fair, it is a difficult example.\n",
    "But no worries, let's simply include this failing example in our list of label examples and try again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.use_cases import EmbeddingBasedClassify, LabelWithExamples\n",
    "\n",
    "\n",
    "labels_with_examples = [\n",
    "    LabelWithExamples(\n",
    "        name=\"positive\",\n",
    "        examples=[\n",
    "            \"I really like this.\",\n",
    "            \"Wow, your hair looks great!\",\n",
    "            \"We're so in love.\",\n",
    "            \"That truly was the best day of my life!\",\n",
    "            \"What a great movie.\",\n",
    "            \"We used to be not like each other, but this changed a lot.\",  # failing example\n",
    "        ],\n",
    "    ),\n",
    "    LabelWithExamples(\n",
    "        name=\"negative\",\n",
    "        examples=[\n",
    "            \"I really dislike this.\",\n",
    "            \"Ugh, Your hair looks horrible!\",\n",
    "            \"We're not in love anymore.\",\n",
    "            \"My day was very bad, I did not have a good time.\",\n",
    "            \"They make terrible food.\",\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "classify = EmbeddingBasedClassify(labels_with_examples, client=client)\n",
    "\n",
    "tracer = InMemoryTracer()\n",
    "result = classify.run(classify_input, tracer)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we now correctly classify this example!\n",
    "\n",
    "One advantage of using the `EmbeddingBasedClassify`-approach is that we can easily tweak our labels by adding new examples.\n",
    "In essence, this guarantees that we never make the same mistake twice.\n",
    "As we increase the number of examples, this makes the method evermore precise.\n",
    "\n",
    "You now have an overview of these two main methods of classification!\n",
    "Feel free to tweak these method and play around with their parameters to finetune them to our specific use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
