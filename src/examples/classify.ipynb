{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "\n",
    "Classification is a methodology that tries to match a text to the correct label.\n",
    "\n",
    "### Prompt-based classification\n",
    "\n",
    "Prompt-based classification is a methodology that relies purely on prompting the LLM in a specific way.\n",
    "\n",
    "### When should you use prompt-based classification?\n",
    "\n",
    "Some situations when you would use this methodology is when:\n",
    "- The labels are easily understood (they don't require explanation or examples)\n",
    "    \n",
    "    An example is sentiment analysis\n",
    "- The labels are not recognized by their semantic meaning\n",
    "    \n",
    "    E.g. Reasoning tasks like classifying contradictions\n",
    "- You don't have many examples\n",
    "\n",
    "### Example snippet\n",
    "Running the following code will instantiate a prompt-based classifier with a debug level for the log.\n",
    "Then it will classify the text given in \"ClassifyInput\".\n",
    "The contents of the `debuglog` will be shown below.\n",
    "The debuglog gives an overview of the steps taken to get the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from aleph_alpha_client import Client\n",
    "from intelligence_layer.classify import SingleLabelClassify, ClassifyInput\n",
    "from intelligence_layer.task import InMemoryDebugLogger\n",
    "\n",
    "text_to_classify = \"In the distant future, a space exploration party embarked on a thrilling journey to the uncharted regions of the galaxy. \\n\\\n",
    "    With excitement in their hearts and the cosmos as their canvas, they ventured into the unknown, discovering breathtaking celestial wonders. \\n\\\n",
    "    As they gazed upon distant stars and nebulas, they forged unforgettable memories that would forever bind them as pioneers of the cosmos.\"\n",
    "labels = [\"happy\", \"angry\", \"sad\"]\n",
    "client = Client(getenv(\"AA_TOKEN\"))\n",
    "task = SingleLabelClassify(client)\n",
    "input = ClassifyInput(\n",
    "    text=text_to_classify,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "debug_log = InMemoryDebugLogger(name=\"classify\")\n",
    "output = task.run(input, debug_log)\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n",
    "debug_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this implementation work?\n",
    "For prompt-based classification, we prompt the model multiple times with the text we want to classify and each of our classes.\n",
    "Instead of letting the model generate the class it thinks fits the text best, we ask it for the probability for each class.\n",
    "\n",
    "To further explain this, let's start with a more familiar case.\n",
    "The intuitive way to ask an LLM if it could label a text could be something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE)\n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=\"\").items[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then answer our question and give us a class that it thinks fits the text.\n",
    "\n",
    "In the case of classification, however, we already have the classes beforehand.\n",
    "Because of this, all we are interested in is the probability that the model would have guessed our specific classes.\n",
    "To get this probability, we can prompt the model with each of our classes and ask the model to return the logprobs for the text.\n",
    "\n",
    "In the case of prompt-based classification, the prompt looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE)\n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=labels[0]).items[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have the same prompt, but with the class already filled in as a response.\n",
    "\n",
    "Our request will now not generate any tokens, but instead will just return us the logprobs that the class would be generated, given the previous tokens.\n",
    "\n",
    "```python\n",
    "CompletionRequest(\n",
    "    prompt=prompt_template.to_prompt(**kwargs),\n",
    "    maximum_tokens=0,\n",
    "    log_probs=0,\n",
    "    tokens=True,\n",
    "    echo=True,\n",
    ")\n",
    "```\n",
    "\n",
    "In the case of the classes \"Space Exploration\" and \"Space Party,\", the logprobs per label might look something like the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import LogEntry\n",
    "result_objects = [log_entry for log_entry in debug_log.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Raw log probs per label\"]\n",
    "result_objects.pop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logprobs, we just need to do some calculations to turn them into our end score.\n",
    "\n",
    "To turn the logprobs into our end scores, we first normalize our probabilities. This will result in the following data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.classify import TreeNode\n",
    "\n",
    "normalized_probs_logs = [log_entry.value for log_entry in debug_log.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs.pop()\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the product of all the paths to get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example mentioned before is rather straightforward, but there are some situations when it isn't as obvious as a single token.\n",
    "\n",
    "What if we take some classes that have some overlap?\n",
    "In the following example, some of the classes overlap in the tokens they have. This makes the calculation a bit more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.classify import SingleLabelClassify, ClassifyInput\n",
    "from intelligence_layer.task import LogEntry\n",
    "\n",
    "labels = [\"Space party\", \"Space exploration\", \"Space exploration party\"]\n",
    "task = SingleLabelClassify(client)\n",
    "input = ClassifyInput(\n",
    "    text=text_to_classify,\n",
    "    labels=labels\n",
    ")\n",
    "logger = InMemoryDebugLogger(name=\"classify\")\n",
    "output = task.run(input, logger)\n",
    "normalized_probs_logs = [log_entry.value for log_entry in logger.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs.pop()\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)\n",
    "\n",
    "print(\"End scores:\")\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the three classes have some overlapping tokens.\n",
    "In the graph above, it can be seen how the calculations would be done in this case.\n",
    "\n",
    "1. At the top, you can see that when there is only one token to choose from, the normalized score will always be 1.\n",
    "\n",
    "2. After that, the first choice is made between \"exploration\" and \"party\". \n",
    "\n",
    "3. If the choice of \"exploration\" is made, finally a choice has to be made between the \"endoftext\" token and \"party\".\n",
    "\n",
    "    This \"endoftext\" token is a token that large language models use internally to make their calculations.\n",
    "    As an end user, you generally shouldn't see this token.\n",
    "\n",
    "    In our case, this translates to choosing between: \"Space exploration party\" and just \"Space exploration\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate how well our new methodology is working. For this, we will first look for classification datasets to use. We found this [dataset](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) on huggingface, let's see if we can get an evaluation going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "data = list(dataset[test_set_name])[:10] # this has 573 datapoints, let's take a look at 20 for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to instantiate an evaluator that takes our classify methodology (`task`) and some datapoints and returns some evaluation metrics.\n",
    "\n",
    "First, let's evaluate a single example and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.classify import SingleLabelClassifyEvaluator\n",
    "\n",
    "evaluator = SingleLabelClassifyEvaluator(task)\n",
    "classify_input = ClassifyInput(\n",
    "        text=\"This is good\",\n",
    "        labels=frozenset({\"positive\", \"negative\"}),\n",
    "    )\n",
    "evaluation_logger = InMemoryDebugLogger(name=\"evaluation logger\")\n",
    "expected_output = \"positive\"\n",
    "evaluation = evaluator.evaluate(\n",
    "    input=classify_input, logger=evaluation_logger, expected_output=[expected_output]\n",
    ")\n",
    "\n",
    "print(\"The task result:\", evaluation.output.scores)\n",
    "print(\"The expected output:\", expected_output)\n",
    "print(\"The eval result:\", evaluation.correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import Example, Dataset \n",
    "\n",
    "\n",
    "all_labels = list(set(c for d in data for c in d[\"label_name\"]))\n",
    "dataset = Dataset(\n",
    "    name=\"tweet topics\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(\n",
    "                text=d[\"text\"],\n",
    "                labels=all_labels\n",
    "            ),\n",
    "            expected_output=d[\"label_name\"]\n",
    "        ) for d in data\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_logger = InMemoryDebugLogger(name=\"evaluation logger\")\n",
    "result = evaluator.evaluate_dataset(dataset=dataset, logger=evaluation_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", result.percentage_correct)\n",
    "print(\"First example\", result.evaluations[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10-intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
