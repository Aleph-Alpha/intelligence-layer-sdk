{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "\n",
    "Classification is a methodology that tries to match a text to the correct label.\n",
    "\n",
    "### Prompt-based classification\n",
    "\n",
    "Prompt-based classification is a methodology that relies purely on prompting the LLM in a specific way.\n",
    "\n",
    "### When should you use prompt-based classification?\n",
    "\n",
    "Some situations when you would use this methodology is when:\n",
    "- The labels are easily understood (they don't require explanation or examples), for example sentiment analysis\n",
    "- The labels are not recognized by their semantic meaning, e.g. \"reasoning\" tasks like classifying contradictions\n",
    "- You don't have many examples\n",
    "\n",
    "### Example snippet\n",
    "\n",
    "Running the following code will instantiate a prompt-based classifier with a debug level for the log.\n",
    "Then it will classify the text given in `ClassifyInput`.\n",
    "The contents of the `debug_log` will be shown below.\n",
    "It gives an overview of the steps taken to get the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "\n",
    "from aleph_alpha_client import Client\n",
    "\n",
    "from intelligence_layer.single_label_classify import ClassifyInput, SingleLabelClassify\n",
    "from intelligence_layer.task import Chunk, InMemoryDebugLogger\n",
    "\n",
    "text_to_classify = Chunk(\"In the distant future, a space exploration party embarked on a thrilling journey to the uncharted regions of the galaxy. \\n\\\n",
    "With excitement in their hearts and the cosmos as their canvas, they ventured into the unknown, discovering breathtaking celestial wonders. \\n\\\n",
    "As they gazed upon distant stars and nebulas, they forged unforgettable memories that would forever bind them as pioneers of the cosmos.\")\n",
    "labels = [\"happy\", \"angry\", \"sad\"]\n",
    "client = Client(getenv(\"AA_TOKEN\"))\n",
    "task = SingleLabelClassify(client)\n",
    "input = ClassifyInput(\n",
    "    chunk=text_to_classify,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "debug_log = InMemoryDebugLogger(name=\"classify\")\n",
    "output = task.run(input, debug_log)\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n",
    "# debug_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this implementation work?\n",
    "\n",
    "For prompt-based classification, we prompt the model multiple times with the text we want to classify and each of our classes.\n",
    "Instead of letting the model generate the class it thinks fits the text best, we ask it for the probability for each class.\n",
    "\n",
    "To further explain this, let's start with a more familiar case.\n",
    "Intuitively, one would probably prompt a model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE)\n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=\"\").items[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then answer our question and generate a class or label that it thinks fits the text best.\n",
    "\n",
    "In the case of classification, however, we already know all possible classes beforehand.\n",
    "Because of this, all we are interested in is the probability that the model would have generated our specific classes.\n",
    "To get this probability, we can prompt the model with each of our classes and ask it to return the \"logprobs\" for the text.\n",
    "\n",
    "In the case of prompt-based classification, the base prompt looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(SingleLabelClassify.PROMPT_TEMPLATE)\n",
    "print(prompt_template.to_prompt(text=text_to_classify, label=\" \" +labels[0]).items[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have the same prompt, but with a potential label candidate already filled in.\n",
    "\n",
    "Now, we will ask the model to evaluate the likelihood of this completion.\n",
    "\n",
    "Our request will now not generate any tokens, but instead return the log probability of this completion given the previous tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the logprobs, we just need to do some calculations to turn them into a final score.\n",
    "\n",
    "To turn the logprobs into our end scores, we first normalize our probabilities.\n",
    "For this, we utilize a probability tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.single_label_classify import TreeNode\n",
    "from intelligence_layer.task import LogEntry\n",
    "\n",
    "task_log = debug_log.logs[-1]\n",
    "normalized_probs_logs = [log_entry.value for log_entry in task_log.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs[-1]\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the product of all the paths to get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example mentioned before is rather straightforward, but there are some situations when it isn't as obvious as a single token.\n",
    "\n",
    "What if we take some classes that have some overlap?\n",
    "In the following example, some of the classes overlap in the tokens they have.\n",
    "This makes the calculation a bit more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.single_label_classify import SingleLabelClassify, ClassifyInput\n",
    "from intelligence_layer.task import LogEntry\n",
    "\n",
    "\n",
    "labels = [\"Space party\", \"Space exploration\", \"Space exploration party\"]\n",
    "task = SingleLabelClassify(client)\n",
    "input = ClassifyInput(\n",
    "    chunk=text_to_classify,\n",
    "    labels=labels\n",
    ")\n",
    "logger = InMemoryDebugLogger(name=\"classify\")\n",
    "output = task.run(input, logger)\n",
    "task_log = logger.logs[-1]\n",
    "normalized_probs_logs = [log_entry.value for log_entry in task_log.logs if isinstance(log_entry, LogEntry) and log_entry.message == \"Normalized Probs\"]\n",
    "log = normalized_probs_logs.pop()\n",
    "\n",
    "root = TreeNode()\n",
    "for probs in log.values():\n",
    "    root.insert_without_calculation(probs)\n",
    "\n",
    "print(\"End scores:\")\n",
    "for label, score in output.scores.items():\n",
    "    print(f\"{label}: {round(score, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the three classes have some overlapping tokens, namely \"Space\", and \"exploration\".\n",
    "\"party\" is not overlapping, because it occurs in two different places (after \"Space\" and after \"exploration\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!\n",
    "Now, let's evaluate how well our new methodology is working.\n",
    "For this, we will first look for classification datasets to use.\n",
    "We found this [dataset](https://huggingface.co/cardiffnlp/tweet-topic-21-multi) on huggingface, let's see if we can get an evaluation going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(f\"cardiffnlp/tweet_topic_multi\")\n",
    "test_set_name = \"validation_random\"\n",
    "data = list(dataset[test_set_name])[:10] # this has 573 datapoints, let's take a look at 20 for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to instantiate an evaluator that takes our classify methodology (`task`) and some datapoints and returns some evaluation metrics.\n",
    "\n",
    "First, let's evaluate a single example and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.single_label_classify import SingleLabelClassifyEvaluator\n",
    "\n",
    "evaluator = SingleLabelClassifyEvaluator(task)\n",
    "classify_input = ClassifyInput(\n",
    "        chunk=Chunk(\"This is good\"),\n",
    "        labels=frozenset({\"positive\", \"negative\"}),\n",
    "    )\n",
    "evaluation_logger = InMemoryDebugLogger(name=\"evaluation logger\")\n",
    "expected_output = \"positive\"\n",
    "evaluation = evaluator.evaluate(\n",
    "    input=classify_input, logger=evaluation_logger, expected_output=[expected_output]\n",
    ")\n",
    "\n",
    "print(\"The task result:\", evaluation.output.scores)\n",
    "print(\"The expected output:\", expected_output)\n",
    "print(\"The eval result:\", evaluation.correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our dataset into the required format. \n",
    "Therefore, let's check out what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, this must be translated into the interface of our `Evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.task import Example, Dataset\n",
    "\n",
    "\n",
    "all_labels = list(set(c for d in data for c in d[\"label_name\"]))\n",
    "dataset = Dataset(\n",
    "    name=\"tweet topics\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=ClassifyInput(\n",
    "                chunk=d[Chunk(\"text\")],\n",
    "                labels=all_labels\n",
    "            ),\n",
    "            expected_output=d[\"label_name\"]\n",
    "        ) for d in data\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_logger = InMemoryDebugLogger(name=\"evaluation logger\")\n",
    "result = evaluator.evaluate_dataset(dataset=dataset, logger=evaluation_logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage correct:\", result.percentage_correct)\n",
    "print(\"First example\", result.evaluations[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10-intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
