{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Iterable, cast\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from intelligence_layer.connectors import (\n",
    "    ArgillaEvaluation,\n",
    "    DefaultArgillaClient,\n",
    "    Field,\n",
    "    LimitedConcurrencyClient,\n",
    "    Question,\n",
    "    RecordData,\n",
    ")\n",
    "from intelligence_layer.core import (\n",
    "    CompleteOutput,\n",
    "    Instruct,\n",
    "    InstructInput,\n",
    "    LuminousControlModel,\n",
    ")\n",
    "from intelligence_layer.evaluation import (\n",
    "    AggregationLogic,\n",
    "    ArgillaAggregator,\n",
    "    ArgillaEvaluationLogic,\n",
    "    ArgillaEvaluationRepository,\n",
    "    ArgillaEvaluator,\n",
    "    Example,\n",
    "    FileAggregationRepository,\n",
    "    FileDatasetRepository,\n",
    "    FileEvaluationRepository,\n",
    "    FileRunRepository,\n",
    "    RecordDataSequence,\n",
    "    Runner,\n",
    "    SuccessfulExampleOutput,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = LimitedConcurrencyClient.from_env()\n",
    "\n",
    "REPOSITORY_ROOT_PATH = Path(\"human-eval-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation using the Intelligence Layer\n",
    "\n",
    "Although there are a variety of ways to automate the evaluation of LLM-based tasks, sometimes it is still necessary to get a human opinion.\n",
    "To make this as painless as possible, we have integrated an [Argilla](https://argilla.io/)-Evaluator into the intelligence layer.\n",
    "This notebook serves as a quick start guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "This notebook expects that you have added your Aleph Alpha token to your .env file.\n",
    "Additionally you need to add the `ARGILLA_API_URL` and `ARGILLA_API_KEY` from env.sample to your .env file. \n",
    "Next, run\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "``` \n",
    "\n",
    "from the intelligence layer base directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you go to `localhost:6900` and you are prompted to enter a username and password, use:\n",
    "- username: `argilla`\n",
    "- password: `1234`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "This notebook is designed such that the creation of the dataset, the submission to Argilla and the aggregation of the Argilla evaluations do not have to be done in a single session.\n",
    "\n",
    "As a result, the data repositories are redefined for each step and we use file-based repositories that persist the data. If you run all steps in a single session, you can use InMemory-based repositories and reuse the same repository object for multiple steps.\n",
    "\n",
    "Running this notebook creates a `human-eval-data` folder, which will be deleted if you run the whole notebook to completion.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Repository definition\n",
    "First we need to define our dataset. Here we use an [Instruction Dataset](https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=0) from Huggingface. Before we can use it for human eval, we need to make an intelligence layer dataset repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingfaceH4/instruction-dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the dataset a bit. It consists of prompts, example completions and metadata for 327 examples. Since we are doing human eval, for now we only need the prompt and corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"meta\"][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now build a single `Example` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Example(\n",
    "    input=InstructInput(instruction=dataset[\"prompt\"][0], input=None),\n",
    "    expected_output=None,\n",
    "    id=str(dataset[\"meta\"][0][\"id\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset repository, we can either use a `FileDatasetRepository` or an `InMemoryDatasetRepository`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5\n",
    "assert num_examples <= len(dataset)\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=InstructInput(instruction=dataset[\"prompt\"][i], input=None),\n",
    "            expected_output=None,\n",
    "            id=str(dataset[\"meta\"][i][\"id\"]),\n",
    "        )\n",
    "        for i in range(num_examples)\n",
    "    ],\n",
    "    dataset_name=\"human-evaluation-dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Setup\n",
    "\n",
    "We use an `Instruction` task to run the examples in our dataset.\n",
    "In addition, we define a `Runner` to generate the completions from the model for our dataset\n",
    "and a `RunRepository` to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LuminousControlModel(name=\"luminous-base-control\", client=client)\n",
    "task = Instruct(model=model)\n",
    "\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "# either remember the id from before (dataset.id) or retrieve as below\n",
    "dataset_id = [\n",
    "    dataset.id\n",
    "    for dataset in dataset_repository.datasets()\n",
    "    if dataset.name == \"human-evaluation-dataset\"\n",
    "][0]\n",
    "dataset_repository.datasets()\n",
    "run_repository = FileRunRepository(REPOSITORY_ROOT_PATH)\n",
    "runner = Runner(task, dataset_repository, run_repository, \"instruct-run\")\n",
    "\n",
    "run_overview = runner.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our evaluation we want a float score $s \\in [1,5]$ describing the model performance.\n",
    "We define this as an `InstructAggregatedEvaluation`, which will be used in our aggregation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructAggregatedEvaluation(BaseModel):\n",
    "    general_rating: float | None\n",
    "    fluency: float | None\n",
    "    evaluated_examples: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to define our human evaluation. This is done with `Questions` and `Fields`.  \n",
    "`Fields` define what a user has to evaluate. In our example, this will be the model input (Instruction) and output (Model Completion). Note that the field names have to match the content keys from the `RecordData` which we will define later in our `InstructArgillaEvaluationLogic`.  \n",
    "`Questions` are what a user has to answer in order to evaluate the `Fields`. The `name` property will later be used to access the human ratings in the aggregation step. In our case we ask how complete and how fluent the completions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(\n",
    "        name=\"general_rating\",  # name of the field in program, used for retrieval later\n",
    "        title=\"Rating\",  # name shown to the user\n",
    "        description=\"Rate this completion on a scale from 1 to 5\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "    Question(\n",
    "        name=\"fluency\",\n",
    "        title=\"Fluency\",\n",
    "        description=\"How fluent is the completion?\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "]\n",
    "\n",
    "fields = [\n",
    "    Field(name=\"input\", title=\"Instruction\"),\n",
    "    Field(name=\"output\", title=\"Model Completion\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our defined fields and questions will look like this:\n",
    "![Argilla Interface](../../assets/argilla_interface.png)\n",
    "\n",
    "We can now define our `InstructArgillaEvaluationLogic` and `InstructArgillaAggregationLogic`.\n",
    "They have to implement the two abstract methods `_to_record` and `aggregate` respectively.\n",
    "Lets look at the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ArgillaEvaluationLogic._to_record)\n",
    "print(\"-\" * 100)\n",
    "help(AggregationLogic.aggregate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of performing the evaluation, the `ArgillaEvaluationLogic` is responsible for converting the evaluation data to a format that is accepted by Argilla. During the evaluation, these records will simply be submitted to argilla.  \n",
    "We will now create everything we need to submit these evaluations to our Argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructArgillaEvaluationLogic(\n",
    "    ArgillaEvaluationLogic[\n",
    "        InstructInput,\n",
    "        CompleteOutput,\n",
    "        None,\n",
    "    ]\n",
    "):\n",
    "    def _to_record(\n",
    "        self,\n",
    "        example: Example[InstructInput, None],\n",
    "        *example_outputs: SuccessfulExampleOutput[CompleteOutput],\n",
    "    ) -> RecordDataSequence:\n",
    "        return RecordDataSequence(\n",
    "            records=[\n",
    "                RecordData(\n",
    "                    content={\n",
    "                        \"input\": example.input.instruction,\n",
    "                        \"output\": example_outputs[0].output.completion,\n",
    "                    },\n",
    "                    example_id=example.id,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "argilla_client = DefaultArgillaClient()\n",
    "workspace_id = argilla_client.ensure_workspace_exists(\"test-human-eval\")\n",
    "\n",
    "dataset_repository = FileDatasetRepository(REPOSITORY_ROOT_PATH)\n",
    "run_repository = FileRunRepository(REPOSITORY_ROOT_PATH)\n",
    "evaluation_repository = FileEvaluationRepository(\n",
    "    REPOSITORY_ROOT_PATH\n",
    ")  # this is only used to store failed evaluations and the evaluation overview\n",
    "argilla_evaluation_repository = ArgillaEvaluationRepository(\n",
    "    evaluation_repository, argilla_client, workspace_id, fields, questions\n",
    ")\n",
    "\n",
    "eval_logic = InstructArgillaEvaluationLogic()\n",
    "evaluator = ArgillaEvaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    argilla_evaluation_repository,\n",
    "    \"instruct-evaluation\",\n",
    "    eval_logic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the `ArgillaEvaluator`, the `evaluate_runs` methods posts the records to the argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either remember the id from before (run_overview.id) or retrieve as below\n",
    "run_id = [\n",
    "    overview.id\n",
    "    for overview in run_repository.run_overviews()\n",
    "    if overview.description == \"instruct-run\"\n",
    "][0]\n",
    "\n",
    "try:\n",
    "    eval_overview = evaluator.evaluate_runs(run_id)\n",
    "    print(eval_overview)\n",
    "\n",
    "except Exception as e:\n",
    "    eval_overview = None\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the evaluation says that 5 examples were successfully evaluated, no real evaluation has happened yet.  \n",
    "If we try to perform an aggregation right now, it will have no evaluations, as none of the submitted records were evaluated by humans through argilla yet.  \n",
    "The aggregation fetches only the results that were already evaluated.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Sometimes it is best to split up the human evaluation effort into multiple people. To best facilitate this, it is possible to split up the dataset by giving them labels.\n",
    "Our Argilla client offers an easy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_id = eval_overview.id\n",
    "argilla_client.split_dataset(eval_id, n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These splits can then be filered by, as shown below.  \n",
    "<img src=\"../../assets/argilla_splits.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "For the Aggregation, we first need to define our `AggregationLogic` that has to take an `ArgillaEvaluation` as an input. As output, we use the `InstructAggregatedEvaluation` we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructArgillaAggregationLogic(\n",
    "    AggregationLogic[ArgillaEvaluation, InstructAggregatedEvaluation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self,\n",
    "        evaluations: Iterable[ArgillaEvaluation],\n",
    "    ) -> InstructAggregatedEvaluation:\n",
    "        evaluations = list(evaluations)\n",
    "\n",
    "        if len(evaluations) == 0:  # if no evaluations were submitted, return\n",
    "            return InstructAggregatedEvaluation(\n",
    "                general_rating=None,\n",
    "                fluency=None,\n",
    "                evaluated_examples=0,\n",
    "            )\n",
    "\n",
    "        general_rating = sum(\n",
    "            cast(float, evaluation.responses[\"general_rating\"])\n",
    "            for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        fluency = sum(\n",
    "            cast(float, evaluation.responses[\"fluency\"]) for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        return InstructAggregatedEvaluation(\n",
    "            general_rating=general_rating,\n",
    "            fluency=fluency,\n",
    "            evaluated_examples=len(evaluations),\n",
    "        )\n",
    "\n",
    "\n",
    "aggregation_logic = InstructArgillaAggregationLogic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can define our `ArgillaAggregator` and retrieve the aggregation of all records that have been evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_repository = FileEvaluationRepository(REPOSITORY_ROOT_PATH)\n",
    "argilla_evaluation_repository = ArgillaEvaluationRepository(\n",
    "    evaluation_repository,\n",
    "    argilla_client,\n",
    "    workspace_id,  # we do not need to set questions and fields here\n",
    ")\n",
    "aggregation_repository = FileAggregationRepository(REPOSITORY_ROOT_PATH)\n",
    "# either remember the id from before (eval_overview.id) or retrieve as below\n",
    "eval_id = [\n",
    "    overview.id\n",
    "    for overview in argilla_evaluation_repository.evaluation_overviews()\n",
    "    if overview.description == \"instruct-evaluation\"\n",
    "][0]\n",
    "\n",
    "\n",
    "aggregator = ArgillaAggregator(\n",
    "    argilla_evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"instruct-aggregation\",\n",
    "    aggregation_logic,\n",
    ")\n",
    "\n",
    "if eval_overview:\n",
    "    output = aggregator.aggregate_evaluation(eval_id)\n",
    "    print(output.statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! WARNING ! This deletes the \"test-human-eval\" argilla workspace and the \"human-eval-data\" folder.\n",
    "argilla_client.delete_workspace(workspace_id)\n",
    "\n",
    "shutil.rmtree(REPOSITORY_ROOT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
