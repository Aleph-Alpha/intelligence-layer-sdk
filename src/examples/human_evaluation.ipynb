{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation using the Intelligence Layer\n",
    "\n",
    "Although there are a variety of ways to automate the evaluation of LLM-based tasks, sometimes it is still necessary to get a human opinion.\n",
    "To make this as painless as possible, we have integrated an [Argilla](https://argilla.io/)-Evaluator into the intelligence layer.\n",
    "This notebook serves as a quick start guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "This notebook expects that you have added your Aleph Alpha token to your .env file.\n",
    "Additionally you need to add the `ARGILLA_API_URL` and `ARGILLA_API_KEY` from env.sample to your .env file. \n",
    "Next, run\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "``` \n",
    "\n",
    "from the intelligence layer base directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you go to `localhost:6900` and you are prompted to enter a username and password, use:\n",
    "- username: `argilla`\n",
    "- password: `1234`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Iterable, cast\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from intelligence_layer.connectors import (\n",
    "    LimitedConcurrencyClient,\n",
    "    Question,\n",
    "    ArgillaEvaluation,\n",
    "    DefaultArgillaClient,\n",
    "    Field,\n",
    "    RecordData,\n",
    ")\n",
    "from intelligence_layer.core import InstructInput, Instruct, CompleteOutput, LuminousControlModel\n",
    "from intelligence_layer.evaluation import (\n",
    "    ArgillaEvaluator,\n",
    "    AggregationLogic,\n",
    "    RecordDataSequence,\n",
    "    ArgillaEvaluationLogic,\n",
    "    ArgillaEvaluationRepository,\n",
    "    Example,\n",
    "    InMemoryDatasetRepository,\n",
    "    InMemoryEvaluationRepository,\n",
    "    InMemoryRunRepository,\n",
    "    InMemoryAggregationRepository,\n",
    "    Runner,\n",
    "    SuccessfulExampleOutput,\n",
    ")\n",
    "\n",
    "from intelligence_layer.evaluation.argilla import ArgillaAggregator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = LimitedConcurrencyClient.from_token(os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Repository definition\n",
    "First we need to define our dataset. Here we use an [Instruction Dataset](https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=0) from Huggingface. Before we can use it for human eval, we need to make an intelligence layer dataset repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingfaceH4/instruction-dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the dataset a bit. It consists of prompts, example completions and metadata for 327 examples. Since we are doing human eval, for now we only need the prompt and corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"meta\"][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now build a single example like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Example(\n",
    "    input=InstructInput(instruction=dataset[\"prompt\"][0], input=None),\n",
    "    expected_output=None,\n",
    "    id=str(dataset[\"meta\"][0][\"id\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset repository, we could use a FileDatasetRepository or an InMemoryDatasetRepository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5\n",
    "assert num_examples <= len(dataset)\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "dataset_id = dataset_repository.create_dataset(\n",
    "    examples=[\n",
    "        Example(\n",
    "            input=InstructInput(instruction=dataset[\"prompt\"][i], input=None),\n",
    "            expected_output=None,\n",
    "            id=str(dataset[\"meta\"][i][\"id\"]),\n",
    "        )\n",
    "        for i in range(num_examples)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Setup\n",
    "\n",
    "We use an Instruction task to run the examples in the Instruct dataset.\n",
    "In addition, we define an `EvaluationRepository` to save the results and a `Runner` to generate the completions from the model for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LuminousControlModel(name = \"luminous-base-control\", client=client)\n",
    "task = Instruct(model=model)\n",
    "run_repository = InMemoryRunRepository()\n",
    "runner = Runner(task, dataset_repository, run_repository, \"Instruct\")\n",
    "run_overview = runner.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our evaluation we want a float score $$s \\in [1,5]$$ describing the model performance.\n",
    "We define this as `InstructAggregatedEvaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructAggregatedEvaluation(BaseModel):\n",
    "    general_rating: float | None\n",
    "    fluency: float | None\n",
    "    evaluated_examples: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Argilla Interface](../../assets/argilla_interface.png)\n",
    "\n",
    "In the Argilla UI, we see our model input (Instruction) and output (Model Completion) on the left side.\n",
    "These are defined using the `fields` list.\n",
    "The field names have to match the content keys from the `RecordData` that we will define in our `InstructArgillaEvaluationLogic`.\n",
    "\n",
    "On the right side of the UI, we see our rating interface.\n",
    "This can serve a number of Questions to be rated.\n",
    "Currently, only integer scales are accepted.\n",
    "The `name` property is used to access the human ratings in the aggregation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(\n",
    "        name=\"general_rating\",\n",
    "        title=\"Rating\",\n",
    "        description=\"Rate this completion on a scale from 1 to 5\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "    Question(\n",
    "        name=\"fluency\",\n",
    "        title=\"Fluency\",\n",
    "        description=\"How fluent is the completion?\",\n",
    "        options=range(1, 6),\n",
    "    ),\n",
    "]\n",
    "\n",
    "fields = [\n",
    "    Field(name=\"input\", title=\"Instruction\"),\n",
    "    Field(name=\"output\", title=\"Model Completion\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our `InstructArgillaEvaluationLogic` and `InstructArgillaAggregationLogic`.\n",
    "They have to implement the two abstract methods `_to_record` and `aggregate` respectively.\n",
    "Lets look at the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(AggregationLogic.aggregate)\n",
    "help(ArgillaEvaluationLogic._to_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InstructArgillaAggregationLogic(\n",
    "    AggregationLogic[ArgillaEvaluation, InstructAggregatedEvaluation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self,\n",
    "        evaluations: Iterable[ArgillaEvaluation],\n",
    "    ) -> InstructAggregatedEvaluation:\n",
    "        evaluations = list(evaluations)\n",
    "\n",
    "        if len(evaluations) == 0:  # if no evaluations were submitted, return\n",
    "            return InstructAggregatedEvaluation(\n",
    "                general_rating=None,\n",
    "                fluency=None,\n",
    "                evaluated_examples=0,\n",
    "            )\n",
    "\n",
    "        general_rating = sum(\n",
    "            cast(float, evaluation.responses[\"general_rating\"])\n",
    "            for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        fluency = sum(\n",
    "            cast(float, evaluation.responses[\"fluency\"]) for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        return InstructAggregatedEvaluation(\n",
    "            general_rating=general_rating,\n",
    "            fluency=fluency,\n",
    "            evaluated_examples=len(evaluations),\n",
    "        )\n",
    "\n",
    "\n",
    "class InstructArgillaEvaluationLogic(\n",
    "    ArgillaEvaluationLogic[\n",
    "        InstructInput,\n",
    "        CompleteOutput,\n",
    "        None,\n",
    "    ]\n",
    "):\n",
    "    def _to_record(\n",
    "        self,\n",
    "        example: Example[InstructInput, None],\n",
    "        *example_outputs: SuccessfulExampleOutput[CompleteOutput],\n",
    "    ) -> RecordDataSequence:\n",
    "        return RecordDataSequence(\n",
    "            records=[\n",
    "                RecordData(\n",
    "                    content={\n",
    "                        \"input\": example.input.instruction,\n",
    "                        \"output\": example_outputs[0].output.completion,\n",
    "                    },\n",
    "                    example_id=example.id,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "argilla_client = DefaultArgillaClient()\n",
    "workspace_id = argilla_client.create_workspace(\"test\")\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "aggregation_repository = InMemoryAggregationRepository()\n",
    "eval_logic = InstructArgillaEvaluationLogic()\n",
    "aggregation_logic = InstructArgillaAggregationLogic()\n",
    "\n",
    "argilla_evaluation_repository = ArgillaEvaluationRepository(\n",
    "    evaluation_repository, argilla_client, workspace_id, fields, questions\n",
    ")\n",
    "\n",
    "evaluator = ArgillaEvaluator(\n",
    "    dataset_repository,\n",
    "    run_repository,\n",
    "    argilla_evaluation_repository,\n",
    "    \"instruct\",\n",
    "    eval_logic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate_runs` posts the records created from a run to the argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eval_overview = evaluator.evaluate_runs(run_overview.id)\n",
    "    print(eval_overview)\n",
    "\n",
    "except Exception as e:\n",
    "    eval_overview = None\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access once we have evaluated some examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = ArgillaAggregator(\n",
    "    argilla_evaluation_repository,\n",
    "    aggregation_repository,\n",
    "    \"instruct\",\n",
    "    aggregation_logic,\n",
    ")\n",
    "\n",
    "if eval_overview:\n",
    "    output = aggregator.aggregate_evaluation(eval_overview.id)\n",
    "    print(output.statistics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
