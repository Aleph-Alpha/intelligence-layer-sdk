{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Evaluation using the Intelligence Layer\n",
    "Even though there are a multitude of automated ways to automate the evaluation of LLM based Tasks, sometimes it is still necessary to get a human opinion. To make this as pain free as possible we integrated an [Argilla Evaluator](https://argilla.io/) into the intelligence layer. This notebook serves as a quick start guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "This notebook expects that you have added your Aleph Alpha token to your .env file. Additionally you need to add the `ARGILLA_API_URL` and `ARGILLA_API_KEY` from env.sample to your .env file. \n",
    "After this you can run \n",
    "```bash\n",
    "docker-compose up -d\n",
    "``` \n",
    "from the intelligence layer base directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intelligence_layer.core import ArgillaEvaluator, ArgillaEvaluationRepository, Example, InstructInput, Instruct, InMemoryDatasetRepository, InMemoryEvaluationRepository, PromptOutput, Runner\n",
    "from intelligence_layer.connectors import LimitedConcurrencyClient, Question, ArgillaEvaluation, DefaultArgillaClient, Field, RecordData\n",
    "from typing import Iterable, cast, Sequence\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = LimitedConcurrencyClient.from_token(os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Repository definition\n",
    "First we need to define our dataset. Here we use an [Instruction Dataset](https://huggingface.co/datasets/HuggingFaceH4/instruction-dataset?row=0) from Huggingface. Before we can use it for human eval, we need to make an intelligence layer dataset repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingfaceH4/instruction-dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the dataset a bit. It consists of prompts, example completions and metadata for 327 examples. Since we are doing human eval, for now we only need the prompt and corresponding id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset['meta'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now build a single example like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Example(\n",
    "    input=InstructInput(instruction=dataset[\"prompt\"][0], input=None), \n",
    "    expected_output=None,\n",
    "    id=str(dataset[\"meta\"][0][\"id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset repository we could either use a FileDatasetRepository or an InMemoryDatasetRepository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5\n",
    "assert num_examples <= len(dataset)\n",
    "dataset_repository = InMemoryDatasetRepository()\n",
    "dataset_id = dataset_repository.create_dataset(examples=[\n",
    "    Example(\n",
    "    input=InstructInput(instruction=dataset[\"prompt\"][i], input=None), \n",
    "    expected_output=None,\n",
    "    id=str(dataset[\"meta\"][i][\"id\"])\n",
    ") for i in range(num_examples)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Setup\n",
    "We use an Instruction task for our Instruct dataset. In addition we define an `EvaluationRepository` to save the results and a `Runner` to generate the completions from the model for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Instruct(client, model=\"luminous-base-control\")\n",
    "evaluation_repository = InMemoryEvaluationRepository()\n",
    "runner = Runner(task, evaluation_repository, dataset_repository, \"Instruct\")\n",
    "run_overview = runner.run_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our evaluation we want a float score $$s \\in [1,5]$$ describing the model performance. We define this as `InstructAggregatedEvaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructAggregatedEvaluation(BaseModel):\n",
    "    general_rating: float | None\n",
    "    fluency: float | None\n",
    "    evaluated_examples: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Argilla Interface](../../assets/argilla_interface.png)\n",
    "In the Argilla UI we will see our model input (Instruction) and output (Model Completion) on the left side. This is defined using the `fields` list. The field names have to match the content keys from the `RecordData` that we will define in our `InstructArgillaEvaluator`. On the right side of the UI we will see our rating interface. This can serves a number of Questions to be rated. Currently only integer scales are accepted. The `name` property is used to access the human ratings in the aggregation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    Question(\n",
    "        name=\"general_rating\",\n",
    "        title=\"Rating\",\n",
    "        description=\"Rate this Instruct completion on a scale from 1 to 5\",\n",
    "        options=range(1,6),\n",
    "    ),\n",
    "    Question(\n",
    "        name=\"fluency\",\n",
    "        title=\"Fluency\",\n",
    "        description=\"How fluent is the completion?\",\n",
    "        options=range(1,6),\n",
    "    )\n",
    "]\n",
    "\n",
    "fields = [\n",
    "    Field(name=\"input\", title=\"Instruction\"),\n",
    "    Field(name=\"output\", title=\"Model Completion\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our `InstructArgillaEvaluator`. It has to implement the two abstract methods `aggregate` and `_to_record`. Lets look at the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ArgillaEvaluator.aggregate)\n",
    "help(ArgillaEvaluator._to_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructArgillaEvaluator(\n",
    "    ArgillaEvaluator[\n",
    "        InstructInput,\n",
    "        PromptOutput,\n",
    "        None,\n",
    "        InstructAggregatedEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def aggregate(\n",
    "        self,\n",
    "        evaluations: Iterable[ArgillaEvaluation],\n",
    "    ) -> InstructAggregatedEvaluation:\n",
    "        evaluations = list(evaluations)\n",
    "\n",
    "        if len(evaluations) == 0: # if no evaluations were submitted, return\n",
    "            return InstructAggregatedEvaluation(\n",
    "                general_rating=None,\n",
    "                fluency=None,\n",
    "                evaluated_examples=0,\n",
    "            )\n",
    "        \n",
    "        general_rating = sum(\n",
    "            cast(float, evaluation.responses[\"general_rating\"]) for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        fluency = sum(\n",
    "            cast(float, evaluation.responses[\"fluency\"]) for evaluation in evaluations\n",
    "        ) / len(evaluations)\n",
    "\n",
    "        return InstructAggregatedEvaluation(\n",
    "            general_rating=general_rating,\n",
    "            fluency=fluency,\n",
    "            evaluated_examples=len(evaluations),\n",
    "        )\n",
    "\n",
    "    def _to_record(\n",
    "        self,\n",
    "        example: Example[InstructInput, None],\n",
    "        output: PromptOutput,\n",
    "    ) -> Sequence[RecordData]:\n",
    "        return [RecordData(\n",
    "            content={\n",
    "                \"input\": example.input.instruction,\n",
    "                \"output\": output.completion,\n",
    "            },\n",
    "            example_id=example.id,\n",
    "        )]\n",
    "    \n",
    "argilla_client = DefaultArgillaClient()\n",
    "workspace_id = argilla_client.create_workspace(\"test\")\n",
    "\n",
    "evaluator = InstructArgillaEvaluator(\n",
    "    ArgillaEvaluationRepository(evaluation_repository, argilla_client),\n",
    "    dataset_repository,\n",
    "    workspace_id,\n",
    "    fields,\n",
    "    questions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `partial_evaluate_dataset` posts the records created from a run to the argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eval_overview = evaluator.partial_evaluate_dataset(run_overview.id)\n",
    "except Exception as e:\n",
    "    print(e.response.json())\n",
    "\n",
    "print(eval_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access Once we have evaluated some examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = evaluator.aggregate_evaluation(eval_overview.id)\n",
    "print(output.statistics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligence-layer-WXd7Z3vu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
